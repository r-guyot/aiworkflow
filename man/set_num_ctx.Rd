% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/workflows.R
\name{set_num_ctx}
\alias{set_num_ctx}
\title{Set the length of the context to be handled by the model}
\usage{
set_num_ctx(workflow_obj, num_ctx)
}
\arguments{
\item{workflow_obj}{A workflow object containing all parameters describing the workflow required}

\item{num_ctx}{a numerical value defining the length of the context to be used, in tokens.}
}
\value{
a workflow object with the new added num_ctx parameter
}
\description{
\code{set_num_ctx} lets you define the length of the context to be supported by the model
}
\details{
Depending on the server settings, the length of the context handled by the model may be shorter than you expect.
For example, Ollama seems to default to a context size of 1024 tokens even if the model actually supports more.
In order to be able to fully use the capabilities of the model, you can specify the length that you expect it to support with num_ctx.
}
\examples{
my_workflow <- ai_workflow() |> 
set_model(model_name="llama3:8b-instruct-q5_0") |> 
set_num_ctx(num_ctx=2048)

}
