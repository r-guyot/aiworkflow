% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ollama.R
\name{get_ollama_completion}
\alias{get_ollama_completion}
\title{Get a completion from ollama server}
\usage{
get_ollama_completion(
  ollama_connection,
  model,
  prompts_vector,
  output_text_only = F,
  num_predict = 200,
  temperature = 0.8,
  system_prompt = NA
)
}
\arguments{
\item{ollama_connection}{a connection object that has the information on how to connect to the ollama server}

\item{model}{Name of the model to use on the ollama server. Note that you can only use the models that are available.}

\item{prompts_vector}{A vector containing one or more messages acting as prompts for the completion.}

\item{output_text_only}{A boolean value (default False) indicating if you just want the text message as output (TRUE) or the whole response coming from the server.}

\item{num_predict}{The number of tokens to generate in the response (maximum amount)}

\item{temperature}{The temperature value for the answer of the model. A temperature of 0 gives always the same answer. A temperature of 1 has a lot more variation. Default is 0.8.}

\item{system_prompt}{The system prompt used for your LLM.}
}
\description{
\code{get_ollama_completion} get a completion from the ollama server API
}
\details{
A simple function to test the connect to ollama
}
