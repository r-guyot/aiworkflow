---
title: "Tool Calling"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tool Calling}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Load the package.

```{r setup}
library(aiworkflow)
```

We are going to see in this vignette how you can declare tools to be used as part of a LLM workflow.

## Tool Declaration

What are tools exactly? Let's not try to make things more complicated than they are - tools are actually just functions that you can call. The idea is that some of the LLMs are trained during their development process on how to call functions in case they want to get specific information that is not part of their knowledge base. Not all LLMs support tool calling - you can use for example Llama 3.1 and Llama 3.2 for tool calling at the moment.

The first step to be able to assign a tool is to create the tool itself! 
Let's write a simple function. This will use the disastr.api package from CRAN for this example.
This function will get a list of recent disasters happening around the world. 

```{r, tool creation}

get_recent_disasters <- function(type_disaster=NA, country_filter=NA, ...) {

  library(disastr.api)
  res <- disastr.api(limit = 100)
  if (!is.na(type_disaster)) {
    type_disaster <- stringr::str_remove(type_disaster, "s$")
    res <- res |> dplyr::filter(grepl(type_disaster,event,ignore.case=T))
  }
  if (!is.na(country_filter)) {
    res <- res |> dplyr::filter(grepl(country_filter,country,ignore.case=T))
  }
  
  res_f <- res |> jsonlite::toJSON()  
  return(res_f)
}


```


As you can see the function accepts filters such as:

- type_disaster: the type of disaster to filter results by, if needed.
- country_filter: the country of interest, if any.

Note that we have also added a "..." extra argument. This is on purpose. This means the function will also accept as input other parameters beyond type_disaster and country_filter. The reason why we are doing this, is to take care of hallucinations. LLMs have a tendency to come up with additional arguments that do not exist for a given function (even when you give them exactly the list of arguments accepted) and adding this "..." parameter will make it possible for the function to *ignore such additional* parameters without failing and returning an error.

Note that smaller models are more prone to hallucinations when it comes to tool calling.

Now that we have our function, we need to prepare the declaration. 
We declare it as a list in R. You can follow the format below, as advised in the case of Llama3.1 and beyond:

```{r, tool declaration}

tool_list <- list(
  list(type="function",
       "function"=list(
         name="get_recent_disasters",
         description="get information about recent disasters that are happening or happened worldwide",
         parameters= list(
           type="object",
           properties = list(
             type_disaster=list(
                            type="string",
                            description="the type of disaster to search for. Make sure this is the singular version of the word"
             ),
             country_filter=list(
               type="string",
               description="a specific country you want to filter results for, related to disasters"
             )
           )
         )
       ))
)

```

As you can see, you need to specify what the arguments mean, what the function does, so that the LLM can *grasp* when it is a good time to use it.
Note that you are not limited to declaring a single function. You can have several functions (tools) as part of the above declaration. This way, you can increase the capability of your workflow to handle different types of requests.

Now let's build a workflow that will integrate this tool calling capability:


```{r, tool as part of workflow}

wflow_tool <- ai_workflow() |>
  set_connector("ollama")  |>
  set_temperature(0) |>
  set_model(model_name= "llama3.2:latest") |>
  set_system_prompt("you are an AI assistant capable of research recent disasters information with a tool connected to the Internet.") |>
  add_tools_declaration(tools = tool_list)

```

```{r, testing prompt}

wflow_tool |> 
  process_prompts("Tell me what recent disasters have happened in Mexico?") |> 
  pull_final_answer()

```
You can see the difference versus the same kind of workflow, with no tools support:

```{r, no tool as part of workflow}

wflow_no_tool <- ai_workflow() |>
  set_connector("ollama")  |>
  set_temperature(0) |>
  set_model(model_name= "llama3.2:latest") 

wflow_no_tool |> 
  process_prompts("Tell me what recent disasters have happened in Mexico?") |> 
  pull_final_answer()

```

As you can see, if you don't provide any tools, the LLM will use whatever memory it can recollect from its training (or whatever it can hallucinate...).

