---
title: "First Steps with aiworkflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{first_steps}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Load the package at first.

```{r setup}
library(aiworkflow)

```

## Setting up your Ollama connection

Ensure that you have a running [Ollama instance](https://ollama.com/download) on your local machine or somewhere available in your network. Installing Ollama is very easy and works on multiple platforms. Ollama will work even on machines equipped only with CPUs, while having machines with GPUs (especially Nvidia) will make processing much faster. Once you have Ollama installed you need to [download at least one model](https://ollama.com/library) to get started. 

You can establish a connection using the below function.

```{r connection to Ollama}

conn <- get_ollama_connection()

```

By default it will use localhost and the 11434 port. 
If you want to use a different setup, you can change such parameters:

```{r connection to Ollama with different parameters}

conn <- get_ollama_connection(ip_ad = "127.0.0.1",port = "3524")

```

## Your First Workflow

This is one of the most simple workflows you can make.
To create a workflow, you need to start with the ai_workflow() container command, and then pipe instructions to it. At the end of the preparation, you need to add an encapsulate() statement, that will prepare it to receive prompts and store answers.

In the below example we specify that we want to use the ollama connector, and that we will use the llama3.1 model. These two elements are the strict minimum required to create a workflow.

```{r first workflow }

wflow_basic <- ai_workflow() |>
  set_connector("ollama")  |>
  set_model(model_name= "llama3.1:8b-instruct-q5_K_M") |>
  encapsulate()

```

When selecting the ollama connector, the default connection parameters will be set. 
You can however set arbitrary IP and port parameters as described below, if you want to connect to an Ollama instance that is living on a different machine.

```{r first workflow with different parameters}

wflow_basic_on_different_machine <- ai_workflow() |>
  set_connector("ollama")  |>
  set_ip_addr(ip_addr = "192.168.1.12") |>
  set_port(port = 5256) |>
  set_model(model_name= "llama3.1:8b-instruct-q5_K_M") |>
  encapsulate()

```

At this stage your workflow exists, but does not do anything. 

The next steps is to ask it to run some specific tasks. We can ask with a simple prompt.

```{r first workflow execution}

wflow_basic |> 
  process_text_prompts(text_prompts = "why is the sky blue? Answer with a short explanation") |>
  pull_final_text_answer() 

```

By default the model answers with a list, so you want to use the pull_final_answer() function to fetch the final textual answer from the list.

## Processing a vector of questions

It is also possible to send more than one prompt to a workflow. You simply need to pass a vector instead a single chain of characters.

```{r several prompts execution}

wflow_basic |> 
  process_text_prompts(text_prompts = c("why is the sky blue? Answer with a short explanation","why is the sun hot?")) |>
  pull_final_text_answer() 

```
Note that the process_text_prompts function also accepts a list instead of vectors, if you prefer.


## Customizing Output

You can now leverage more features from the package. Such as setting the audience for your answers.
Here we specify that the audience is 5 years old kids.

```{r eli5}

wflow_eli5 <- ai_workflow() |> 
  set_connector("ollama")  |>
  set_model(model_name= "llama3.1:8b-instruct-q5_K_M") |>
  set_audience("Five years old kids") |>
  encapsulate()
  
```

You can see how it changes the output. Don't expect a great explanation!

```{r eli5 example}

explanation_eli5 <- wflow_eli5 |> 
  process_text_prompts("why is the sky blue? Answer with a short explanation") |>
  pull_final_answer() 

```

This is the kind of explanation you get for the little kids out there:

*"`r explanation_eli5`"*

## Modifying an existing workflow

When you want to modify an existing workflow, you need to first decapsulate() it to allow for its elements to be modified. You can imagine the encapsulation as a "lock" of current parameters, and you need to unlock things before you change the configuration. When you are done with the settings change, you finally call the encapsulate() again.

For example let's modify the existing workflow *wflow_basic* before calling the prompt:

```{r modification of existing workflow}

explainer_low_tech <- wflow_basic |> decapsulate() |>
  set_audience("people without scientific knowledge or background") |>
  encapsulate()

explanation_low_tech <- explainer_low_tech |>
  process_text_prompts("why is the sky blue? Answer with a short explanation") |>
  pull_final_answer() 

```

You should get something like that:

*"`r explanation_low_tech`"*

You can also set a specific tone or personality to answer a question. 

```{r snoop}

wflow_snoop <- ai_workflow() |>
  set_connector("ollama")  |>
  set_model(model_name= "llama3.1:8b-instruct-q5_K_M")|>
  set_style_of_voice("Snoop Dogg") |>
  encapsulate()

snoop_answer <- wflow_snoop |>
  process_text_prompts("Explain how the stock exchange works in a short paragraph") |>
  pull_final_text_answer() 

```

You should get something like that:

*"`r snoop_answer`"*

