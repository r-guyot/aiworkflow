---
title: "First Steps with aiworkflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{first_steps}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Load the package at first.

```{r setup}
library(aiworkflow)

```

## Setting up your Ollama connection

Ensure that you have a running [Ollama instance](https://ollama.com/download) on your local machine or somewhere available in your network. Installing Ollama is very easy and works on multiple platforms. Ollama will work even on machines equipped only with CPUs, while having machines with GPUs (especially Nvidia) will make processing much faster. Once you have Ollama installed you need to [download at least one model](https://ollama.com/library) to get started. 

You can establish a connection using the below function.

```{r connection to Ollama}

conn <- get_ollama_connection()

```

By default it will use localhost and the 11434 port. 
If you want to use a different setup, you can change such parameters:

```{r connection to Ollama with different parameters}

conn <- get_ollama_connection(ip_ad = "127.0.0.1",port = "3524")

```

## Your First Workflow

This is one of the most simple workflows you can make.
To create a workflow, you need to start with the ai_workflow() container command, and then pipe instructions to it.
In the below example we specify that we want to use the ollama connector, and that we will use the llama3.1 model.

```{r first workflow }

wflow_basic <- ai_workflow() |>
  set_connector("ollama")  |>
  set_model(model_name= "llama3.1:8b-instruct-q5_K_M") 

```

When selecting the ollama connector, it will use the default connection parameters. 
You can however set arbitrary IP and port parameters as described below, if you want to connect to an Ollama instance that is living on a different machine.

```{r first workflow with different parameters}

wflow_basic_on_different_machine <- ai_workflow() |>
  set_connector("ollama")  |>
  set_ip_addr(ip_addr = "192.168.1.12") |>
  set_port(port = 5256) |>
  set_model(model_name= "llama3.1:8b-instruct-q5_K_M") 

```

At this stage your workflow exists, but does not do anything. 

The next steps is to ask it to run some specific tasks. We can ask with a simple prompt.

```{r first workflow execution}

wflow_basic |> 
  process_prompts(prompts_vector = "why is the sky blue? Answer with a short explanation") |>
  pull_final_answer() |> cat()

```

By default the model answers with a list, so you want to use the pull_final_answer() function to fetch the final textual answer from the list.

## Customizing Output

You can now leverage more features from the package. Such as setting the audience for your answers.
Here we specify that the audience is 5 years old kids.

```{r eli5}

wflow_eli5 <- ai_workflow() |>
  set_connector("ollama")  |>
  set_model(model_name= "llama3.1:8b-instruct-q5_K_M") |>
  set_audience("Five years old kids")
  
```

You can see how it changes the output. Don't expect a great explanation!

```{r eli5 example}

explanation_eli5 <- wflow_eli5 |> 
  process_prompts(prompts_vector = "why is the sky blue? Answer with a short explanation") |>
  pull_final_answer() 

```

This is the kind of explanation you get for the little kids out there:

*"`r explanation_eli5`"*

Note that you can also change an existing workflow directly by piping parameter setting into it.
For example let's modify the existing workflow wflow_basic before calling the prompt:


```{r modification of existing workflow}

explanation_low_tech <- wflow_basic |> 
  set_audience("people without scientific knowledge or background") |>
  process_prompts(prompts_vector = "why is the sky blue? Answer with a short explanation") |>
  pull_final_answer() 

```

You should get something like that:

*"`r explanation_low_tech`"*

You can also set a specific tone or personality to answer a question. 

```{r snoop}

wflow_snoop <- ai_workflow() |>
  set_connector("ollama")  |>
  set_model(model_name= "llama3.1:8b-instruct-q5_K_M")|>
  set_style_of_voice("Snoop Dogg")

snoop_answer <- wflow_snoop |>
  process_prompts(prompts_vector = "Explain how the stock exchange works in a short paragraph") |>
  pull_final_answer() 

```

You should get something like that:

*"`r snoop_answer`"*

