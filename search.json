[{"path":"https://r-guyot.github.io/aiworkflow/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU Lesser General Public License","title":"GNU Lesser General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed. version GNU Lesser General Public License incorporates terms conditions version 3 GNU General Public License, supplemented additional permissions listed .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/LICENSE.html","id":"id_0-additional-definitions","dir":"","previous_headings":"","what":"0. Additional Definitions","title":"GNU Lesser General Public License","text":"used herein, “License” refers version 3 GNU Lesser General Public License, “GNU GPL” refers version 3 GNU General Public License. “Library” refers covered work governed License, Application Combined Work defined . “Application” work makes use interface provided Library, otherwise based Library. Defining subclass class defined Library deemed mode using interface provided Library. “Combined Work” work produced combining linking Application Library. particular version Library Combined Work made also called “Linked Version”. “Minimal Corresponding Source” Combined Work means Corresponding Source Combined Work, excluding source code portions Combined Work , considered isolation, based Application, Linked Version. “Corresponding Application Code” Combined Work means object code /source code Application, including data utility programs needed reproducing Combined Work Application, excluding System Libraries Combined Work.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/LICENSE.html","id":"id_1-exception-to-section-3-of-the-gnu-gpl","dir":"","previous_headings":"","what":"1. Exception to Section 3 of the GNU GPL","title":"GNU Lesser General Public License","text":"may convey covered work sections 3 4 License without bound section 3 GNU GPL.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/LICENSE.html","id":"id_2-conveying-modified-versions","dir":"","previous_headings":"","what":"2. Conveying Modified Versions","title":"GNU Lesser General Public License","text":"modify copy Library, , modifications, facility refers function data supplied Application uses facility (argument passed facility invoked), may convey copy modified version: ) License, provided make good faith effort ensure , event Application supply function data, facility still operates, performs whatever part purpose remains meaningful, b) GNU GPL, none additional permissions License applicable copy.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/LICENSE.html","id":"id_3-object-code-incorporating-material-from-library-header-files","dir":"","previous_headings":"","what":"3. Object Code Incorporating Material from Library Header Files","title":"GNU Lesser General Public License","text":"object code form Application may incorporate material header file part Library. may convey object code terms choice, provided , incorporated material limited numerical parameters, data structure layouts accessors, small macros, inline functions templates (ten fewer lines length), following: ) Give prominent notice copy object code Library used Library use covered License. b) Accompany object code copy GNU GPL license document.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/LICENSE.html","id":"id_4-combined-works","dir":"","previous_headings":"","what":"4. Combined Works","title":"GNU Lesser General Public License","text":"may convey Combined Work terms choice , taken together, effectively restrict modification portions Library contained Combined Work reverse engineering debugging modifications, also following: ) Give prominent notice copy Combined Work Library used Library use covered License. b) Accompany Combined Work copy GNU GPL license document. c) Combined Work displays copyright notices execution, include copyright notice Library among notices, well reference directing user copies GNU GPL license document. d) one following: 0) Convey Minimal Corresponding Source terms License, Corresponding Application Code form suitable , terms permit, user recombine relink Application modified version Linked Version produce modified Combined Work, manner specified section 6 GNU GPL conveying Corresponding Source. 1) Use suitable shared library mechanism linking Library. suitable mechanism one () uses run time copy Library already present user’s computer system, (b) operate properly modified version Library interface-compatible Linked Version. e) Provide Installation Information, otherwise required provide information section 6 GNU GPL, extent information necessary install execute modified version Combined Work produced recombining relinking Application modified version Linked Version. (use option 4d0, Installation Information must accompany Minimal Corresponding Source Corresponding Application Code. use option 4d1, must provide Installation Information manner specified section 6 GNU GPL conveying Corresponding Source.)","code":""},{"path":"https://r-guyot.github.io/aiworkflow/LICENSE.html","id":"id_5-combined-libraries","dir":"","previous_headings":"","what":"5. Combined Libraries","title":"GNU Lesser General Public License","text":"may place library facilities work based Library side side single library together library facilities Applications covered License, convey combined library terms choice, following: ) Accompany combined library copy work based Library, uncombined library facilities, conveyed terms License. b) Give prominent notice combined library part work based Library, explaining find accompanying uncombined form work.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/LICENSE.html","id":"id_6-revised-versions-of-the-gnu-lesser-general-public-license","dir":"","previous_headings":"","what":"6. Revised Versions of the GNU Lesser General Public License","title":"GNU Lesser General Public License","text":"Free Software Foundation may publish revised /new versions GNU Lesser General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Library received specifies certain numbered version GNU Lesser General Public License “later version” applies , option following terms conditions either published version later version published Free Software Foundation. Library received specify version number GNU Lesser General Public License, may choose version GNU Lesser General Public License ever published Free Software Foundation. Library received specifies proxy can decide whether future versions GNU Lesser General Public License shall apply, proxy’s public statement acceptance version permanent authorization choose version Library.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/articles/first_steps.html","id":"setting-up-your-ollama-connection","dir":"Articles","previous_headings":"","what":"Setting up your Ollama connection","title":"First Steps with aiworkflow","text":"Ensure running Ollama instance local machine somewhere available network. Installing Ollama easy works multiple platforms. Ollama work even machines equipped CPUs, machines GPUs (especially Nvidia) make processing much faster. Ollama installed need download least one model get started. can establish connection using function. default use localhost 11434 port. want use different setup, can change parameters:","code":"conn <- get_ollama_connection() conn <- get_ollama_connection(ip_ad = \"127.0.0.1\",port = \"3524\")"},{"path":"https://r-guyot.github.io/aiworkflow/articles/first_steps.html","id":"your-first-workflow","dir":"Articles","previous_headings":"","what":"Your First Workflow","title":"First Steps with aiworkflow","text":"one simple workflows can make. create workflow, need start ai_workflow() container command, pipe instructions . example specify want use ollama connector, use llama3.1 model. selecting ollama connector, use default connection parameters. can however set arbitrary IP port parameters described , want connect Ollama instance living different machine. stage workflow exists, anything. next steps ask run specific tasks. can ask simple prompt. default model answers list, want use pull_final_answer() function fetch final textual answer list.","code":"wflow_basic <- ai_workflow() |>   set_connector(\"ollama\")  |>   set_model(model_name= \"llama3.1:8b-instruct-q5_K_M\")  #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. wflow_basic_on_different_machine <- ai_workflow() |>   set_connector(\"ollama\")  |>   set_ip_addr(ip_addr = \"192.168.1.12\") |>   set_port(port = 5256) |>   set_model(model_name= \"llama3.1:8b-instruct-q5_K_M\")  #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. #> → IP address has been changed to 192.168.1.12. #> → Port has been changed to 5256. wflow_basic |>    process_prompts(prompts_vector = \"why is the sky blue? Answer with a short explanation\") |>   pull_final_answer() |> cat() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Temperature was not specified and given a default value of 0.8. #> → N_predict was not specified and given a default value of 200. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> The sky appears blue because of a phenomenon called scattering, where shorter (blue) wavelengths of light are scattered more than longer (red) wavelengths by tiny molecules of gases in the atmosphere. This scattering effect gives our sky its distinct blue color!"},{"path":"https://r-guyot.github.io/aiworkflow/articles/first_steps.html","id":"customizing-output","dir":"Articles","previous_headings":"","what":"Customizing Output","title":"First Steps with aiworkflow","text":"can now leverage features package. setting audience answers. specify audience 5 years old kids. can see changes output. Don’t expect great explanation! kind explanation get little kids : *“Let tell SECRET SKY! sky looks BLUE something called LIGHT! sunlight comes sun, ’s like big bunch colorful rays shining towards us. guess happens light rays travel air atmosphere? get SCATTERED start bouncing around everywhere! Blue one colors gets scattered color. look sky, blue light rays bounce back eyes, making LOOK BLUE! Isn’t COOL?!“* Note can also change existing workflow directly piping parameter setting . example let’s modify existing workflow wflow_basic calling prompt: get something like : *“sky appears blue something called light scattering. sunlight enters atmosphere, ’s made different colors like big ol’ rainbow. tiny particles air (like dust water vapor) bounce colorful lights, scatter shorter wavelengths longer ones. guess ? Blue one short-wavelength colors! , eyes see scattered light every direction us, perceive big blue sky!“* can also set specific tone personality answer question. get something like : “Yo ’s good fam? Alright wanna know ‘bout da stock exchangah, right? Okay lemme break ya. See, ’s like one big ol’ party investors come buy sell shares companies think gonna hot future. put money stocks, kinda like bettin’ sports team somethin’. da company good, da stock price goes make dough! tanks… well, let’s just say might wanna sit one , G. exchange buyin’ sellin’ happens, kinda like online market real people makin’ deals face--face phone calls. Word.”","code":"wflow_eli5 <- ai_workflow() |>   set_connector(\"ollama\")  |>   set_model(model_name= \"llama3.1:8b-instruct-q5_K_M\") |>   set_audience(\"Five years old kids\") #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. explanation_eli5 <- wflow_eli5 |>    process_prompts(prompts_vector = \"why is the sky blue? Answer with a short explanation\") |>   pull_final_answer()  #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Temperature was not specified and given a default value of 0.8. #> → N_predict was not specified and given a default value of 200. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode explanation_low_tech <- wflow_basic |>    set_audience(\"people without scientific knowledge or background\") |>   process_prompts(prompts_vector = \"why is the sky blue? Answer with a short explanation\") |>   pull_final_answer()  #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Temperature was not specified and given a default value of 0.8. #> → N_predict was not specified and given a default value of 200. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode wflow_snoop <- ai_workflow() |>   set_connector(\"ollama\")  |>   set_model(model_name= \"llama3.1:8b-instruct-q5_K_M\")|>   set_style_of_voice(\"Snoop Dogg\") #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434.  snoop_answer <- wflow_snoop |>   process_prompts(prompts_vector = \"Explain how the stock exchange works in a short paragraph\") |>   pull_final_answer()  #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Temperature was not specified and given a default value of 0.8. #> → N_predict was not specified and given a default value of 200. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode"},{"path":"https://r-guyot.github.io/aiworkflow/articles/processing_skills.html","id":"what-are-processing-skills","dir":"Articles","previous_headings":"","what":"What are Processing Skills?","title":"Processing Skills","text":"Processing skills fancy name series pre-defined prompts can process inputs specific way. Instead leaving hanging, aiworkflow comes batteries included help numerous tasks box. First, get know kind skills available, can use ’list_processing_skills()` function. give name skills available right now current version package.","code":"list_processing_skills() #>  [1] \"add_code_comments\"              \"add_details\"                    #>  [3] \"add_emoji\"                      \"break_down_tasks\"               #>  [5] \"categorize_product_complaint\"   \"categorize_sentiment\"           #>  [7] \"chain_of_thought\"               \"clean_up_audio_transcription\"   #>  [9] \"extract_actions\"                \"extract_names\"                  #> [11] \"fix_copy\"                       \"fix_translation\"                #> [13] \"generate_code\"                  \"generate_creative_introduction\" #> [15] \"generate_knowledge_graph_v2\"    \"generate_knowledge_graph\"       #> [17] \"generate_quiz_with_answers\"     \"generate_quiz\"                  #> [19] \"identify_product\"               \"rewrite_active_voice\"           #> [21] \"rewrite_blog_copy\"              \"rewrite_bullet_points\"          #> [23] \"rewrite_casual\"                 \"rewrite_emoji\"                  #> [25] \"rewrite_in_first_person\"        \"rewrite_jargon\"                 #> [27] \"rewrite_paraphrase\"             \"rewrite_passive_voice\"          #> [29] \"rewrite_positive\"               \"rewrite_prompt\"                 #> [31] \"rewrite_text_as_anonymized\"     \"tldr\"                           #> [33] \"translate\"                      \"write_abstract\"                 #> [35] \"write_article_from_title\"       \"write_email\"                    #> [37] \"write_fiction_chapter\"          \"write_pitch\"                    #> [39] \"write_qa\"                       \"write_quarto_presentation\"      #> [41] \"write_recommendation\"           \"write_resume_narrative\"         #> [43] \"write_short_summary\"            \"write_text_expansion\"           #> [45] \"write_title\"                    \"write_tweet\""},{"path":"https://r-guyot.github.io/aiworkflow/articles/processing_skills.html","id":"applying-skills-to-a-workflow","dir":"Articles","previous_headings":"","what":"Applying Skills to a Workflow","title":"Processing Skills","text":"seen list, quite things choose . Let’s start something simple. can ask LLM write pitch us, simple product idea. use set_processing_skill() function specify skill want use. Now let’s test . trying sell new razors… Let’s try one example . next skill add color sentences sprinkling emojis .","code":"myflow_pitch <- ai_workflow() |>   set_connector(connector = \"ollama\") |>   set_model(model_name = \"llama3.2:latest\") |>   set_processing_skill(processing_skill = \"write_pitch\") |>   set_n_predict(1500) #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. myflow_pitch |>   process_prompts(prompts_vector = \"A new razor with 20 blades and a lot of springs for an ultimate and safe shaving experience aimed at adult men\") |>   pull_final_answer() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Temperature was not specified and given a default value of 0.8. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> [1] \"Introducing Shavion - The Ultimate Safety Razor Experience\\n\\nAre you tired of dealing with nicks, cuts, and uneven shaves? Do you want to achieve a closer, smoother shave without sacrificing your safety?\\n\\nOur new razor features an unprecedented 20 blades and multiple springs that work together in harmony to provide the ultimate shaving experience. With Shavion, say goodbye to ingrown hairs, razor burn, and other common shaving woes.\\n\\nIdeal for busy professionals and men who demand the best from their grooming routine, Shavion is designed to deliver a safe, efficient, and comfortable shave every time. Whether you're looking to upgrade your morning ritual or simply want to experience the art of traditional shaving, our innovative safety razor has got you covered.\\n\\nJoin the smoothest revolution in shaver history - order yours today!\" myflow_emoji <- ai_workflow() |>   set_connector(connector = \"ollama\") |>   set_model(model_name = \"llama3.2:latest\") |>   set_processing_skill(processing_skill = \"add_emoji\") |>   set_n_predict(1500) |>   set_temperature(0.5) #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. myflow_emoji |>   process_prompts(\"The sun is shining today. Just the right temperature. I can finally relax and enjoy a good book.\") |>   pull_final_answer() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> [1] \"The ☀️ sun is shining today 🌞. Just the ⚖️ right temperature. I can 💆‍♀️ finally relax and 👓 enjoy a 😊 good book.\""},{"path":"https://r-guyot.github.io/aiworkflow/articles/processing_skills.html","id":"applying-skills-in-sequence","dir":"Articles","previous_headings":"","what":"Applying Skills in Sequence","title":"Processing Skills","text":"may wondering… great, can single-action skills, way combine one ? answer YES! Let’s start depressing piece news: Let’s make workflow turns positive light. don’t just want add positivity. also want add details summary. Now let’s chain together using switch_to_workflow() function, using pipes.","code":"company_report <- \"The Q2 quarter was awful. Our leading product, CLEAN-Z, has lost 15% market share in a few months, hurting our bottom line very badly. We now have too much inventory on our hands and wholesalers may be shipping more back to us if we can't rapidly increase our sales. Our Product development team lucked out and failed to release the new formula on time. It looks like it will be delayed another 2 or three months. The oulook for the rest of the year looks grim.\" myflow_positive <- ai_workflow() |>   set_connector(connector = \"ollama\") |>   set_model(model_name = \"llama3.2:latest\") |>   set_processing_skill(processing_skill = \"rewrite_positive\") |>   set_n_predict(1500) |>   set_temperature(0.5) #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. myflow_more_details <- ai_workflow() |>   set_connector(connector = \"ollama\") |>   set_model(model_name = \"llama3.2:latest\") |>   set_processing_skill(processing_skill = \"add_details\") |>   set_n_predict(1500) |>   set_temperature(0.5) #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. # you start from the first workflow here myflow_positive |>    # you bring your first prompt here   process_prompts(prompts_vector = company_report) |>   # you can display the intermediate answer if needed   display_intermediate_answer() |>   # and then answer from the previous workflow becomes the input for the next one   switch_to_workflow(new_workflow = myflow_more_details) |>   # and we pull the final answer   pull_final_answer() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> → Intermediate answer from last step: #> Despite initial challenges, Q2 has presented an opportunity to reassess and #> refocus our strategy. Our leading product, CLEAN-Z, is undergoing a necessary #> adjustment period as it navigates changes in market demand, resulting in a #> temporary 15% decline in share. However, this shift also allows us to #> re-evaluate and optimize our sales approach, ultimately driving growth and #> improvement. The Product development team's successful identification of the #> need for a new formula has led to an exciting timeline extension, enabling #> further refinement and iteration before its anticipated release later this #> year. With a refreshed outlook, we're poised for a strong finish in Q4, driven #> by innovative solutions and strategic adjustments that will propel us forward. #> → Frequency Penalty was not specified and given a default value of 1. #>  #> → Presence Penalty was not specified and given a default value of 1.5. #>  #> → Repeat Penalty was not specified and given a default value of 1.2. #>  #> → Mode was not specified and 'chat' was selected by default. #>  #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #>  #> → Chat mode #> [1] \"Despite initial challenges, Q2 has presented an opportunity to reassess and refocus our strategy as we navigate uncharted territory. Our leading product, CLEAN-Z, is undergoing a necessary adjustment period as it grapples with changes in market demand, resulting in a temporary 15% decline in share. However, this shift also allows us to re-evaluate and optimize our sales approach, ultimately driving growth and improvement.\\n\\nA thorough analysis of customer feedback has revealed key insights into the evolving needs of our target audience, highlighting areas where we can refine our product offerings to better meet their demands. The Product development team's successful identification of the need for a new formula has led to an exciting timeline extension, enabling further refinement and iteration before its anticipated release later this year.\\n\\nAs we move forward with enhanced research capabilities and increased investment in quality control measures, we're confident that CLEAN-Z will emerge from this period even stronger. The new formulation promises to deliver improved performance, increased durability, and a more sustainable user experience – all of which are critical factors driving adoption and loyalty among our core customer base.\\n\\nWith a refreshed outlook, we're poised for a strong finish in Q4, driven by innovative solutions and strategic adjustments that will propel us forward. Our sales teams have undergone comprehensive training to effectively communicate the value proposition of CLEAN-Z's upgraded features, ensuring seamless integration into existing workflows and expanded market reach. By leveraging these advancements, we'll solidify our position as industry leaders and cement a lasting competitive advantage in an increasingly dynamic marketplace.\""},{"path":"https://r-guyot.github.io/aiworkflow/articles/processing_skills.html","id":"chaining-workflows-other-method","dir":"Articles","previous_headings":"","what":"Chaining Workflows (Other Method)","title":"Processing Skills","text":"another way chain workflow together, using add_workflow_step() function. result similar previous example: key difference way observe intermediate results . makes easier add steps single workflow: now able get one go, final text translated Spanish way. Note quality Spanish translation case depend heavily well model selected can handle kind tasks. models equal comes multilingual capabilities.","code":"# you start from the first workflow here myflow_chained <-   myflow_positive |>   add_workflow_step(myflow_more_details) # you start from the combined workflow here myflow_chained |>    # you bring your prompt here   process_prompts(prompts_vector = company_report) |>   pull_final_answer() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> [[1]] #> [1] \"Despite initial setbacks, we're taking a proactive approach to address our Q2 quarter challenges. Our leading product, CLEAN-Z, has experienced an opportunity for growth as it faces increased competition in the market from rival brands that have been aggressively expanding their portfolios of eco-friendly cleaning solutions. While this shift presents some short-term difficulties, such as managing inventory levels and meeting sales targets - particularly with regards to maintaining our competitive pricing strategy while also investing in new marketing campaigns to reach a wider audience - it also opens doors for strategic re-evaluation of our offerings.\\n\\nThe Product development team's recent misstep serves as a valuable learning experience that will inform their approach to future product launches, including more rigorous testing and quality control measures. This is particularly important given the growing demand from environmentally conscious consumers who are willing to pay a premium for products with proven efficacy in reducing waste and promoting sustainability. With a revised timeline now in place, we're confident that our new CLEAN-Z formula will hit the market with renewed excitement - featuring enhanced performance characteristics such as improved stain resistance and longer-lasting fragrances.\\n\\nWe anticipate significant growth for the remainder of the year, driven by both existing customers who have come to trust our brand's commitment to innovation and customer satisfaction. Additionally, we expect a boost in new business from retailers looking to capitalize on emerging trends in eco-friendly cleaning solutions. As we move forward with this revised plan, we remain committed to delivering exceptional value to our shareholders while also staying true to the core values that have driven our success since inception: quality, innovation, and customer satisfaction.\\n\\nThe Product development team's renewed focus on R&D is expected to yield a pipeline of new products in the coming months - including potential spin-offs from CLEAN-Z technology. This will not only help us stay ahead of competitors but also provide opportunities for growth through strategic partnerships with key industry players. With our revised strategy firmly in place, we're confident that we'll emerge stronger and more resilient than ever, poised to capitalize on emerging trends and drive long-term success for the company.\" myflow_translate_to_spanish <- ai_workflow() |>   set_connector(connector = \"ollama\") |>   set_model(model_name = \"llama3.2:latest\") |>   set_processing_skill(processing_skill = \"translate\",target_language=\"spanish\") |>   set_n_predict(2000) |>   set_temperature(0.5) #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434.  myflow_chained <-   myflow_positive |>   add_workflow_step(myflow_more_details) |>   add_workflow_step(myflow_translate_to_spanish) # you start from the combined workflow here myflow_chained |>    # you bring your prompt here   process_prompts(prompts_vector = company_report) |>   pull_final_answer() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> → Applying additional variables to working env #> [[1]] #> [1] \"===\\nEl cuarto del año II presentó una oportunidad para el crecimiento y la mejora, ya que nuestro producto líder CLEAN-Z experimentó un lento descenso en su participación de mercado debido a la competencia aumentada por marcas emergentes en el segmento de productos limpiadores premium.\\nEstas nuevas entrantes han logrado con éxito aprovechar las plataformas de redes sociales para conectarse con consumidores conscientes del medio ambiente, quienes están impulsando la demanda de alternativas amigables con el medio ambiente como CLEAN-Z.\\n\\nEste breve contratiempo nos ha motivado a reevaluar nuestra estrategia de ventas y explorar soluciones innovadoras para aumentar la demanda por parte de nuestros clientes.\\nAhora estamos enfocándonos en campañas publicitarias dirigidas que resaltan los beneficios únicos del formula de CLEAN-Z basada en plantas, como su embalaje biodegradable y huella de carbono reducida.\\nAdemás, aumentaremos nuestra presencia en línea a través de capacidades comerciales electrónicas mejoradas y acuerdos estratégicos con influencers benéficos para la salud.\\n\\nMientras tanto, nuestro equipo de desarrollo de productos está trabajando incansablemente para llevar al mercado el nuevo formula altamente esperado codificado como \\\"CLEAN-Z 2.0\\\".\\nEste producto innovador presenta tecnología nanotecnológica que mejora la eficiencia limpia mientras minimiza su impacto ambiental.\\nCon su lanzamiento previsto en los próximos 2-3 meses, CLEAN-Z 2.0 promete consolidar aún más nuestra marca como líder en el segmentio de productos limpiadores premium.\\n\\nCon este nuevo impulso, estamos preparados para un fuerte segundo semestre del año, impulsado por la mayor interés consumista en los beneficios únicos y características de CLEAN-Z.\\nAnticipamos crecimiento significativo a través de todos nuestros canales de ventas, incluyendo retail electrónico, tiendas físicas y nuestro modelo directo al cliente.\\nAl entrar en el resto del Q2 y más allá, estamos confiados que nuestras ajustes estratégicos producirán un gran retorno sobre la inversión y impulsarán éxito a largo plazo para CLEAN-Z.\""},{"path":"https://r-guyot.github.io/aiworkflow/articles/saving_workflows.html","id":"how-to-save-of-workflow","dir":"Articles","previous_headings":"","what":"How to save of workflow","title":"Saving workflows","text":"Since workflows can contain quite parameter declarations, may need save specific objects portability. exactly can save_workflow() : save configuration workflow JSON file.","code":"myflow_pitch <- ai_workflow() |>   set_connector(connector = \"ollama\") |>   set_model(model_name = \"llama3.2:latest\") |>   set_audience(audience = \"Marketing professionals\") |>   set_temperature(temperature = 0.6) |>   set_processing_skill(processing_skill = \"write_pitch\") |>   set_n_predict(3000) #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434.  res <- myflow_pitch |> save_workflow(filepath = \"wflow_pitch.json\") #> → The workflow has been saved to wflow_pitch.json."},{"path":"https://r-guyot.github.io/aiworkflow/articles/saving_workflows.html","id":"how-to-reload-a-workflow","dir":"Articles","previous_headings":"","what":"How to reload a workflow","title":"Saving workflows","text":"reverse operation uses load_workflow() : Note JSON ensure loaded workflow identical one saved . example, figures stored double may reloaded integers. lead issues running workflow. Let’s clean-afterwards:","code":"myflow_pitch_reloaded <- load_workflow(filepath = \"wflow_pitch.json\") #> → The workflow has been successfully reloaded from wflow_pitch.json. unlink(\"wflow_pitch.json\")"},{"path":"https://r-guyot.github.io/aiworkflow/articles/tool_calling.html","id":"tool-preparation","dir":"Articles","previous_headings":"","what":"Tool Preparation","title":"Tool Calling","text":"tools exactly? Let’s try make things complicated - tools actually just functions can call. idea LLMs trained development process call functions case want get specific information part knowledge base. LLMs support tool calling - can use example Llama 3.1 Llama 3.2 tool calling moment. first step able assign tool create tool ! Let’s write simple function. use disastr.api package CRAN example. function get list recent disasters happening around world. Note write function LLM, need ensure : function returns data JSON format function loaded global environment first LLM tries call function, added filters : type_disaster: type disaster filter results , needed. country_filter: country interest, . Note also added “…” extra argument. purpose. means function also accept input parameters beyond type_disaster country_filter. reason , take care hallucinations. LLMs tendency come additional arguments exist given function (even give exactly list arguments accepted) adding “…” parameter make possible function ignore additional parameters without failing returning error. Note smaller models prone hallucinations comes tool calling.","code":"get_recent_disasters <- function(type_disaster=NA, country_filter=NA, ...) {    library(disastr.api)   res <- disastr.api(limit = 100)   if (!is.na(type_disaster)) {     type_disaster <- stringr::str_remove(type_disaster, \"s$\")     res <- res |> dplyr::filter(grepl(type_disaster,event,ignore.case=T))   }   if (!is.na(country_filter)) {     res <- res |> dplyr::filter(grepl(country_filter,country,ignore.case=T))   }      res_f <- res |> jsonlite::toJSON()     return(res_f) }"},{"path":"https://r-guyot.github.io/aiworkflow/articles/tool_calling.html","id":"tool-declaration","dir":"Articles","previous_headings":"","what":"Tool Declaration","title":"Tool Calling","text":"Now function, need prepare declaration. declare list R. can follow format , advised case Llama3.1 beyond: can see, need specify arguments mean, function , LLM can grasp good time use . Note limited declaring single function. can several functions (tools) part declaration. way, can increase capability workflow handle different types requests.","code":"tool_list <- list(   list(type=\"function\",        \"function\"=list(          name=\"get_recent_disasters\",          description=\"get information about recent disasters that are happening or happened worldwide\",          parameters= list(            type=\"object\",            properties = list(              type_disaster=list(                             type=\"string\",                             description=\"the type of disaster to search for. Make sure this is the singular version of the word\"              ),              country_filter=list(                type=\"string\",                description=\"a specific country you want to filter results for, related to disasters\"              )            )          )        )) )"},{"path":"https://r-guyot.github.io/aiworkflow/articles/tool_calling.html","id":"building-a-workflow-with-tools","dir":"Articles","previous_headings":"","what":"Building a workflow with tools","title":"Tool Calling","text":"Now let’s build workflow integrate tool calling capability, using add_tools_declaration() function: Now workflow ready, can try . won’t see internal details, actually happening LLM first function call, confirm happened Mexico first, based information received function, LLM formulate second answer (final one) uses info. can see difference versus kind workflow, tools support: can see, don’t provide tools, LLM use whatever memory can recollect training (whatever can hallucinate…).","code":"wflow_tool <- ai_workflow() |>   set_connector(\"ollama\")  |>   set_temperature(0) |>   set_model(model_name= \"llama3.2:latest\") |>   set_system_prompt(\"you are an AI assistant capable of research recent disasters information with a tool connected to the Internet.\") |>   add_tools_declaration(tools = tool_list) #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. wflow_tool |>    process_prompts(\"Tell me what recent disasters have happened in Mexico?\") |>    pull_final_answer() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → N_predict was not specified and given a default value of 200. #> → Mode was not specified and 'chat' was selected by default. #> → Chat mode #> → Adding tools #>  #> The disastR.api package may be cited as: #> Dworschak, Christoph. 2021. \"Disastr.api: Wrapper for the UN OCHA #> ReliefWeb Disaster Events API.\" R package. CRAN version 1.0.6. #> Your disaster event data request was successful. #> [1] \"Based on the information I have access to, here are some recent disasters that have occurred in Mexico:\\n\\n1. Hurricane Beryl (June 2024): A tropical cyclone that affected several countries in the Caribbean and Central America, including Mexico.\\n2. Earthquake (February 2023): A magnitude 7.6 earthquake struck southern Mexico, causing widespread damage and loss of life.\\n\\nPlease note that my knowledge cutoff is December 2023, so I may not have information on more recent disasters that occurred after this date.\" wflow_no_tool <- ai_workflow() |>   set_connector(\"ollama\")  |>   set_temperature(0) |>   set_model(model_name= \"llama3.2:latest\")  #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434.  wflow_no_tool |>    process_prompts(\"Tell me what recent disasters have happened in Mexico?\") |>    pull_final_answer() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → N_predict was not specified and given a default value of 200. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> [1] \"I'll provide you with some information on recent natural disasters that have occurred in Mexico:\\n\\n1. **Hurricane Patricia (2015)**: A Category 5 hurricane made landfall in Jalisco, Mexico, causing widespread damage and flooding.\\n2. **Earthquake in Puebla (2017)**: A magnitude 7.1 earthquake struck the state of Puebla, killing over 300 people and injuring many more.\\n3. **Hurricane Odile (2014)**: Although not as destructive as Patricia, Hurricane Odile still caused significant damage to coastal areas in Baja California Sur.\\n4. **Floods in Oaxaca (2020)**: Heavy rainfall led to severe flooding in the state of Oaxaca, affecting thousands of people and causing widespread destruction.\\n5. **Volcanic eruptions in Mexico City (2019-2021)**: The Popocatépetl volcano erupted several times between 2019 and 2021, forcing\""},{"path":"https://r-guyot.github.io/aiworkflow/articles/tool_calling.html","id":"offline-tools","dir":"Articles","previous_headings":"","what":"Offline Tools","title":"Tool Calling","text":"often see examples online tools used connect API online sources pull information, way. can example build tool support calculations math operations, since LLMs notoriously bad (good reasons, don’t embed concept numbers, tokens). Let’s first declare function maths: Don’t expect much function. work simple operations. won’t solve binomial equations something. Now declare math tool: Let’s ask Snoop Dogg answer math problem: take calculator, see expect answer 566233.71 less. LLM equipped tool give right answer. Now can see difference makes equipped tool: answer differs somewhat. case, ’s far, still incorrect. LLM equipped tools typically perform better across range different problems.","code":"do_math <- function(expression_to_evaluate) {      res <- eval(parse(text=expression_to_evaluate))   return(paste0(\"{'expression':'\",expression_to_evaluate,\",'result':'\",res,\"'}\"))    } tool_list <- list(   list(type=\"function\",        \"function\"=list(          name=\"do_math\",          description=\"Do simple math calculations by providing a math expression to evaluate\",          parameters= list(            type=\"object\",            properties = list(              expression_to_evaluate=list(                type=\"string\",                description=\"the mathematical expression to evaluate, without an equal sign\"              )            ),            required=list(\"expression_to_evaluate\")          ))   ) ) myflow_math <- ai_workflow() |>   set_connector(\"ollama\")  |>   set_model(model_name= \"llama3.2:latest\") |>    set_n_predict(1000) |>   set_temperature(0.2) |>    set_style_of_voice(\"Snoop Dogg\") |>    add_tools_declaration(tools = tool_list) #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434.  myflow_math |>   process_prompts(prompts_vector = \"Can you help me solve this math problem? How much is 1321212* 3 , and dividing this whole thing by 7 in the end?\") |>   pull_final_answer() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> → Adding tools #> [1] \"Yo, what's good fam? So you got a math problem that's like, \\\"Hey Snoop, can you help me out?\\\" Alright, let's get to it.\\n\\nSo, we gotta multiply 1,321,212 by 3 first, ya dig? That's like, 3 times the number, foo'. And then we divide that whole thing by 7. Word.\\n\\nAlright, so... (1321212 * 3) = 3,966,336\\n\\nNow, let's get to dividing it by 7...\\n\\n(3,966,336 / 7) = 566,233.71\\n\\nSo there you have it, my G! The answer is like, 566,233.71. You feel me?\" myflow_clueless_at_math <- ai_workflow() |>   set_connector(\"ollama\")  |>   set_model(model_name= \"llama3.2:latest\") |>    set_n_predict(1000) |>   set_temperature(0.2) |>    set_style_of_voice(\"Snoop Dogg\")  #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434.  myflow_clueless_at_math |>   process_prompts(prompts_vector = \"Can you help me solve this math problem? How much is 1321212* 3 , and dividing this whole thing by 7 in the end?\") |>   pull_final_answer() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> [1] \"Yo, what's good fam? It sounds like you're tryin' to get that math done, know what I'm sayin'? Alright, let's break it down. You gotta multiply 1,321,212 by 3 first, then divide the whole thing by 7.\\n\\nSo, ya do that multiplication, and... (pauses for a sec) ...you're lookin' at somethin' like this: 1321212 * 3 = 3963646. Word up!\\n\\nNow, you gotta take that result and divide it by 7. So, we got 3963646 ÷ 7 = 566522.\\n\\nThere ya have it, homie! The answer is 566,522. You're all set now, ain't nothin' to worry 'bout.\""},{"path":"https://r-guyot.github.io/aiworkflow/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Raphael Guyot. Author, maintainer.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Guyot R (2024). aiworkflow: Enable R users leverage AI frameworks like Ollama Llama.cpp. R package version 0.1.3.0, https://r-guyot.github.io/aiworkflow/.","code":"@Manual{,   title = {aiworkflow: Enable R users to leverage AI frameworks like Ollama and Llama.cpp},   author = {Raphael Guyot},   year = {2024},   note = {R package version 0.1.3.0},   url = {https://r-guyot.github.io/aiworkflow/}, }"},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"aiworkflow-package-for-r","dir":"","previous_headings":"","what":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"package, aiworkflow, aimed making simple interact local LLMs using R. package aimed creating chat clients (definitely support use case) rather executing LLMs large amount data, reproducible way. Think executing LLMs NLP tasks dataframes typical use case. package much alpha stages. can already lot things, lacks complete testing coverage, API package subject change. aware functions may change deprecated 1.0 version reached.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"can use install github version directly devtools: pak: currently CRAN package may change near future.","code":"devtools::install_github(\"r-guyot/aiworkflow\") pak::pkg_install(\"r-guyot/aiworkflow\")"},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"LGPL v3, means, layman terms: share redistribute modification make package free use package -power applications, whether Open-source proprietary. release code proprietary apps use package. Please refer full details LICENSE document case.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"requirements","dir":"","previous_headings":"","what":"Requirements","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"need least: Ollama instance running machine, along embedding model LLM already downloaded . able use package moment. qdrant instance optional.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"current-features","dir":"","previous_headings":"","what":"Current Features","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"current version brings following features: pipes support LLM operations client Ollama run local LLM operations client Qdrant database store vector embeddings support basic RAG support tool calling LLMs support (like Llama3.1) support local vector embeddings database using feather file numerous processing skills (pre-defined prompts) can used box support chaining multiple LLM operations pipes support numerous prompt modification functions (audience, role, style, etc…) support JSON output extraction probably …","code":""},{"path":[]},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"cran","dir":"","previous_headings":"Upcoming Features","what":"CRAN","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"goal published CRAN robust enough complete shape.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"llm-backend-support","dir":"","previous_headings":"Upcoming Features","what":"LLM Backend support","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"Ultimately idea package expand backends run LLMs: llama.cpp VLLM llamafile","code":""},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"vision-llm-support","dir":"","previous_headings":"Upcoming Features","what":"Vision LLM Support","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"package also support Vision models accept images inputs, road.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"image-generation-support","dir":"","previous_headings":"Upcoming Features","what":"Image Generation Support","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"package eventually also support image generation ComfyUI API (likely).","code":""},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"contributions","dir":"","previous_headings":"","what":"Contributions","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"interested contribute package, welcome issue PR. Please also consider filing requests new features course bug reports.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_api_key_header.html","id":null,"dir":"Reference","previous_headings":"","what":"Adds API key header to the qdrant connection for security — add_api_key_header","title":"Adds API key header to the qdrant connection for security — add_api_key_header","text":"add_api_key_header adds security header connect Qdrant instance API key.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_api_key_header.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adds API key header to the qdrant connection for security — add_api_key_header","text":"","code":"add_api_key_header(req, conn)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_api_key_header.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adds API key header to the qdrant connection for security — add_api_key_header","text":"req httr2 request object prepared Qdrant REST API calls. conn connection object created get_qdrant_connection()","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_api_key_header.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adds API key header to the qdrant connection for security — add_api_key_header","text":"Adds security header connect Qdrant instance API key. API key value provided connection object obtained get_qdrant_connection(). function returns request object along necessary header.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_context.html","id":null,"dir":"Reference","previous_headings":"","what":"Adds context to be used by the model when answering — add_context","title":"Adds context to be used by the model when answering — add_context","text":"add_context lets add context info (shape context dataframe containing embeddings text info) supplement narrow answers LLM.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_context.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adds context to be used by the model when answering — add_context","text":"","code":"add_context(workflow_obj, context_df, context_usage_mandatory = FALSE)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_context.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adds context to be used by the model when answering — add_context","text":"workflow_obj workflow object containing parameters describing workflow required context_df dataframe containing text embeddings text information can used retrieve relevant context context_usage_mandatory boolean value (defaults FALSE) let LLM know use context answer, can use context optionally answer question.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_context.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adds context to be used by the model when answering — add_context","text":"workflow object new added context parameter.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_context.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adds context to be used by the model when answering — add_context","text":"work need add context_df build using generate_document_embeddings function.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_context.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adds context to be used by the model when answering — add_context","text":"","code":"conn <- get_ollama_connection()  document <- \"Standing proudly on the Île de la Cité in the heart of Paris,  France's capital city, lies one of the world's most beloved and historic  landmarks: the magnificent Notre Dame Cathedral. This Gothic masterpiece  has been welcoming pilgrims and tourists alike for over 850 years, since its  construction began in 1163 under King Louis VII. With its towering spires,  stunning stained glass windows, and intricate stone carvings, this beautiful  church is a testament to medieval architecture and engineering skill.  Unfortunately, a devastating fire ravaged the cathedral on April 15, 2019,  but thanks to swift action from firefighters and restoration efforts  underway, Notre Dame continues to inspire awe in those who visit her.\"  writeLines(document, con = \"doc1.txt\")  write_vectors_to_feather_file(file_name = \"doc1.feather\", vector_data = generate_document_embeddings(conn,  document_path = \"doc1.txt\", splitter = \"paragraph\"))  notre_dame_embeddings <- load_context_embeddings_from_feather_files(filenames = \"doc1.feather\")  my_workflow <- ai_workflow() |>  set_system_prompt(system_prompt=\"You are a helpful AI assistant.  Answer to the best of your knowledge.\") |> add_context(context_df = notre_dame_embeddings, use_context_mandatory=TRUE) #> Error in add_context(set_system_prompt(ai_workflow(), system_prompt = \"You are a helpful AI assistant. \\nAnswer to the best of your knowledge.\"),     context_df = notre_dame_embeddings, use_context_mandatory = TRUE): unused argument (use_context_mandatory = TRUE)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_tools_declaration.html","id":null,"dir":"Reference","previous_headings":"","what":"Add tools declaration for the LLM to use — add_tools_declaration","title":"Add tools declaration for the LLM to use — add_tools_declaration","text":"add_tools_declaration lets add tools (functions) can used needed LLM answer specific questions","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_tools_declaration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add tools declaration for the LLM to use — add_tools_declaration","text":"","code":"add_tools_declaration(workflow_obj, tools)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_tools_declaration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add tools declaration for the LLM to use — add_tools_declaration","text":"workflow_obj workflow object containing parameters describing workflow required tools list tools declared R list, see examples.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_tools_declaration.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add tools declaration for the LLM to use — add_tools_declaration","text":"Lets add tools (functions) can used LLM. works models tool calling function supported, case Llama3.1 example. structure expected R list.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_tools_declaration.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add tools declaration for the LLM to use — add_tools_declaration","text":"","code":"tool_list <- list(  list(type=\"function\",       \"function\"=list(         name=\"get_flight_times\",         description=\"get the flight times between two cities\",         parameters= list(           type=\"object\",           properties = list(             departure=list(               type=\"string\",               description=\"the departure city (airport code)\"             ),             arrival=list(               type=\"string\",               description=\"the arrival city (airport code)\"             )           ),           required=c(\"departure\", \"arrival\")         )       )))  myflow_test <- ai_workflow() |>    set_connector(\"ollama\")  |>     set_model(model_name= \"llama3.1:8b-instruct-q5_K_M\") |>    set_n_predict(1000) |>    set_temperature(0.8) |>     set_default_missing_parameters_in_workflow() |>     add_tools_declaration(tool_list) #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'."},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_vision_capability.html","id":null,"dir":"Reference","previous_headings":"","what":"Add vision capability to the workflow. — add_vision_capability","title":"Add vision capability to the workflow. — add_vision_capability","text":"add_vision_capability lets declare model can support description extraction information images. models support capabilities.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_vision_capability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add vision capability to the workflow. — add_vision_capability","text":"","code":"add_vision_capability(workflow_obj, max_image_dimension = NA)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_vision_capability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add vision capability to the workflow. — add_vision_capability","text":"workflow_obj workflow object containing parameters describing workflow required max_image_dimension numerical value (defaults 672 provided) defines largest dimension (width height) pictures sent model.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_vision_capability.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add vision capability to the workflow. — add_vision_capability","text":"Lets declare workflow can leverage vision capabilities (.e. can also send images top prompt, optionally). Make sure model include workflow vision capability first place. usually limited models like llava, moondream, llama3.2-11b models (probably ).","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_vision_capability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add vision capability to the workflow. — add_vision_capability","text":"","code":"myflow_test <- ai_workflow() |>    set_connector(\"ollama\")  |>     set_model(model_name= \"llama3.1:8b-instruct-q5_K_M\") |>    set_n_predict(1000) |>    set_temperature(0.8) |>     set_default_missing_parameters_in_workflow() |>     add_vision_capability() #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → No numerical max_image_dimension provided, will default to resizing all images to 672 pixels as max dimension."},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_workflow_step.html","id":null,"dir":"Reference","previous_headings":"","what":"Add a step (i.e. another workflow) to an existing workflow — add_workflow_step","title":"Add a step (i.e. another workflow) to an existing workflow — add_workflow_step","text":"add_workflow_step adds another workflow existing one. default chains new workflow previous one(s).","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_workflow_step.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add a step (i.e. another workflow) to an existing workflow — add_workflow_step","text":"","code":"add_workflow_step(workflow_obj, workflow_obj_to_add, type = \"chain\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_workflow_step.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add a step (i.e. another workflow) to an existing workflow — add_workflow_step","text":"workflow_obj previous workflow object want build workflow_obj_to_add workflow object want add top existing one type type step want add existing workflow. Defaults \"chain\".","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_workflow_step.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add a step (i.e. another workflow) to an existing workflow — add_workflow_step","text":"function add new workflow existing one. default way new workflow added chaining previous one. way works use previous output last workflow element input next one.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_workflow_step.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add a step (i.e. another workflow) to an existing workflow — add_workflow_step","text":"","code":"myflow_template <- ai_workflow() |>  set_connector(\"ollama\") |>   set_model(model_name= \"llama3.1:8b-instruct-q5_K_M\") |>    set_n_predict(1000) |>    set_temperature(0.8)  #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434."},{"path":"https://r-guyot.github.io/aiworkflow/reference/ai_workflow.html","id":null,"dir":"Reference","previous_headings":"","what":"Define AI workflow — ai_workflow","title":"Define AI workflow — ai_workflow","text":"ai_workflow creates list encapsulates parameters related AI workflow want design.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/ai_workflow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define AI workflow — ai_workflow","text":"","code":"ai_workflow()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/ai_workflow.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define AI workflow — ai_workflow","text":"empty list ai_workflow() format used capture settings parameters workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/ai_workflow.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Define AI workflow — ai_workflow","text":"starting point defining AI workflow package. workflow object contains connector model placeholders.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/ai_workflow.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define AI workflow — ai_workflow","text":"","code":"my_workflow <- ai_workflow()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/aiworkflow-package.html","id":null,"dir":"Reference","previous_headings":"","what":"aiworkflow: Enable R users to leverage AI frameworks like Ollama and Llama.cpp — aiworkflow-package","title":"aiworkflow: Enable R users to leverage AI frameworks like Ollama and Llama.cpp — aiworkflow-package","text":"ai_workflow R package makes easy run typical AI-powered (mostly LLM-related) tasks R. main focus Local AI support, can expanded third-party LLMs future.","code":""},{"path":[]},{"path":"https://r-guyot.github.io/aiworkflow/reference/aiworkflow-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"aiworkflow: Enable R users to leverage AI frameworks like Ollama and Llama.cpp — aiworkflow-package","text":"Maintainer: Raphael Guyot rguyot@sanqualis.com","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/apply_processing_skill.html","id":null,"dir":"Reference","previous_headings":"","what":"Applies a processing skill to the current workflow — apply_processing_skill","title":"Applies a processing skill to the current workflow — apply_processing_skill","text":"apply_processing_skill applies processing skill (.e. pre-engineered prompt)","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/apply_processing_skill.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Applies a processing skill to the current workflow — apply_processing_skill","text":"","code":"apply_processing_skill(   prompts_vector,   processing_skill = NA,   processing_skill_args = list() )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/apply_processing_skill.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Applies a processing skill to the current workflow — apply_processing_skill","text":"prompts_vector vector containing existing prompts text skill applied. processing_skill character vector containing filepath processing skill text template skill applied. processing_skill_args list additional parameters feed processing skill, accepts .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/apply_processing_skill.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Applies a processing skill to the current workflow — apply_processing_skill","text":"new vector containing modified prompts applying processing skill template.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/apply_processing_skill.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Applies a processing skill to the current workflow — apply_processing_skill","text":"simple function apply pre-defined prompt format (.e. processing skill) current workflow. can find existing processing skills using list_processing_skills() function. function usually made used , set_processing_skill() function","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_batch_documents_to_embeddings.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Batch documents to Embeddings — convert_batch_documents_to_embeddings","title":"Convert Batch documents to Embeddings — convert_batch_documents_to_embeddings","text":"convert_batch_documents_to_embeddings converts whole batch documents vector embeddings ","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_batch_documents_to_embeddings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Batch documents to Embeddings — convert_batch_documents_to_embeddings","text":"","code":"convert_batch_documents_to_embeddings(   ollama_connection,   document_path_list,   splitter = \"paragraph\",   model = \"bge-large\" )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_batch_documents_to_embeddings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Batch documents to Embeddings — convert_batch_documents_to_embeddings","text":"ollama_connection connector object ollama instance provide embeddings. document_path_list list containing full paths files need converted embeddings. splitter splitter type use preparing embeddings. Defaults \"paragraph\". \"sentence\" can also used. model model use creating embeddings. Defaults \"bge-large\"","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_batch_documents_to_embeddings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert Batch documents to Embeddings — convert_batch_documents_to_embeddings","text":"Lets convert many documents necessary targeting path list documents, can easily generated functions manually. returns dataframe end can used provide context LLM workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_batch_documents_to_embeddings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert Batch documents to Embeddings — convert_batch_documents_to_embeddings","text":"","code":"conn <- get_ollama_connection()  document <- \"Standing proudly on the Île de la Cité in the heart of Paris,  France's capital city, lies one of the world's most beloved and historic  landmarks: the magnificent Notre Dame Cathedral. This Gothic masterpiece  has been welcoming pilgrims and tourists alike for over 850 years, since its  construction began in 1163 under King Louis VII. With its towering spires,  stunning stained glass windows, and intricate stone carvings, this beautiful  church is a testament to medieval architecture and engineering skill.  Unfortunately, a devastating fire ravaged the cathedral on April 15, 2019,  but thanks to swift action from firefighters and restoration efforts  underway, Notre Dame continues to inspire awe in those who visit her.\"  writeLines(document, con = \"doc1.txt\")  convert_batch_documents_to_embeddings(ollama_connection = conn,  document_path_list = list(\"doc1.txt\")) #>      embeddings #>          <AsIs> #> 1: -0.02001.... #>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         text #>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       <char> #> 1: Standing proudly on the Île de la Cité in the heart of Paris,  France's capital city, lies one of the world's most beloved and historic  landmarks: the magnificent Notre Dame Cathedral. This Gothic masterpiece  has been welcoming pilgrims and tourists alike for over 850 years, since its  construction began in 1163 under King Louis VII. With its towering spires,  stunning stained glass windows, and intricate stone carvings, this beautiful  church is a testament to medieval architecture and engineering skill.  Unfortunately, a devastating fire ravaged the cathedral on April 15, 2019,  but thanks to swift action from firefighters and restoration efforts  underway, Notre Dame continues to inspire awe in those who visit her."},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_embeddings_to_qdrant_format.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert embeddings to Qdrant format — convert_embeddings_to_qdrant_format","title":"Convert embeddings to Qdrant format — convert_embeddings_to_qdrant_format","text":"convert_embeddings_to_qdrant_format converts embeddings expected qdrant format upserting. '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_embeddings_to_qdrant_format.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert embeddings to Qdrant format — convert_embeddings_to_qdrant_format","text":"","code":"convert_embeddings_to_qdrant_format(input)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_embeddings_to_qdrant_format.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert embeddings to Qdrant format — convert_embeddings_to_qdrant_format","text":"input embeddings resulting generate_document_embeddings() function.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_embeddings_to_qdrant_format.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert embeddings to Qdrant format — convert_embeddings_to_qdrant_format","text":"Embeddings generated generate_document_embeddings() function different structure. make embeddings directly usable qdrant function can use make seamless.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_embeddings_to_qdrant_format.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert embeddings to Qdrant format — convert_embeddings_to_qdrant_format","text":"","code":"sentence <- \"hi there this is a great day\" tmp_path <- tempfile() writeLines(sentence, tmp_path) conn <- get_ollama_connection() doc_embeddings <- generate_document_embeddings(ollama_connection = conn,document_path = tmp_path) embeddings_for_qdrant <- doc_embeddings |> convert_embeddings_to_qdrant_format()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_completion_response_to_tibble.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert an ollama server completion response to a tibble — convert_ollama_completion_response_to_tibble","title":"Convert an ollama server completion response to a tibble — convert_ollama_completion_response_to_tibble","text":"get_ollama_completion get completion ollama server API","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_completion_response_to_tibble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert an ollama server completion response to a tibble — convert_ollama_completion_response_to_tibble","text":"","code":"convert_ollama_completion_response_to_tibble(ollama_response)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_completion_response_to_tibble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert an ollama server completion response to a tibble — convert_ollama_completion_response_to_tibble","text":"ollama_response contain full response ollama API completion call.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_completion_response_to_tibble.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert an ollama server completion response to a tibble — convert_ollama_completion_response_to_tibble","text":"function convert full output ollama tibble format","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_model_info_response_to_tibble.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert ollama response for model info to a tibble — convert_ollama_model_info_response_to_tibble","title":"Convert ollama response for model info to a tibble — convert_ollama_model_info_response_to_tibble","text":"convert_ollama_model_info_response_to_tibble Converts ollama response model info tibble","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_model_info_response_to_tibble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert ollama response for model info to a tibble — convert_ollama_model_info_response_to_tibble","text":"","code":"convert_ollama_model_info_response_to_tibble(ollama_response)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_model_info_response_to_tibble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert ollama response for model info to a tibble — convert_ollama_model_info_response_to_tibble","text":"ollama_response response object coming payload delivered ollama server instance.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_model_info_response_to_tibble.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert ollama response for model info to a tibble — convert_ollama_model_info_response_to_tibble","text":"Converts ollama response model info tibble","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_tags_response_to_tibble.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert an ollama server tags response to a tibble — convert_ollama_tags_response_to_tibble","title":"Convert an ollama server tags response to a tibble — convert_ollama_tags_response_to_tibble","text":"convert_ollama_tags_response_to_tibble get completion ollama server API","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_tags_response_to_tibble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert an ollama server tags response to a tibble — convert_ollama_tags_response_to_tibble","text":"","code":"convert_ollama_tags_response_to_tibble(ollama_response)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_tags_response_to_tibble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert an ollama server tags response to a tibble — convert_ollama_tags_response_to_tibble","text":"ollama_response contain full response ollama API call tags.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_tags_response_to_tibble.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert an ollama server tags response to a tibble — convert_ollama_tags_response_to_tibble","text":"function convert full output ollama tibble format","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/display_intermediate_answer.html","id":null,"dir":"Reference","previous_headings":"","what":"Display Intermediate Answer — display_intermediate_answer","title":"Display Intermediate Answer — display_intermediate_answer","text":"display_intermediate_answer function displays results last executed workflow","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/display_intermediate_answer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Display Intermediate Answer — display_intermediate_answer","text":"","code":"display_intermediate_answer(workflow)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/display_intermediate_answer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Display Intermediate Answer — display_intermediate_answer","text":"workflow workflow object containing parameters describing flow required","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/display_intermediate_answer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Display Intermediate Answer — display_intermediate_answer","text":"function print output last workflow executed. Especially helpful chaining several workflows.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/execute_workflow.html","id":null,"dir":"Reference","previous_headings":"","what":"Execute an AI workflow — execute_workflow","title":"Execute an AI workflow — execute_workflow","text":"execute_workflow executes AI workflow combining prompt vectors workflow object.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/execute_workflow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Execute an AI workflow — execute_workflow","text":"","code":"execute_workflow(prompts_vector, images_vector = NA, workflow_obj)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/execute_workflow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Execute an AI workflow — execute_workflow","text":"prompts_vector vector containing prompts executed AI workflow images_vector optional vector containing images sent reference AI workflow (Defaults NA). needs length prompts_vector. workflow_obj workflow object containing parameters describing flow required","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/execute_workflow.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Execute an AI workflow — execute_workflow","text":"function executes AI workflow combining prompt vectors, potential images vector, workflow object.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/execute_workflow_on_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Execute an AI workflow on a dataframe (with or without a pipe) — execute_workflow_on_df","title":"Execute an AI workflow on a dataframe (with or without a pipe) — execute_workflow_on_df","text":"execute_workflow_on_df executes AI workflow applying workflow object input dataframe (tibble).","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/execute_workflow_on_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Execute an AI workflow on a dataframe (with or without a pipe) — execute_workflow_on_df","text":"","code":"execute_workflow_on_df(   df,   prompt_column_name = \"prompt\",   image_column_name = NA_character_,   workflow_obj,   result_column_name = \"result\",   auto_use_df_variables = F )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/execute_workflow_on_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Execute an AI workflow on a dataframe (with or without a pipe) — execute_workflow_on_df","text":"df source dataframe tibble use input workflow execute. prompt_column_name vector name column corresponds prompts send workflow. Defaults \"prompt\" image_column_name optional vector (defaults NA) corresponds images sent workflow. can work use model Vision capabilities capability turned . workflow_obj workflow object containing parameters describing flow required execution result_column_name vector name column corresponds outcome workflow. Defaults \"result\". new column created. auto_use_df_variables boolean value (defaults FALSE). TRUE, attempt find column names corresponds parameters can used modify workflow, temperature, n_ctx, etc...","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/execute_workflow_on_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Execute an AI workflow on a dataframe (with or without a pipe) — execute_workflow_on_df","text":"original df new column corresponding results workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/execute_workflow_on_df.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Execute an AI workflow on a dataframe (with or without a pipe) — execute_workflow_on_df","text":"function executes AI workflow combining prompt vectors workflow object. auto_use_df_variables set TRUE right columns tweak model every line, end different parameters applied workflow every line. parameters recognized currently columns : \"temperature\" \"n_predict\" \"seed\" \"num_ctx\" \"model\" \"overall_background\" \"system_prompt\" \"style_of_voice\" \"frequency_penalty\" \"presence_penalty\" \"repeat_penalty\" \"n_predict\"","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/execute_workflow_on_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Execute an AI workflow on a dataframe (with or without a pipe) — execute_workflow_on_df","text":"","code":"myflow <- ai_workflow() |>    set_connector(\"ollama\")  |>   set_model(model_name= \"llama3.2:3b-instruct-q5_K_M\") |>    set_n_predict(3000) |>    set_temperature(0.8) |>   set_seed(seed = 13) |>    set_num_ctx(num_ctx = 5000)  #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434.    df_test <- tibble::tribble(~prompt,~temperature,~seed,~system_prompt,~style_of_voice, \"why is the sky blue?\", 0.8,13123,NA_character_,\"Yoda\", \"why is the sky blue?\", 0.6,12321255,NA_character_,\"Tolkien\", \"why is the sky blue?\", 0.2,12,\"You are a expert in colorimetry.\",\"George R Martin\", \"why is the sky blue?\", 0,11111111,NA_character_,\"CS Lewis\", \"what is 2+2 ??\", 0.8,124,NA_character_,\"Hemingway\", \"what is 2+2 ??\", 0,34343,NA_character_,\"Gandalf\", \"what is tequila made of?\",0,34343,NA_character_,\"Obama\")  df_test <- df_test |>  execute_workflow_on_df(workflow_obj = myflow, auto_use_df_variables=T) #> → Will be using the following variables from the dataframe to supplement the workflow: temperature, seed, system_prompt, style_of_voice #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → Chat mode #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → Chat mode #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → Chat mode #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → Chat mode #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → Chat mode #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → Chat mode #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → Chat mode"},{"path":"https://r-guyot.github.io/aiworkflow/reference/extract_snippets.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Snippets — extract_snippets","title":"Extract Snippets — extract_snippets","text":"extract_snippets makes easy extract snippets generated answers LLM","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/extract_snippets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Snippets — extract_snippets","text":"","code":"extract_snippets(text)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/extract_snippets.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Snippets — extract_snippets","text":"text text extract snippets .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/extract_snippets.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Snippets — extract_snippets","text":"function makes easy extract snippets generated answers LLM","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/extract_snippets.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Snippets — extract_snippets","text":"","code":"llm_answer <- \"This is a simple function in r: ``` yo <- function(message) { print(message) } ``` Enjoy this function!\"  extracted_function <- llm_answer |> extract_snippets()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_document_embeddings.html","id":null,"dir":"Reference","previous_headings":"","what":"Get embeddings for a piece of context through an ollama server instance020 — generate_document_embeddings","title":"Get embeddings for a piece of context through an ollama server instance020 — generate_document_embeddings","text":"generate_document_embeddings Generates vector embeddings document ollama","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_document_embeddings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get embeddings for a piece of context through an ollama server instance020 — generate_document_embeddings","text":"","code":"generate_document_embeddings(   ollama_connection,   document_path,   splitter = \"sentence\",   model = \"bge-large\" )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_document_embeddings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get embeddings for a piece of context through an ollama server instance020 — generate_document_embeddings","text":"ollama_connection connection object information connect ollama server document_path document path create embeddings splitter type splitter going used. Defaults \"sentence\". \"paragraph\" can also used. model model used generates embeddings. Defaults \"bge-large\". can select model available generating embeddings.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_document_embeddings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get embeddings for a piece of context through an ollama server instance020 — generate_document_embeddings","text":"Converts ollama response model info tibble","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_document_embeddings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get embeddings for a piece of context through an ollama server instance020 — generate_document_embeddings","text":"","code":"#conn<-get_ollama_connection() #embeddings<- get_ollama_embeddings(ollama_connection=conn, prompt=\"Roma is a beautiful city.\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_numeric_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Numeric List — generate_numeric_list","title":"Generate Numeric List — generate_numeric_list","text":"generate_numeric_list generates random list simulate vector embeddings look like","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_numeric_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Numeric List — generate_numeric_list","text":"","code":"generate_numeric_list(decimals = 1, length)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_numeric_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Numeric List — generate_numeric_list","text":"decimals number decimals needed figure. Defaults 1. length length list want produce","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_numeric_list.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Numeric List — generate_numeric_list","text":"Generates list numerical values specific length. can used simulate vector embeddings look like.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_numeric_list.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Numeric List — generate_numeric_list","text":"","code":"generate_numeric_list(decimals=1, length=384) #> [[1]] #> [1] 0.1 #>  #> [[2]] #> [1] 0.8 #>  #> [[3]] #> [1] 0.6 #>  #> [[4]] #> [1] 0.2 #>  #> [[5]] #> [1] 0 #>  #> [[6]] #> [1] 0.5 #>  #> [[7]] #> [1] 0.5 #>  #> [[8]] #> [1] 0.3 #>  #> [[9]] #> [1] 0.7 #>  #> [[10]] #> [1] 0.8 #>  #> [[11]] #> [1] 0.9 #>  #> [[12]] #> [1] 0.2 #>  #> [[13]] #> [1] 0 #>  #> [[14]] #> [1] 0.3 #>  #> [[15]] #> [1] 0.4 #>  #> [[16]] #> [1] 0.2 #>  #> [[17]] #> [1] 0.4 #>  #> [[18]] #> [1] 0.1 #>  #> [[19]] #> [1] 0.4 #>  #> [[20]] #> [1] 1 #>  #> [[21]] #> [1] 0.3 #>  #> [[22]] #> [1] 0.7 #>  #> [[23]] #> [1] 0.7 #>  #> [[24]] #> [1] 0.2 #>  #> [[25]] #> [1] 1 #>  #> [[26]] #> [1] 0.7 #>  #> [[27]] #> [1] 0.1 #>  #> [[28]] #> [1] 0.5 #>  #> [[29]] #> [1] 0.7 #>  #> [[30]] #> [1] 0.7 #>  #> [[31]] #> [1] 0 #>  #> [[32]] #> [1] 0.2 #>  #> [[33]] #> [1] 0.3 #>  #> [[34]] #> [1] 0.6 #>  #> [[35]] #> [1] 0.5 #>  #> [[36]] #> [1] 0.4 #>  #> [[37]] #> [1] 0.7 #>  #> [[38]] #> [1] 0.9 #>  #> [[39]] #> [1] 0.2 #>  #> [[40]] #> [1] 0.2 #>  #> [[41]] #> [1] 0.7 #>  #> [[42]] #> [1] 0.5 #>  #> [[43]] #> [1] 0.6 #>  #> [[44]] #> [1] 0.7 #>  #> [[45]] #> [1] 0.1 #>  #> [[46]] #> [1] 0.8 #>  #> [[47]] #> [1] 0.8 #>  #> [[48]] #> [1] 1 #>  #> [[49]] #> [1] 1 #>  #> [[50]] #> [1] 0.4 #>  #> [[51]] #> [1] 0.5 #>  #> [[52]] #> [1] 0.3 #>  #> [[53]] #> [1] 0.2 #>  #> [[54]] #> [1] 0.5 #>  #> [[55]] #> [1] 0.5 #>  #> [[56]] #> [1] 0.8 #>  #> [[57]] #> [1] 0.2 #>  #> [[58]] #> [1] 0.7 #>  #> [[59]] #> [1] 0.1 #>  #> [[60]] #> [1] 0.4 #>  #> [[61]] #> [1] 0.8 #>  #> [[62]] #> [1] 0.3 #>  #> [[63]] #> [1] 0.6 #>  #> [[64]] #> [1] 0.3 #>  #> [[65]] #> [1] 0.6 #>  #> [[66]] #> [1] 0.2 #>  #> [[67]] #> [1] 0.9 #>  #> [[68]] #> [1] 0.5 #>  #> [[69]] #> [1] 0.5 #>  #> [[70]] #> [1] 0.3 #>  #> [[71]] #> [1] 0.4 #>  #> [[72]] #> [1] 0.4 #>  #> [[73]] #> [1] 0 #>  #> [[74]] #> [1] 0.5 #>  #> [[75]] #> [1] 0.4 #>  #> [[76]] #> [1] 0 #>  #> [[77]] #> [1] 0.4 #>  #> [[78]] #> [1] 0.6 #>  #> [[79]] #> [1] 0.9 #>  #> [[80]] #> [1] 0.4 #>  #> [[81]] #> [1] 0.5 #>  #> [[82]] #> [1] 0.6 #>  #> [[83]] #> [1] 0.3 #>  #> [[84]] #> [1] 0.3 #>  #> [[85]] #> [1] 0.5 #>  #> [[86]] #> [1] 0.9 #>  #> [[87]] #> [1] 0.4 #>  #> [[88]] #> [1] 0.2 #>  #> [[89]] #> [1] 0.7 #>  #> [[90]] #> [1] 0.1 #>  #> [[91]] #> [1] 1 #>  #> [[92]] #> [1] 0.1 #>  #> [[93]] #> [1] 0.5 #>  #> [[94]] #> [1] 0.8 #>  #> [[95]] #> [1] 0.7 #>  #> [[96]] #> [1] 0.2 #>  #> [[97]] #> [1] 0.5 #>  #> [[98]] #> [1] 0.8 #>  #> [[99]] #> [1] 0 #>  #> [[100]] #> [1] 0.5 #>  #> [[101]] #> [1] 0.8 #>  #> [[102]] #> [1] 0.8 #>  #> [[103]] #> [1] 0.4 #>  #> [[104]] #> [1] 0.2 #>  #> [[105]] #> [1] 0.4 #>  #> [[106]] #> [1] 0.7 #>  #> [[107]] #> [1] 0.5 #>  #> [[108]] #> [1] 0.7 #>  #> [[109]] #> [1] 0.5 #>  #> [[110]] #> [1] 0.8 #>  #> [[111]] #> [1] 0.7 #>  #> [[112]] #> [1] 0.9 #>  #> [[113]] #> [1] 0 #>  #> [[114]] #> [1] 0.9 #>  #> [[115]] #> [1] 1 #>  #> [[116]] #> [1] 0.5 #>  #> [[117]] #> [1] 0.4 #>  #> [[118]] #> [1] 0.8 #>  #> [[119]] #> [1] 0.6 #>  #> [[120]] #> [1] 0.6 #>  #> [[121]] #> [1] 0.9 #>  #> [[122]] #> [1] 0.6 #>  #> [[123]] #> [1] 0.3 #>  #> [[124]] #> [1] 0.9 #>  #> [[125]] #> [1] 0.9 #>  #> [[126]] #> [1] 0.2 #>  #> [[127]] #> [1] 0.4 #>  #> [[128]] #> [1] 0.8 #>  #> [[129]] #> [1] 0.1 #>  #> [[130]] #> [1] 0.2 #>  #> [[131]] #> [1] 0.2 #>  #> [[132]] #> [1] 0.7 #>  #> [[133]] #> [1] 0.9 #>  #> [[134]] #> [1] 0.9 #>  #> [[135]] #> [1] 0.6 #>  #> [[136]] #> [1] 0.6 #>  #> [[137]] #> [1] 0.7 #>  #> [[138]] #> [1] 0.2 #>  #> [[139]] #> [1] 0 #>  #> [[140]] #> [1] 0.9 #>  #> [[141]] #> [1] 0.1 #>  #> [[142]] #> [1] 1 #>  #> [[143]] #> [1] 0.6 #>  #> [[144]] #> [1] 0.2 #>  #> [[145]] #> [1] 0.9 #>  #> [[146]] #> [1] 0 #>  #> [[147]] #> [1] 0.6 #>  #> [[148]] #> [1] 0.2 #>  #> [[149]] #> [1] 0.9 #>  #> [[150]] #> [1] 0.8 #>  #> [[151]] #> [1] 0.7 #>  #> [[152]] #> [1] 0.2 #>  #> [[153]] #> [1] 0.7 #>  #> [[154]] #> [1] 0.2 #>  #> [[155]] #> [1] 0.5 #>  #> [[156]] #> [1] 0.3 #>  #> [[157]] #> [1] 0.6 #>  #> [[158]] #> [1] 0.3 #>  #> [[159]] #> [1] 0.5 #>  #> [[160]] #> [1] 0.9 #>  #> [[161]] #> [1] 1 #>  #> [[162]] #> [1] 0.8 #>  #> [[163]] #> [1] 0.9 #>  #> [[164]] #> [1] 0.6 #>  #> [[165]] #> [1] 0.8 #>  #> [[166]] #> [1] 1 #>  #> [[167]] #> [1] 0.7 #>  #> [[168]] #> [1] 0.2 #>  #> [[169]] #> [1] 0.3 #>  #> [[170]] #> [1] 0.7 #>  #> [[171]] #> [1] 0.9 #>  #> [[172]] #> [1] 0.2 #>  #> [[173]] #> [1] 0 #>  #> [[174]] #> [1] 0.9 #>  #> [[175]] #> [1] 0.4 #>  #> [[176]] #> [1] 0.8 #>  #> [[177]] #> [1] 0.7 #>  #> [[178]] #> [1] 0.4 #>  #> [[179]] #> [1] 0.4 #>  #> [[180]] #> [1] 0.7 #>  #> [[181]] #> [1] 0.7 #>  #> [[182]] #> [1] 0.1 #>  #> [[183]] #> [1] 0.9 #>  #> [[184]] #> [1] 0.1 #>  #> [[185]] #> [1] 0.9 #>  #> [[186]] #> [1] 0.1 #>  #> [[187]] #> [1] 0.5 #>  #> [[188]] #> [1] 0.2 #>  #> [[189]] #> [1] 0.7 #>  #> [[190]] #> [1] 0.2 #>  #> [[191]] #> [1] 1 #>  #> [[192]] #> [1] 0.3 #>  #> [[193]] #> [1] 0.4 #>  #> [[194]] #> [1] 0.9 #>  #> [[195]] #> [1] 0.8 #>  #> [[196]] #> [1] 0.1 #>  #> [[197]] #> [1] 0.9 #>  #> [[198]] #> [1] 0.8 #>  #> [[199]] #> [1] 0.4 #>  #> [[200]] #> [1] 1 #>  #> [[201]] #> [1] 0.3 #>  #> [[202]] #> [1] 0.9 #>  #> [[203]] #> [1] 0.2 #>  #> [[204]] #> [1] 0 #>  #> [[205]] #> [1] 0.1 #>  #> [[206]] #> [1] 0.9 #>  #> [[207]] #> [1] 0.2 #>  #> [[208]] #> [1] 0.2 #>  #> [[209]] #> [1] 0 #>  #> [[210]] #> [1] 0.9 #>  #> [[211]] #> [1] 0.2 #>  #> [[212]] #> [1] 0.8 #>  #> [[213]] #> [1] 0.3 #>  #> [[214]] #> [1] 0.9 #>  #> [[215]] #> [1] 0.3 #>  #> [[216]] #> [1] 1 #>  #> [[217]] #> [1] 0.6 #>  #> [[218]] #> [1] 0.8 #>  #> [[219]] #> [1] 0.8 #>  #> [[220]] #> [1] 0.8 #>  #> [[221]] #> [1] 0.4 #>  #> [[222]] #> [1] 0.1 #>  #> [[223]] #> [1] 0.1 #>  #> [[224]] #> [1] 0.7 #>  #> [[225]] #> [1] 0.6 #>  #> [[226]] #> [1] 0.7 #>  #> [[227]] #> [1] 0.3 #>  #> [[228]] #> [1] 1 #>  #> [[229]] #> [1] 0.7 #>  #> [[230]] #> [1] 0.9 #>  #> [[231]] #> [1] 1 #>  #> [[232]] #> [1] 0.2 #>  #> [[233]] #> [1] 0.7 #>  #> [[234]] #> [1] 0.4 #>  #> [[235]] #> [1] 0 #>  #> [[236]] #> [1] 0.6 #>  #> [[237]] #> [1] 0.6 #>  #> [[238]] #> [1] 0.4 #>  #> [[239]] #> [1] 0.9 #>  #> [[240]] #> [1] 0.5 #>  #> [[241]] #> [1] 1 #>  #> [[242]] #> [1] 0 #>  #> [[243]] #> [1] 0.7 #>  #> [[244]] #> [1] 0.4 #>  #> [[245]] #> [1] 0.9 #>  #> [[246]] #> [1] 0.7 #>  #> [[247]] #> [1] 0.7 #>  #> [[248]] #> [1] 0.8 #>  #> [[249]] #> [1] 0.5 #>  #> [[250]] #> [1] 0.2 #>  #> [[251]] #> [1] 0.5 #>  #> [[252]] #> [1] 0.4 #>  #> [[253]] #> [1] 0.7 #>  #> [[254]] #> [1] 0.6 #>  #> [[255]] #> [1] 0.4 #>  #> [[256]] #> [1] 1 #>  #> [[257]] #> [1] 0.3 #>  #> [[258]] #> [1] 0.1 #>  #> [[259]] #> [1] 0.6 #>  #> [[260]] #> [1] 0.2 #>  #> [[261]] #> [1] 0.1 #>  #> [[262]] #> [1] 0.9 #>  #> [[263]] #> [1] 0.8 #>  #> [[264]] #> [1] 0.8 #>  #> [[265]] #> [1] 0.5 #>  #> [[266]] #> [1] 0.5 #>  #> [[267]] #> [1] 0.1 #>  #> [[268]] #> [1] 0.4 #>  #> [[269]] #> [1] 0.2 #>  #> [[270]] #> [1] 0.7 #>  #> [[271]] #> [1] 0.7 #>  #> [[272]] #> [1] 0.9 #>  #> [[273]] #> [1] 0.2 #>  #> [[274]] #> [1] 1 #>  #> [[275]] #> [1] 0.4 #>  #> [[276]] #> [1] 0.7 #>  #> [[277]] #> [1] 0.8 #>  #> [[278]] #> [1] 0.1 #>  #> [[279]] #> [1] 0.5 #>  #> [[280]] #> [1] 0.8 #>  #> [[281]] #> [1] 0.4 #>  #> [[282]] #> [1] 0.6 #>  #> [[283]] #> [1] 0.2 #>  #> [[284]] #> [1] 0 #>  #> [[285]] #> [1] 1 #>  #> [[286]] #> [1] 0.2 #>  #> [[287]] #> [1] 0.8 #>  #> [[288]] #> [1] 0.9 #>  #> [[289]] #> [1] 0.8 #>  #> [[290]] #> [1] 0.6 #>  #> [[291]] #> [1] 0.9 #>  #> [[292]] #> [1] 0 #>  #> [[293]] #> [1] 0.1 #>  #> [[294]] #> [1] 0.1 #>  #> [[295]] #> [1] 0.7 #>  #> [[296]] #> [1] 0.4 #>  #> [[297]] #> [1] 0.1 #>  #> [[298]] #> [1] 0.1 #>  #> [[299]] #> [1] 0.9 #>  #> [[300]] #> [1] 0.2 #>  #> [[301]] #> [1] 0.7 #>  #> [[302]] #> [1] 0.2 #>  #> [[303]] #> [1] 0.5 #>  #> [[304]] #> [1] 0.8 #>  #> [[305]] #> [1] 0.1 #>  #> [[306]] #> [1] 0.3 #>  #> [[307]] #> [1] 0.3 #>  #> [[308]] #> [1] 0 #>  #> [[309]] #> [1] 0.9 #>  #> [[310]] #> [1] 0.4 #>  #> [[311]] #> [1] 0.8 #>  #> [[312]] #> [1] 0.7 #>  #> [[313]] #> [1] 0.5 #>  #> [[314]] #> [1] 1 #>  #> [[315]] #> [1] 0.4 #>  #> [[316]] #> [1] 0.2 #>  #> [[317]] #> [1] 0.8 #>  #> [[318]] #> [1] 0 #>  #> [[319]] #> [1] 0.8 #>  #> [[320]] #> [1] 0.2 #>  #> [[321]] #> [1] 0 #>  #> [[322]] #> [1] 0.8 #>  #> [[323]] #> [1] 0.8 #>  #> [[324]] #> [1] 1 #>  #> [[325]] #> [1] 0.4 #>  #> [[326]] #> [1] 0.2 #>  #> [[327]] #> [1] 0.6 #>  #> [[328]] #> [1] 0.8 #>  #> [[329]] #> [1] 0 #>  #> [[330]] #> [1] 0.7 #>  #> [[331]] #> [1] 0.2 #>  #> [[332]] #> [1] 0 #>  #> [[333]] #> [1] 0.1 #>  #> [[334]] #> [1] 0.7 #>  #> [[335]] #> [1] 0.6 #>  #> [[336]] #> [1] 0.3 #>  #> [[337]] #> [1] 0.4 #>  #> [[338]] #> [1] 0.7 #>  #> [[339]] #> [1] 0.7 #>  #> [[340]] #> [1] 0.9 #>  #> [[341]] #> [1] 0.5 #>  #> [[342]] #> [1] 0.6 #>  #> [[343]] #> [1] 0.2 #>  #> [[344]] #> [1] 0.6 #>  #> [[345]] #> [1] 0.9 #>  #> [[346]] #> [1] 0.6 #>  #> [[347]] #> [1] 0.8 #>  #> [[348]] #> [1] 0.6 #>  #> [[349]] #> [1] 0.8 #>  #> [[350]] #> [1] 0.4 #>  #> [[351]] #> [1] 0.8 #>  #> [[352]] #> [1] 0.7 #>  #> [[353]] #> [1] 0.3 #>  #> [[354]] #> [1] 0.1 #>  #> [[355]] #> [1] 0.1 #>  #> [[356]] #> [1] 0.8 #>  #> [[357]] #> [1] 0.4 #>  #> [[358]] #> [1] 0.1 #>  #> [[359]] #> [1] 1 #>  #> [[360]] #> [1] 0.6 #>  #> [[361]] #> [1] 0.1 #>  #> [[362]] #> [1] 0.5 #>  #> [[363]] #> [1] 0.1 #>  #> [[364]] #> [1] 1 #>  #> [[365]] #> [1] 0 #>  #> [[366]] #> [1] 0.2 #>  #> [[367]] #> [1] 0.9 #>  #> [[368]] #> [1] 1 #>  #> [[369]] #> [1] 0.7 #>  #> [[370]] #> [1] 0.9 #>  #> [[371]] #> [1] 1 #>  #> [[372]] #> [1] 0 #>  #> [[373]] #> [1] 0.4 #>  #> [[374]] #> [1] 0.6 #>  #> [[375]] #> [1] 1 #>  #> [[376]] #> [1] 0.3 #>  #> [[377]] #> [1] 0.9 #>  #> [[378]] #> [1] 0.5 #>  #> [[379]] #> [1] 0.9 #>  #> [[380]] #> [1] 0.5 #>  #> [[381]] #> [1] 0.9 #>  #> [[382]] #> [1] 0.9 #>  #> [[383]] #> [1] 0.3 #>  #> [[384]] #> [1] 0 #>"},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_uuid_from_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate UUID from text — generate_uuid_from_text","title":"Generate UUID from text — generate_uuid_from_text","text":"generate_uuid_from_text generates unique UUID given piece text.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_uuid_from_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate UUID from text — generate_uuid_from_text","text":"","code":"generate_uuid_from_text(text = NA_character_)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_uuid_from_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate UUID from text — generate_uuid_from_text","text":"text piece text generate UUID. Defaults NA.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_uuid_from_text.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate UUID from text — generate_uuid_from_text","text":"Generates UUID piece text. text give id. useful generate id inserting vectors qdrant.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_uuid_from_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate UUID from text — generate_uuid_from_text","text":"","code":"generate_uuid_from_text(\"hi there\") #> [1] \"9b96a1fe-1d54-4cbb-c960-cc6a0286668f\""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_list_ollama_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a list of models available from the ollama server — get_list_ollama_models","title":"Get a list of models available from the ollama server — get_list_ollama_models","text":"get_list_ollama_models gets list models available ollama server","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_list_ollama_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a list of models available from the ollama server — get_list_ollama_models","text":"","code":"get_list_ollama_models(ollama_connection)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_list_ollama_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a list of models available from the ollama server — get_list_ollama_models","text":"ollama_connection ollama connection objection object created get_ollama_connection().","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_list_ollama_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get a list of models available from the ollama server — get_list_ollama_models","text":"dataframe contains information LLM available running ollama server","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_list_ollama_models.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get a list of models available from the ollama server — get_list_ollama_models","text":"simple function get models available ollama server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_list_ollama_models.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get a list of models available from the ollama server — get_list_ollama_models","text":"","code":"ollama_conn <- get_ollama_connection() list_of_models <- get_list_ollama_models(ollama_conn) #> <error/tibble_error_incompatible_new_cols> #> Error in `tibble::add_column()`: #> ! New columns must be compatible with `.data`. #> ✖ New columns have 2 rows. #> ℹ `.data` has 6 rows. #> --- #> Backtrace: #>      ▆ #>   1. ├─base::tryCatch(...) #>   2. │ └─base (local) tryCatchList(expr, classes, parentenv, handlers) #>   3. │   ├─base (local) tryCatchOne(...) #>   4. │   │ └─base (local) doTryCatch(return(expr), name, parentenv, handler) #>   5. │   └─base (local) tryCatchList(expr, names[-nh], parentenv, handlers[-nh]) #>   6. │     └─base (local) tryCatchOne(expr, names, parentenv, handlers[[1L]]) #>   7. │       └─base (local) doTryCatch(return(expr), name, parentenv, handler) #>   8. ├─base::withCallingHandlers(...) #>   9. ├─base::saveRDS(...) #>  10. ├─base::do.call(...) #>  11. ├─base (local) `<fn>`(...) #>  12. └─global `<fn>`(...) #>  13.   └─pkgdown::build_site(...) #>  14.     └─pkgdown:::build_site_local(...) #>  15.       └─pkgdown::build_reference(...) #>  16.         ├─pkgdown:::unwrap_purrr_error(...) #>  17.         │ └─base::withCallingHandlers(...) #>  18.         └─purrr::map(...) #>  19.           └─purrr:::map_(\"list\", .x, .f, ..., .progress = .progress) #>  20.             ├─purrr:::with_indexed_errors(...) #>  21.             │ └─base::withCallingHandlers(...) #>  22.             ├─purrr:::call_with_cleanup(...) #>  23.             └─pkgdown (local) .f(.x[[i]], ...) #>  24.               ├─base::withCallingHandlers(...) #>  25.               └─pkgdown:::data_reference_topic(...) #>  26.                 └─pkgdown:::run_examples(...) #>  27.                   └─pkgdown:::highlight_examples(code, topic, env = env) #>  28.                     └─downlit::evaluate_and_highlight(...) #>  29.                       └─evaluate::evaluate(code, child_env(env), new_device = TRUE, output_handler = output_handler) #>  30.                         ├─base::withRestarts(...) #>  31.                         │ └─base (local) withRestartList(expr, restarts) #>  32.                         │   ├─base (local) withOneRestart(withRestartList(expr, restarts[-nr]), restarts[[nr]]) #>  33.                         │   │ └─base (local) doWithOneRestart(return(expr), restart) #>  34.                         │   └─base (local) withRestartList(expr, restarts[-nr]) #>  35.                         │     ├─base (local) withOneRestart(withRestartList(expr, restarts[-nr]), restarts[[nr]]) #>  36.                         │     │ └─base (local) doWithOneRestart(return(expr), restart) #>  37.                         │     └─base (local) withRestartList(expr, restarts[-nr]) #>  38.                         │       └─base (local) withOneRestart(expr, restarts[[1L]]) #>  39.                         │         └─base (local) doWithOneRestart(return(expr), restart) #>  40.                         ├─evaluate:::with_handlers(...) #>  41.                         │ ├─base::eval(call) #>  42.                         │ │ └─base::eval(call) #>  43.                         │ └─base::withCallingHandlers(...) #>  44.                         ├─base::withVisible(eval(expr, envir)) #>  45.                         └─base::eval(expr, envir) #>  46.                           └─base::eval(expr, envir) #>  47.                             └─aiworkflow::get_list_ollama_models(ollama_conn) #>  48.                               ├─base::tryCatch(...) #>  49.                               │ └─base (local) tryCatchList(expr, classes, parentenv, handlers) #>  50.                               │   └─base (local) tryCatchOne(expr, names, parentenv, handlers[[1L]]) #>  51.                               │     └─base (local) doTryCatch(return(expr), name, parentenv, handler) #>  52.                               └─aiworkflow:::convert_ollama_tags_response_to_tibble(httr2::resp_body_json(result)) #>  53.                                 ├─dplyr::distinct(...) #>  54.                                 └─tibble::add_column(dplyr::select(temp, -details), details)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_chat_completion.html","id":null,"dir":"Reference","previous_headings":"","what":"Get chat completion from ollama server — get_ollama_chat_completion","title":"Get chat completion from ollama server — get_ollama_chat_completion","text":"get_ollama_chat_completion get chat completion ollama server API","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_chat_completion.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get chat completion from ollama server — get_ollama_chat_completion","text":"","code":"get_ollama_chat_completion(   ollama_connection,   model,   embedding_model,   prompts_vector,   images_vector = NA,   output_text_only = F,   num_predict = 200,   temperature = 0.8,   role = \"user\",   repeat_penalty = 1.2,   seed = sample(1:1e+07, 1),   system_prompt = NA,   context_info = NA,   context_usage_mandatory = FALSE,   num_ctx = NA,   tools = NA,   vision = FALSE )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_chat_completion.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get chat completion from ollama server — get_ollama_chat_completion","text":"ollama_connection connection object information connect ollama server model Name model use ollama server. Note can use models available. embedding_model Name embedding model use ollama server create embeddings fly prompts. prompts_vector vector containing one Messages acting prompt completion. images_vector optional vector (defaults NA) containing images send model completion. output_text_only Boolean value (default False) indicating just want text message output (TRUE) whole response coming server. num_predict number tokens generate response (maximum amount). Defaults 200. temperature temperature value answer model. temperature 0 gives always answer. temperature 1 lot variation. Default 0.8. role role taken chat prompt. Typically \"user\", can also \"assistant\" \"system\". Defaults \"user\". repeat_penalty penalty give model avoid repeats tokens. Default 1.2. seed seed used generate answer. Note seed effect temperature zero. default random number 1 10000000 system_prompt system prompt used LLM. context_info used RAG, want provide context LLM ground answers. Currently incompatible template_prompt context_usage_mandatory boolean value (defaults FALSE) let LLM know use context answer, can use context optionally answer question. tools R list tools (.e. functions) can passed LLM. Note supported specific LLMs like Llama3.1 - may work LLMs trained tools calling.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_chat_completion.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get chat completion from ollama server — get_ollama_chat_completion","text":"function get chat completion ollama server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_chat_completion.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get chat completion from ollama server — get_ollama_chat_completion","text":"","code":"conn <- get_ollama_connection() get_ollama_chat_completion(ollama_connection=conn,  model+\"llama3:8b-instruct-q4_K_S\", prompts_vector=\"is the sky blue at night?\") #> <simpleError in eval(expr, envir): object 'model' not found>"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_completion.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a completion from ollama server — get_ollama_completion","title":"Get a completion from ollama server — get_ollama_completion","text":"get_ollama_completion get completion ollama server API","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_completion.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a completion from ollama server — get_ollama_completion","text":"","code":"get_ollama_completion(   ollama_connection,   model,   prompts_vector,   output_text_only = F,   num_predict = 200,   temperature = 0.8,   system_prompt = NA )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_completion.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a completion from ollama server — get_ollama_completion","text":"ollama_connection connection object information connect ollama server model Name model use ollama server. Note can use models available. prompts_vector vector containing one messages acting prompts completion. output_text_only boolean value (default False) indicating just want text message output (TRUE) whole response coming server. num_predict number tokens generate response (maximum amount) temperature temperature value answer model. temperature 0 gives always answer. temperature 1 lot variation. Default 0.8. system_prompt system prompt used LLM.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_completion.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get a completion from ollama server — get_ollama_completion","text":"simple function test connect ollama","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"Define a connection to a local ollama server — get_ollama_connection","title":"Define a connection to a local ollama server — get_ollama_connection","text":"get_ollama_connection sets variables used define connection ollama server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define a connection to a local ollama server — get_ollama_connection","text":"","code":"get_ollama_connection(ip_ad = \"127.0.0.1\", port = \"11434\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define a connection to a local ollama server — get_ollama_connection","text":"ip_ad IP address server running ollama. Default localhost 127.0.0.1. port port used run ollama service. Default 11374.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_connection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define a connection to a local ollama server — get_ollama_connection","text":"list contains information ollama connection.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_connection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Define a connection to a local ollama server — get_ollama_connection","text":"simple function set connection details ollama server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_connection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define a connection to a local ollama server — get_ollama_connection","text":"","code":"#get_ollama_connection(ip_ad=\"127.0.0.1\", port=\"11434\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_embeddings.html","id":null,"dir":"Reference","previous_headings":"","what":"Get embeddings for a piece of context through an ollama server instance — get_ollama_embeddings","title":"Get embeddings for a piece of context through an ollama server instance — get_ollama_embeddings","text":"get_ollama_embeddings Retrieves embeddings string (can series words, sentence, paragraph, whole document) ollama","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_embeddings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get embeddings for a piece of context through an ollama server instance — get_ollama_embeddings","text":"","code":"get_ollama_embeddings(ollama_connection, model = \"bge-large\", input)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_embeddings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get embeddings for a piece of context through an ollama server instance — get_ollama_embeddings","text":"ollama_connection connection object information connect ollama server model model used generates embeddings. Defaults \"-minilm\". can select model available generating embeddings. input single character vector contains text convert embeddings","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_embeddings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get embeddings for a piece of context through an ollama server instance — get_ollama_embeddings","text":"Converts ollama response model info tibble","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_embeddings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get embeddings for a piece of context through an ollama server instance — get_ollama_embeddings","text":"","code":"conn <-get_ollama_connection() embeddings <- get_ollama_embeddings(conn, input=\"Hi there, how are you doing?\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_model_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Get information about one ollama model — get_ollama_model_info","title":"Get information about one ollama model — get_ollama_model_info","text":"get_ollama_model_info Gets information one model available ollama.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_model_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get information about one ollama model — get_ollama_model_info","text":"","code":"get_ollama_model_info(ollama_connection, model)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_model_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get information about one ollama model — get_ollama_model_info","text":"ollama_connection connection object information connect Oollama server model name model want get info .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_model_info.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get information about one ollama model — get_ollama_model_info","text":"Gets information one model available ollama.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_model_info.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get information about one ollama model — get_ollama_model_info","text":"","code":"#ollama_conn <- get_ollama_connection() #get_ollama_model_info(ollama_connection= ollama_conn, model=\"llama3:8b-instruct-q5_0\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_qdrant_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Qdrant connection — get_qdrant_connection","title":"Get Qdrant connection — get_qdrant_connection","text":"get_qdrant_connection establishes connection qdrant instance. '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_qdrant_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Qdrant connection — get_qdrant_connection","text":"","code":"get_qdrant_connection(   endpoint = \"http://localhost\",   port = 6333,   api_key = NA_character_ )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_qdrant_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Qdrant connection — get_qdrant_connection","text":"endpoint URL pointing qdrant instance. Defaults http://localhost port port use connect qdrant instance. Defaults 6333 api_key optional security. Defaults NA. given, use API key connect Qdrant.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_qdrant_connection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Qdrant connection — get_qdrant_connection","text":"Creates connection object qdrant. using API key connect Qdrant, need specify api_key parameter.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_qdrant_connection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Qdrant connection — get_qdrant_connection","text":"","code":"conn <- get_qdrant_connection() #> → Connection to Qdrant confirmed"},{"path":"https://r-guyot.github.io/aiworkflow/reference/inspect_processing_skill.html","id":null,"dir":"Reference","previous_headings":"","what":"Inspect a specific processing skill — inspect_processing_skill","title":"Inspect a specific processing skill — inspect_processing_skill","text":"inspect_processing_skill lets inspect specific processing skill written.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/inspect_processing_skill.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inspect a specific processing skill — inspect_processing_skill","text":"","code":"inspect_processing_skill(processing_skill)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/inspect_processing_skill.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inspect a specific processing skill — inspect_processing_skill","text":"processing_skill name processing skill want model use specific task","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/inspect_processing_skill.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Inspect a specific processing skill — inspect_processing_skill","text":"text vector containing exact prompt used processing skill.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/inspect_processing_skill.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Inspect a specific processing skill — inspect_processing_skill","text":"outputs text contained processing skill. processing skill name looking found, throw error.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/inspect_processing_skill.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Inspect a specific processing skill — inspect_processing_skill","text":"","code":"inspect_processing_skill(\"break_down_tasks\") #> @SYSTEM #> The below text indicates a task or project that needs to be completed.  #> Your role is to break-down this objective into small, actionable tasks and sub-tasks (bullet point style), in a correct and logical order to be able to reach the final objective. #>  #> For example, if you were asked to build a surveillance camera, the planning you would output would look like this: #>  #> - Define Objectives and Requirements #>   - Determine the primary purpose (e.g., home security, traffic monitoring). #>   - Specify key features (e.g., resolution, night vision, motion detection). #> - Market research #>   - Analyze existing surveillance cameras. #>   - Identify market trends and user preferences. #>   - Study competitor products for features and pricing. #> - Establish technical feasibility #>   - Assess available technologies (e.g., camera sensors, processors, storage solutions). #>   - Identify potential challenges (e.g., low-light performance, connectivity). #> - Hardware Design #>   - Select camera sensor (e.g., CMOS, CCD). #>   - Choose a suitable processor (e.g., ARM, custom ASIC). #>   - Design or select an enclosure for the camera. #>   - Plan power supply options (e.g., battery, AC adapter). #> - Software Design #>   - Develop or choose an operating system (e.g., Linux-based). #>   - Plan firmware for camera control and image processing. #>   - Design user interface for configuration and monitoring (e.g., mobile app, web interface). #> - Connectivity and Integration: #>   - Decide on communication protocols (e.g., Wi-Fi, Ethernet, cellular). #>   - Plan integration with cloud storage or local storage options. #>   - Design APIs for third-party integrations (e.g., smart home systems).   #> - Prototyping #>   - Build initial hardware prototypes. #>   - Develop basic software functionality. #>   - Test individual components for performance and compatibility. #> - Software Development #>   - Implement camera control algorithms. #>   - Develop image processing and enhancement features. #>   - Create user interface and ensure it’s user-friendly. #>   - Integrate connectivity and storage features. #> - Hardware Development #>   - Finalize PCB design and layout. #>   - Manufacture prototype enclosures. #>   - Assemble and test complete hardware prototypes. #>  #>  #> @CHAT #> Below is the goal and objective we are trying to achieve now: #>  #> === #> {text_to_replace} #> === #>  #> Answer with the list of tasks broken down in logical order. #> Remember: Do not add extra comments! Just the tasks, broken down in bullet points."},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_global_functions.html","id":null,"dir":"Reference","previous_headings":"","what":"List global functions — list_global_functions","title":"List global functions — list_global_functions","text":"list_global_functions lists global functions available environment.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_global_functions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List global functions — list_global_functions","text":"","code":"list_global_functions()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_global_functions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List global functions — list_global_functions","text":"lists global functions available. useful check functions used tools calling present .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_global_functions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List global functions — list_global_functions","text":"","code":"list_global_functions() #> character(0)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_processing_skill_parameters.html","id":null,"dir":"Reference","previous_headings":"","what":"List extra parameters for a given processing skill — list_processing_skill_parameters","title":"List extra parameters for a given processing skill — list_processing_skill_parameters","text":"list_processing_skill_parameters returns extra parameters required processing skill.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_processing_skill_parameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List extra parameters for a given processing skill — list_processing_skill_parameters","text":"","code":"list_processing_skill_parameters(processing_skill)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_processing_skill_parameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List extra parameters for a given processing skill — list_processing_skill_parameters","text":"processing_skill name processing skill want model use specific task","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_processing_skill_parameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List extra parameters for a given processing skill — list_processing_skill_parameters","text":"text vector containing exact extra variables needed processing skill.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_processing_skill_parameters.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List extra parameters for a given processing skill — list_processing_skill_parameters","text":"outputs variable names need given extra parameters able use processing skill. extra variable, NA returned.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_processing_skill_parameters.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List extra parameters for a given processing skill — list_processing_skill_parameters","text":"","code":"list_processing_skill_parameters(\"translate\") #> [1] \"target_language\""},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_processing_skills.html","id":null,"dir":"Reference","previous_headings":"","what":"list the processing skills — list_processing_skills","title":"list the processing skills — list_processing_skills","text":"list_processing_skills lists processing skills (.e. prompts already defined) can use box package.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_processing_skills.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"list the processing skills — list_processing_skills","text":"","code":"list_processing_skills()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_processing_skills.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"list the processing skills — list_processing_skills","text":"list vectors describing skills available currently package.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_processing_skills.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"list the processing skills — list_processing_skills","text":"gives vector skills available use default. can used along set_processing_skill() function.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_processing_skills.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"list the processing skills — list_processing_skills","text":"","code":"list_processing_skills() #>  [1] \"add_code_comments\"              \"add_details\"                    #>  [3] \"add_emoji\"                      \"break_down_tasks\"               #>  [5] \"categorize_product_complaint\"   \"categorize_sentiment\"           #>  [7] \"chain_of_thought\"               \"clean_up_audio_transcription\"   #>  [9] \"extract_actions\"                \"extract_names\"                  #> [11] \"fix_copy\"                       \"fix_translation\"                #> [13] \"generate_code\"                  \"generate_creative_introduction\" #> [15] \"generate_knowledge_graph\"       \"generate_knowledge_graph_v2\"    #> [17] \"generate_quiz\"                  \"generate_quiz_with_answers\"     #> [19] \"identify_product\"               \"rewrite_active_voice\"           #> [21] \"rewrite_blog_copy\"              \"rewrite_bullet_points\"          #> [23] \"rewrite_casual\"                 \"rewrite_emoji\"                  #> [25] \"rewrite_in_first_person\"        \"rewrite_jargon\"                 #> [27] \"rewrite_paraphrase\"             \"rewrite_passive_voice\"          #> [29] \"rewrite_positive\"               \"rewrite_prompt\"                 #> [31] \"rewrite_text_as_anonymized\"     \"tldr\"                           #> [33] \"translate\"                      \"write_abstract\"                 #> [35] \"write_article_from_title\"       \"write_email\"                    #> [37] \"write_fiction_chapter\"          \"write_pitch\"                    #> [39] \"write_qa\"                       \"write_quarto_presentation\"      #> [41] \"write_recommendation\"           \"write_resume_narrative\"         #> [43] \"write_short_summary\"            \"write_text_expansion\"           #> [45] \"write_title\"                    \"write_tweet\""},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_context_embeddings_from_feather_files.html","id":null,"dir":"Reference","previous_headings":"","what":"Load Context Embeddings From Feather Files — load_context_embeddings_from_feather_files","title":"Load Context Embeddings From Feather Files — load_context_embeddings_from_feather_files","text":"load_context_embeddings_from_feather_files lets load vector data feather file","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_context_embeddings_from_feather_files.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load Context Embeddings From Feather Files — load_context_embeddings_from_feather_files","text":"","code":"load_context_embeddings_from_feather_files(filenames)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_context_embeddings_from_feather_files.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load Context Embeddings From Feather Files — load_context_embeddings_from_feather_files","text":"filenames list feather filenames contain vector embeddings text information","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_context_embeddings_from_feather_files.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load Context Embeddings From Feather Files — load_context_embeddings_from_feather_files","text":"vector data contained feather file.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_context_embeddings_from_feather_files.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Load Context Embeddings From Feather Files — load_context_embeddings_from_feather_files","text":"function provides simple way load vector data (embeddings) binary feather file, instead using regular databases.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_context_embeddings_from_feather_files.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load Context Embeddings From Feather Files — load_context_embeddings_from_feather_files","text":"","code":"conn <- get_ollama_connection()  document <- \"Standing proudly on the Île de la Cité in the heart of Paris,  France's capital city, lies one of the world's most beloved and historic  landmarks: the magnificent Notre Dame Cathedral. This Gothic masterpiece  has been welcoming pilgrims and tourists alike for over 850 years, since its  construction began in 1163 under King Louis VII. With its towering spires,  stunning stained glass windows, and intricate stone carvings, this beautiful  church is a testament to medieval architecture and engineering skill.  Unfortunately, a devastating fire ravaged the cathedral on April 15, 2019,  but thanks to swift action from firefighters and restoration efforts  underway, Notre Dame continues to inspire awe in those who visit her.\"  writeLines(document, con = \"doc1.txt\")  write_vectors_to_feather_file(file_name = \"doc1.feather\", vector_data = generate_document_embeddings(conn,  document_path = \"doc1.txt\", splitter = \"paragraph\"))  load_context_embeddings_from_feather_files(filenames = \"doc1.feather\") #>      embeddings #>          <AsIs> #> 1: -0.02001.... #>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         text #>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       <char> #> 1: Standing proudly on the Île de la Cité in the heart of Paris,  France's capital city, lies one of the world's most beloved and historic  landmarks: the magnificent Notre Dame Cathedral. This Gothic masterpiece  has been welcoming pilgrims and tourists alike for over 850 years, since its  construction began in 1163 under King Louis VII. With its towering spires,  stunning stained glass windows, and intricate stone carvings, this beautiful  church is a testament to medieval architecture and engineering skill.  Unfortunately, a devastating fire ravaged the cathedral on April 15, 2019,  but thanks to swift action from firefighters and restoration efforts  underway, Notre Dame continues to inspire awe in those who visit her."},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_workflow.html","id":null,"dir":"Reference","previous_headings":"","what":"Load workflow — load_workflow","title":"Load workflow — load_workflow","text":"load_workflow makes possible reload workflow saved configuration file JSON.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_workflow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load workflow — load_workflow","text":"","code":"load_workflow(filepath)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_workflow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load workflow — load_workflow","text":"filepath filepath JSON file contains parameters workflow, created save_workflow().","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_workflow.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load workflow — load_workflow","text":"workflow loading settings parameters.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_workflow.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Load workflow — load_workflow","text":"function reload workflow specific JSON file created save_workflow_settings().","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_workflow.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load workflow — load_workflow","text":"","code":"my_workflow <- load_workflow(filepath=\"myworkflow.json\") #> → The workflow has been successfully reloaded from myworkflow.json."},{"path":"https://r-guyot.github.io/aiworkflow/reference/make_cosine_similarity_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Make Cosine Similarity Matrix — make_cosine_similarity_matrix","title":"Make Cosine Similarity Matrix — make_cosine_similarity_matrix","text":"make_cosine_similarity_matrix lets calculate cosine similarity given matrix.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/make_cosine_similarity_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make Cosine Similarity Matrix — make_cosine_similarity_matrix","text":"","code":"make_cosine_similarity_matrix(input_matrix)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/make_cosine_similarity_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make Cosine Similarity Matrix — make_cosine_similarity_matrix","text":"input_matrix input matrix used derive cosine similarity matrix end","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/make_cosine_similarity_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make Cosine Similarity Matrix — make_cosine_similarity_matrix","text":"cosine similarity matrix","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/make_cosine_similarity_matrix.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Make Cosine Similarity Matrix — make_cosine_similarity_matrix","text":"Calculate cosine similarity matrix.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/make_cosine_similarity_matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make Cosine Similarity Matrix — make_cosine_similarity_matrix","text":"","code":"conn <- get_ollama_connection()  document <- \"Standing proudly on the Île de la Cité in the heart of Paris,  France's capital city, lies one of the world's most beloved and historic  landmarks: the magnificent Notre Dame Cathedral. This Gothic masterpiece  has been welcoming pilgrims and tourists alike for over 850 years, since its  construction began in 1163 under King Louis VII. With its towering spires,  stunning stained glass windows, and intricate stone carvings, this beautiful  church is a testament to medieval architecture and engineering skill.  Unfortunately, a devastating fire ravaged the cathedral on April 15, 2019,  but thanks to swift action from firefighters and restoration efforts  underway, Notre Dame continues to inspire awe in those who visit her.\"  writeLines(document, con = \"doc1.txt\")  context_df <- convert_batch_documents_to_embeddings(ollama_connection = conn,                                        document_path_list = list(\"doc1.txt\"))  prompt <- \"When was Notre Dame in Paris built?\" prompt_vector <- get_ollama_embeddings(ollama_connection = conn, input =  prompt)  whole_list <- c(list(prompt_vector), context_df$embeddings) mat <- do.call(rbind,whole_list) cos_sim_mat <- make_cosine_similarity_matrix(mat)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/parse_json_result.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse JSON answer from the LLM — parse_json_result","title":"Parse JSON answer from the LLM — parse_json_result","text":"parse_json_result attempts parse JSON result LLM","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/parse_json_result.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse JSON answer from the LLM — parse_json_result","text":"","code":"parse_json_result(json_string)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/parse_json_result.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse JSON answer from the LLM — parse_json_result","text":"json_string JSON string want parse","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/parse_json_result.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Parse JSON answer from the LLM — parse_json_result","text":"function assume result LLM provided JSON format format correct, parse result R object. typically expect format used request_json_answer() function. typically used pipe, pull_final_answer().","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/parse_json_result.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parse JSON answer from the LLM — parse_json_result","text":"","code":"myflow_template <- ai_workflow() |>  set_connector(\"ollama\")  |>    set_model(model_name= \"llama3.1:8b-instruct-q5_K_M\") |>    set_n_predict(1000) |>    set_temperature(0.8) |>    request_json_answer() #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434.    myflow_template |>  process_prompts(\"what is the usual color of the sky on Earth?\") |> pull_final_answer() |> parse_json_result() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> $answer #> [1] \"blue\" #>"},{"path":"https://r-guyot.github.io/aiworkflow/reference/process_prompts.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Prompts starting from a workflow — process_prompts","title":"Process Prompts starting from a workflow — process_prompts","text":"process_prompts way process vector prompts starting workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/process_prompts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Prompts starting from a workflow — process_prompts","text":"","code":"process_prompts(workflow_obj, prompts_vector, images_vector = NA)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/process_prompts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Prompts starting from a workflow — process_prompts","text":"workflow_obj workflow object containing parameters describing flow required prompts_vector vector containing prompts executed AI workflow images_vector optional vector (defaults NA) containing images send AI workflow","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/process_prompts.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Process Prompts starting from a workflow — process_prompts","text":"function provides way process vector prompts starting workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/pull_final_answer.html","id":null,"dir":"Reference","previous_headings":"","what":"Pull Final Answer — pull_final_answer","title":"Pull Final Answer — pull_final_answer","text":"pull_final_answer function extract final answer series workflows","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/pull_final_answer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pull Final Answer — pull_final_answer","text":"","code":"pull_final_answer(workflow)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/pull_final_answer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pull Final Answer — pull_final_answer","text":"workflow workflow object containing parameters describing flow required","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/pull_final_answer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Pull Final Answer — pull_final_answer","text":"function extract final text result usually coming series workflows.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_collection_existence.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: Check collection existence — qdrant_check_collection_existence","title":"Qdrant: Check collection existence — qdrant_check_collection_existence","text":"qdrant_check_collection_existence establishes connection qdrant instance checks collection exists. '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_collection_existence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: Check collection existence — qdrant_check_collection_existence","text":"","code":"qdrant_check_collection_existence(conn, collection_name = NA_character_)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_collection_existence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: Check collection existence — qdrant_check_collection_existence","text":"conn connection object created get_qdrant_connection() collection_name name collection want check existence. Defaults NA.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_collection_existence.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: Check collection existence — qdrant_check_collection_existence","text":"Confirms existence qdrant collection. Returns TRUE FALSE","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_collection_existence.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: Check collection existence — qdrant_check_collection_existence","text":"","code":"conn <- get_qdrant_connection() #> → Connection to Qdrant confirmed qdrant_check_collection_existence(conn, collection_name=\"hi_there\") #> [1] FALSE"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_connection_validity.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: Check if the Connection is valid — qdrant_check_connection_validity","title":"Qdrant: Check if the Connection is valid — qdrant_check_connection_validity","text":"qdrant_check_connection_validity checks qdrant connection proposed valid. '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_connection_validity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: Check if the Connection is valid — qdrant_check_connection_validity","text":"","code":"qdrant_check_connection_validity(conn, silent = TRUE)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_connection_validity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: Check if the Connection is valid — qdrant_check_connection_validity","text":"conn connection object created get_qdrant_connection() silent boolean, defaults TRUE. FALSE, prints message connection established.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_connection_validity.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: Check if the Connection is valid — qdrant_check_connection_validity","text":"simple test confirm qdrant connection working expected.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_connection_validity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: Check if the Connection is valid — qdrant_check_connection_validity","text":"","code":"conn <- get_qdrant_connection() #> → Connection to Qdrant confirmed qdrant_check_connection_validity(conn,silent=FALSE) #> → Connection to Qdrant confirmed #> [1] TRUE"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_create_new_collection.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: Create new collection — qdrant_create_new_collection","title":"Qdrant: Create new collection — qdrant_create_new_collection","text":"qdrant_create_new_collection establishes connection qdrant instance creates new collection. '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_create_new_collection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: Create new collection — qdrant_create_new_collection","text":"","code":"qdrant_create_new_collection(   conn,   collection_name = NA_character_,   vectors = list(size = 384, distance = \"Cosine\") )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_create_new_collection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: Create new collection — qdrant_create_new_collection","text":"conn connection object created get_qdrant_connection() collection_name name collection want create. Defaults NA. vectors R list object needs contain 'size' 'distance'. Defaults size=384 distance='Cosine' needs adapted embedding model use. Different models different resulting vector sizes.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_create_new_collection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: Create new collection — qdrant_create_new_collection","text":"Creates new collection vector embeddings. can customize size embeddings needed type distance similarity search. collection already exists name throw error.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_create_new_collection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: Create new collection — qdrant_create_new_collection","text":"","code":"conn <- get_qdrant_connection() #> → Connection to Qdrant confirmed qdrant_create_new_collection(conn, collection_name=\"story_of_my_life\") #> → Collection story_of_my_life was just created on the qdrant instance. qdrant_delete_collection(conn, collection_name=\"story_of_my_life\") #> → Collection story_of_my_life was just deleted on the qdrant instance."},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_collection.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: Delete collection — qdrant_delete_collection","title":"Qdrant: Delete collection — qdrant_delete_collection","text":"qdrant_delete_collection establishes connection qdrant instance deletes existing collection. '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_collection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: Delete collection — qdrant_delete_collection","text":"","code":"qdrant_delete_collection(conn, collection_name = NA_character_)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_collection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: Delete collection — qdrant_delete_collection","text":"conn connection object created get_qdrant_connection() collection_name name collection want delete. Defaults NA.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_collection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: Delete collection — qdrant_delete_collection","text":"Deletes existing collection qdrant. collection exists, throw error. deletion successful display confirmation message","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_collection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: Delete collection — qdrant_delete_collection","text":"","code":"conn <- get_qdrant_connection() #> → Connection to Qdrant confirmed qdrant_create_new_collection(conn, collection_name=\"story_of_alice\") #> → Collection story_of_alice was just created on the qdrant instance. qdrant_delete_collection(conn, collection_name=\"story_of_alice\") #> → Collection story_of_alice was just deleted on the qdrant instance."},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_points.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: Delete points (vectors) — qdrant_delete_points","title":"Qdrant: Delete points (vectors) — qdrant_delete_points","text":"qdrant_delete_points deletes specific vectors (points) '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_points.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: Delete points (vectors) — qdrant_delete_points","text":"","code":"qdrant_delete_points(conn, collection_name = NA_character_, ids = list())"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_points.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: Delete points (vectors) — qdrant_delete_points","text":"conn connection object created get_qdrant_connection() collection_name name collection want use. Defaults NA. ids list ids delete. Defaults empty list. value \"ids\" needs format: list(1,2,3) want delete vectors 1,2 3.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_points.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: Delete points (vectors) — qdrant_delete_points","text":"delete specific vectors based ids.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_points.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: Delete points (vectors) — qdrant_delete_points","text":"","code":"conn <- get_qdrant_connection() #> → Connection to Qdrant confirmed qdrant_create_new_collection(conn, collection_name=\"test_db\", vectors=list(size=3,distance=\"Cosine\")) #> → Collection test_db was just created on the qdrant instance. new_vectors <- list(points=list( list(id=1, payload=list(text=\"hi there\"),vector=list(0.1,0.5,0.6)), list(id=2, payload=list(text=\"well well well\"),vector=list(0.6,0.1,0.3)) )) qdrant_upsert_points(conn, points=new_vectors,collection_name=\"test_db\") #> → Vectors were added in the test_db collection on the qdrant instance. qdrant_delete_points(conn, collection_name=\"test_db\", ids=list(1,2)) #> → Successfully deleted points. qdrant_delete_collection(conn, \"test_db\") #> → Collection test_db was just deleted on the qdrant instance."},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_get_collection_details.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: Get collection details — qdrant_get_collection_details","title":"Qdrant: Get collection details — qdrant_get_collection_details","text":"qdrant_get_collection_details establishes connection qdrant instance returns details specific collection. '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_get_collection_details.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: Get collection details — qdrant_get_collection_details","text":"","code":"qdrant_get_collection_details(conn, collection_name = NA_character_)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_get_collection_details.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: Get collection details — qdrant_get_collection_details","text":"conn connection object created get_qdrant_connection() collection_name name collection want check details. Defaults NA.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_get_collection_details.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: Get collection details — qdrant_get_collection_details","text":"Returns details specific qdrant collection.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_get_collection_details.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: Get collection details — qdrant_get_collection_details","text":"","code":"conn <- get_qdrant_connection() #> → Connection to Qdrant confirmed qdrant_create_new_collection(conn, collection_name=\"story_of_my_life\") #> → Collection story_of_my_life was just created on the qdrant instance. qdrant_get_collection_details(conn, collection_name=\"story_of_my_life\") #> $result #> $result$status #> [1] \"green\" #>  #> $result$optimizer_status #> [1] \"ok\" #>  #> $result$indexed_vectors_count #> [1] 0 #>  #> $result$points_count #> [1] 0 #>  #> $result$segments_count #> [1] 8 #>  #> $result$config #> $result$config$params #> $result$config$params$vectors #> $result$config$params$vectors$size #> [1] 384 #>  #> $result$config$params$vectors$distance #> [1] \"Cosine\" #>  #>  #> $result$config$params$shard_number #> [1] 1 #>  #> $result$config$params$replication_factor #> [1] 1 #>  #> $result$config$params$write_consistency_factor #> [1] 1 #>  #> $result$config$params$on_disk_payload #> [1] TRUE #>  #>  #> $result$config$hnsw_config #> $result$config$hnsw_config$m #> [1] 16 #>  #> $result$config$hnsw_config$ef_construct #> [1] 100 #>  #> $result$config$hnsw_config$full_scan_threshold #> [1] 10000 #>  #> $result$config$hnsw_config$max_indexing_threads #> [1] 0 #>  #> $result$config$hnsw_config$on_disk #> [1] FALSE #>  #>  #> $result$config$optimizer_config #> $result$config$optimizer_config$deleted_threshold #> [1] 0.2 #>  #> $result$config$optimizer_config$vacuum_min_vector_number #> [1] 1000 #>  #> $result$config$optimizer_config$default_segment_number #> [1] 0 #>  #> $result$config$optimizer_config$max_segment_size #> NULL #>  #> $result$config$optimizer_config$memmap_threshold #> NULL #>  #> $result$config$optimizer_config$indexing_threshold #> [1] 20000 #>  #> $result$config$optimizer_config$flush_interval_sec #> [1] 5 #>  #> $result$config$optimizer_config$max_optimization_threads #> NULL #>  #>  #> $result$config$wal_config #> $result$config$wal_config$wal_capacity_mb #> [1] 32 #>  #> $result$config$wal_config$wal_segments_ahead #> [1] 0 #>  #>  #> $result$config$quantization_config #> NULL #>  #>  #> $result$payload_schema #> named list() #>  #>  #> $status #> [1] \"ok\" #>  #> $time #> [1] 0.001256239 #>  qdrant_delete_collection(conn, collection_name=\"story_of_my_life\") #> → Collection story_of_my_life was just deleted on the qdrant instance."},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_list_all_collections.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: List all collections — qdrant_list_all_collections","title":"Qdrant: List all collections — qdrant_list_all_collections","text":"qdrant_list_all_collections establishes connection qdrant instance lists collections. '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_list_all_collections.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: List all collections — qdrant_list_all_collections","text":"","code":"qdrant_list_all_collections(conn)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_list_all_collections.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: List all collections — qdrant_list_all_collections","text":"conn connection object created get_qdrant_connection()","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_list_all_collections.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: List all collections — qdrant_list_all_collections","text":"Lists collections available qdrant instance. collection can different size vector embeddings","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_list_all_collections.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: List all collections — qdrant_list_all_collections","text":"","code":"conn <- get_qdrant_connection() #> → Connection to Qdrant confirmed qdrant_list_all_collections(conn) #> $result #> $result$collections #> list() #>  #>  #> $status #> [1] \"ok\" #>  #> $time #> [1] 2.505e-06 #>"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_retrieve_point.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: Retrieve a specific point (vector) — qdrant_retrieve_point","title":"Qdrant: Retrieve a specific point (vector) — qdrant_retrieve_point","text":"qdrant_retrieve_point retrieves specific vector (point) '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_retrieve_point.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: Retrieve a specific point (vector) — qdrant_retrieve_point","text":"","code":"qdrant_retrieve_point(   conn,   collection_name = NA_character_,   id = NA_character_ )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_retrieve_point.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: Retrieve a specific point (vector) — qdrant_retrieve_point","text":"conn connection object created get_qdrant_connection() collection_name name collection want use. Defaults NA. id id point retrieve. Defaults NA.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_retrieve_point.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: Retrieve a specific point (vector) — qdrant_retrieve_point","text":"Retrieves value specific vector/point, based id. return list info available qdrant.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_retrieve_point.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: Retrieve a specific point (vector) — qdrant_retrieve_point","text":"","code":"conn <- get_qdrant_connection() #> → Connection to Qdrant confirmed qdrant_create_new_collection(conn, collection_name=\"test_db\", vectors=list(size=3,distance=\"Cosine\")) #> → Collection test_db was just created on the qdrant instance. new_vectors <- list(points=list(list(id=1, payload=list(text=\"hi there\"),                       vector=list(0.1,0.5,0.6)))) qdrant_upsert_points(conn, points=new_vectors,collection_name=\"test_db\",generate_id=FALSE) #> → Vectors were added in the test_db collection on the qdrant instance. qdrant_retrieve_point(conn, collection_name=\"test_db\", id=1) #> $result #> $result$id #> [1] 1 #>  #> $result$payload #> $result$payload$text #> [1] \"hi there\" #>  #>  #> $result$vector #> $result$vector[[1]] #> [1] 0.1270001 #>  #> $result$vector[[2]] #> [1] 0.6350007 #>  #> $result$vector[[3]] #> [1] 0.7620008 #>  #>  #>  #> $status #> [1] \"ok\" #>  #> $time #> [1] 0.003804432 #>  qdrant_delete_collection(conn, \"test_db\") #> → Collection test_db was just deleted on the qdrant instance."},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_search_points.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: Search points (vectors) — qdrant_search_points","title":"Qdrant: Search points (vectors) — qdrant_search_points","text":"qdrant_search_points searches similar vectors (points) '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_search_points.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: Search points (vectors) — qdrant_search_points","text":"","code":"qdrant_search_points(   conn,   collection_name = NA_character_,   vector = list(),   limit = 5,   with_payload = TRUE,   with_vector = FALSE,   score_threshold = NA_real_ )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_search_points.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: Search points (vectors) — qdrant_search_points","text":"conn connection object created get_qdrant_connection() collection_name name collection want use. Defaults NA. vector vector embedding values used initiate search. Defaults empty list. Note vector simply list containing numerical values embeddings. use embedding size 384, simple list 384 numerical values . limit maximum amount vectors return search. Defaults 5. with_payload TRUE, display payload resulting vectors. Defaults TRUE. with_vector TRUE, display full vector values results. Defaults FALSE. score_threshold threshold minimum similarity score return results. Defaults NA.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_search_points.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: Search points (vectors) — qdrant_search_points","text":"run search similar vectors qdrant.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_search_points.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: Search points (vectors) — qdrant_search_points","text":"","code":"conn <- get_qdrant_connection() #> → Connection to Qdrant confirmed qdrant_create_new_collection(conn, collection_name=\"test_db\", vectors=list(size=3,distance=\"Cosine\")) #> → Collection test_db was just created on the qdrant instance. new_vectors <- list(points=list( list(id=1, payload=list(text=\"hi there\"),vector=list(0.1,0.5,0.6)), list(id=2, payload=list(text=\"well well well\"),vector=list(0.6,0.1,0.3)) )) qdrant_upsert_points(conn, points=new_vectors, collection_name=\"test_db\") #> → Vectors were added in the test_db collection on the qdrant instance. qdrant_search_points(conn, collection_name=\"test_db\", vector=list(0.2,0.5,0.6)) #> $result #> $result[[1]] #> $result[[1]]$id #> [1] \"9b96a1fe-1d54-4cbb-c960-cc6a0286668f\" #>  #> $result[[1]]$version #> [1] 0 #>  #> $result[[1]]$score #> [1] 0.9924029 #>  #> $result[[1]]$payload #> $result[[1]]$payload$text #> [1] \"hi there\" #>  #>  #>  #> $result[[2]] #> $result[[2]]$id #> [1] \"005ae072-9959-484d-19c5-b26b61200dd0\" #>  #> $result[[2]]$version #> [1] 0 #>  #> $result[[2]]$score #> [1] 0.6400773 #>  #> $result[[2]]$payload #> $result[[2]]$payload$text #> [1] \"well well well\" #>  #>  #>  #>  #> $status #> [1] \"ok\" #>  #> $time #> [1] 0.005521686 #>  qdrant_delete_collection(conn, \"test_db\") #> → Collection test_db was just deleted on the qdrant instance."},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_upsert_points.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: Upsert points — qdrant_upsert_points","title":"Qdrant: Upsert points — qdrant_upsert_points","text":"qdrant_upsert_points establishes connection qdrant instance insert points (vector) collection. '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_upsert_points.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: Upsert points — qdrant_upsert_points","text":"","code":"qdrant_upsert_points(   conn,   points = list(),   collection_name,   generate_id = TRUE )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_upsert_points.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: Upsert points — qdrant_upsert_points","text":"conn connection object created get_qdrant_connection() points list containing vector embeddings. specific format expected. format expected (example vector size 3): list(points=list(list(id=1, payload=list(text=\"hi \"), vector=list(0.1,0.5,0.6)))) Note 'payload' can contain lot data 'text'. can add variables needed within payload list. collection_name name collection want use. Defaults NA. generate_id boolean, defaults TRUE. TRUE, automatically create unique UUID vector, needed qdrant. use , provide id .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_upsert_points.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: Upsert points — qdrant_upsert_points","text":"Inserts vector entries existing collection.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_upsert_points.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: Upsert points — qdrant_upsert_points","text":"","code":"conn <- get_qdrant_connection() #> → Connection to Qdrant confirmed qdrant_create_new_collection(conn, collection_name=\"test_db\", vectors=list(size=3,distance=\"Cosine\")) #> → Collection test_db was just created on the qdrant instance. new_vectors <- list(points=list(list(payload=list(text=\"hi there\"),                       vector=list(0.1,0.5,0.6)))) qdrant_upsert_points(conn, points=new_vectors, collection_name=\"test_db\",generate_id=TRUE) #> → Vectors were added in the test_db collection on the qdrant instance. qdrant_delete_collection(conn, \"test_db\") #> → Collection test_db was just deleted on the qdrant instance."},{"path":"https://r-guyot.github.io/aiworkflow/reference/request_json_answer.html","id":null,"dir":"Reference","previous_headings":"","what":"Request JSON answer from the LLM — request_json_answer","title":"Request JSON answer from the LLM — request_json_answer","text":"request_json_answer request current workflow answer using JSON format.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/request_json_answer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Request JSON answer from the LLM — request_json_answer","text":"","code":"request_json_answer(workflow_obj, json_object_format = list())"},{"path":"https://r-guyot.github.io/aiworkflow/reference/request_json_answer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Request JSON answer from the LLM — request_json_answer","text":"workflow_obj current workflow object want build json_object_format format required answer.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/request_json_answer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Request JSON answer from the LLM — request_json_answer","text":"function request LLM answer using JSON format. default simply focus simple JSON format just answer single object. can add different JSON format argument.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/request_json_answer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Request JSON answer from the LLM — request_json_answer","text":"","code":"myflow_template <- ai_workflow() |>  set_connector(\"ollama\")  |>    set_model(model_name= \"llama3.1:8b-instruct-q5_K_M\") |>    set_n_predict(1000) |>    set_temperature(0.8) |>    request_json_answer() #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434."},{"path":"https://r-guyot.github.io/aiworkflow/reference/retrieve_similar_vectors.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve Similar Vectors — retrieve_similar_vectors","title":"Retrieve Similar Vectors — retrieve_similar_vectors","text":"retrieve_similar_vectors lets retrieve similar content, based cosine similarity, given context.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/retrieve_similar_vectors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve Similar Vectors — retrieve_similar_vectors","text":"","code":"retrieve_similar_vectors(   context_df,   prompt_vector,   max_results = 10,   similarity_threshold = 0.5 )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/retrieve_similar_vectors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve Similar Vectors — retrieve_similar_vectors","text":"context_df context dataframe contains text embeddings text information retrieval prompt_vector prompt transformed vector embeddings order kick search max_results maximum number results retrieved similarity_threshold threshold 0 1 (defaults 0.5) remove least relevant results. 1 means perfect similarity, 0 similarity .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/retrieve_similar_vectors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve Similar Vectors — retrieve_similar_vectors","text":"text vectors similar enough given prompt.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/retrieve_similar_vectors.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve Similar Vectors — retrieve_similar_vectors","text":"function provides way make simple RAG process retrieve content based context dataframe prompt vector.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/retrieve_similar_vectors.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve Similar Vectors — retrieve_similar_vectors","text":"","code":"conn <- get_ollama_connection()  document <- \"Standing proudly on the Île de la Cité in the heart of Paris,  France's capital city, lies one of the world's most beloved and historic  landmarks: the magnificent Notre Dame Cathedral. This Gothic masterpiece  has been welcoming pilgrims and tourists alike for over 850 years, since its  construction began in 1163 under King Louis VII. With its towering spires,  stunning stained glass windows, and intricate stone carvings, this beautiful  church is a testament to medieval architecture and engineering skill.  Unfortunately, a devastating fire ravaged the cathedral on April 15, 2019,  but thanks to swift action from firefighters and restoration efforts  underway, Notre Dame continues to inspire awe in those who visit her.\"  writeLines(document, con = \"doc1.txt\")  context_df <- convert_batch_documents_to_embeddings(ollama_connection = conn,                                        document_path_list = list(\"doc1.txt\"))  prompt <- \"When was Notre Dame in Paris built?\" prompt_vector <- get_ollama_embeddings(ollama_connection = conn, input =  prompt) similar_text <- retrieve_similar_vectors(context_df, prompt_vector)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/save_workflow.html","id":null,"dir":"Reference","previous_headings":"","what":"Save workflow — save_workflow","title":"Save workflow — save_workflow","text":"save_workflow makes possible save workflow settings later re-use.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/save_workflow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save workflow — save_workflow","text":"","code":"save_workflow(workflow_obj, filepath)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/save_workflow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save workflow — save_workflow","text":"workflow_obj workflow object containing parameters describing workflow required filepath filepath JSON file capture workflow configuration.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/save_workflow.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save workflow — save_workflow","text":"workflow saving , can included inside series pipes.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/save_workflow.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Save workflow — save_workflow","text":"function save workflow settings specific JSON file. reload settings use load_workflow() function.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/save_workflow.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Save workflow — save_workflow","text":"","code":"my_workflow <- ai_workflow() |> set_connector(\"ollama\")  |>  set_model(model_name= \"llama3:8b-instruct-q5_K_S\") |>  set_n_predict(500) |>  set_temperature(0.8) |>   set_default_missing_parameters_in_workflow() |>   set_system_prompt(\"You are a computer hardware and software specialist.\") |>  save_workflow(filepath=\"myworkflow.json\") #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → The workflow has been saved to myworkflow.json."},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_audience.html","id":null,"dir":"Reference","previous_headings":"","what":"Define a specific audience you want the model to prepare an answer for — set_audience","title":"Define a specific audience you want the model to prepare an answer for — set_audience","text":"set_audience lets define audience model answer. tweak answers accordingly match expectations.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_audience.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define a specific audience you want the model to prepare an answer for — set_audience","text":"","code":"set_audience(workflow_obj, audience)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_audience.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define a specific audience you want the model to prepare an answer for — set_audience","text":"workflow_obj ai_workflow object created ai_workflow() first place. audience text description audience want LLM write .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_audience.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define a specific audience you want the model to prepare an answer for — set_audience","text":"workflow object new added audience parameter","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_audience.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Define a specific audience you want the model to prepare an answer for — set_audience","text":"lets modify typical workflow adding information expected audience.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_audience.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define a specific audience you want the model to prepare an answer for — set_audience","text":"","code":"my_workflow <- ai_workflow() |>  set_system_prompt(system_prompt=\"You are a helpful AI assistant.  Answer to the best of your knowledge\") |> set_audience(audience=\"Marketing Professionals\")  my_workflow <- ai_workflow() |>  set_system_prompt(system_prompt=\"You are a helpful AI assistant.  Answer to the best of your knowledge\") |> set_audience(audience=\"5 years old kids\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_connector.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the connector required to operate the workflow. — set_connector","title":"Set the connector required to operate the workflow. — set_connector","text":"set_connector sets connector type expected server calls. can example Ollama server Llama.cpp server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_connector.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the connector required to operate the workflow. — set_connector","text":"","code":"set_connector(workflow_obj, connector)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_connector.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the connector required to operate the workflow. — set_connector","text":"workflow_obj ai_workflow object created ai_workflow() first place. connector name connector type use. can either ollama llamacpp (taken care later versions).","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_connector.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the connector required to operate the workflow. — set_connector","text":"Set connector required operate workflow. connector server reached API. can either local remote server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_connector.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the connector required to operate the workflow. — set_connector","text":"","code":"my_workflow <- ai_workflow() |>  set_connector(connector=\"ollama\") #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434."},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_current_time_and_date_reference.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the current time and date as addition reference — set_current_time_and_date_reference","title":"Set the current time and date as addition reference — set_current_time_and_date_reference","text":"set_current_time_and_date_reference lets set current time date addition reference part background LLM's answer.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_current_time_and_date_reference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the current time and date as addition reference — set_current_time_and_date_reference","text":"","code":"set_current_time_and_date_reference(workflow_obj)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_current_time_and_date_reference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the current time and date as addition reference — set_current_time_and_date_reference","text":"workflow_obj workflow object containing parameters describing workflow required","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_current_time_and_date_reference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the current time and date as addition reference — set_current_time_and_date_reference","text":"workflow object new added current_timedate parameter","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_current_time_and_date_reference.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the current time and date as addition reference — set_current_time_and_date_reference","text":"Setting additional time date reference background info used LLM. can useful expect LLM answer questions related future, present past.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_current_time_and_date_reference.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the current time and date as addition reference — set_current_time_and_date_reference","text":"","code":"my_workflow <- ai_workflow() |>  set_system_prompt(system_prompt=\"You are a helpful AI assistant.  Answer to the best of your knowledge\") |> set_current_time_and_date_reference()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_default_missing_parameters_in_workflow.html","id":null,"dir":"Reference","previous_headings":"","what":"Set Defaults for missing workflow parameters — set_default_missing_parameters_in_workflow","title":"Set Defaults for missing workflow parameters — set_default_missing_parameters_in_workflow","text":"set_default_missing_parameters_in_workflow ensure parameters present filling gaps","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_default_missing_parameters_in_workflow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set Defaults for missing workflow parameters — set_default_missing_parameters_in_workflow","text":"","code":"set_default_missing_parameters_in_workflow(workflow_obj, silent = F)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_default_missing_parameters_in_workflow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set Defaults for missing workflow parameters — set_default_missing_parameters_in_workflow","text":"workflow_obj workflow object containing parameters describing flow required silent ensure action happens without warning sent standard output","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_default_missing_parameters_in_workflow.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set Defaults for missing workflow parameters — set_default_missing_parameters_in_workflow","text":"workflow object along added parameters missing.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_default_missing_parameters_in_workflow.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set Defaults for missing workflow parameters — set_default_missing_parameters_in_workflow","text":"function add default values missing parameters user-defined workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_default_missing_parameters_in_workflow.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set Defaults for missing workflow parameters — set_default_missing_parameters_in_workflow","text":"","code":"# my_workflow <- ai_workflow() |> set_default_missing_parameters_in_workflow()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_embedding_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the embedding model to be used by the workflow — set_embedding_model","title":"Set the embedding model to be used by the workflow — set_embedding_model","text":"set_embedding_model sets model used workflow object.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_embedding_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the embedding model to be used by the workflow — set_embedding_model","text":"","code":"set_embedding_model(workflow_obj, model_name)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_embedding_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the embedding model to be used by the workflow — set_embedding_model","text":"workflow_obj ai_workflow object created ai_workflow() first place. model_name name embedding model use workflow","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_embedding_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the embedding model to be used by the workflow — set_embedding_model","text":"simple function set embedding model used. Note model needs available instance connect .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_embedding_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the embedding model to be used by the workflow — set_embedding_model","text":"","code":"my_workflow <- ai_workflow() |>  set_model(model_name=\"llama3.2:latest\") |> set_embedding_model(model_name=\"nomic-embed-text:latest\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_frequency_penalty.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the frequency penalty of the model used by the flow. — set_frequency_penalty","title":"Set the frequency penalty of the model used by the flow. — set_frequency_penalty","text":"set_frequency_penalty sets frequency penalty related model workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_frequency_penalty.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the frequency penalty of the model used by the flow. — set_frequency_penalty","text":"","code":"set_frequency_penalty(workflow_obj, frequency_penalty)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_frequency_penalty.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the frequency penalty of the model used by the flow. — set_frequency_penalty","text":"workflow_obj ai_workflow object created ai_workflow() first place. frequency_penalty frequency penalty value used model. higher value (closer 1) makes AI avoid repeating words phrases, lower value (closer 0) allows repetitions.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_frequency_penalty.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the frequency penalty of the model used by the flow. — set_frequency_penalty","text":"workflow object frequency penalty specified new parameter applied.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_frequency_penalty.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the frequency penalty of the model used by the flow. — set_frequency_penalty","text":"Set frequency_penalty used model workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_frequency_penalty.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the frequency penalty of the model used by the flow. — set_frequency_penalty","text":"","code":"my_workflow <- ai_workflow() |>  set_model(model_name=\"llama3:8b-instruct-q5_0\") |>  set_frequency_penalty(0.6)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_ip_addr.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the IP Address required to connect to an API server. — set_ip_addr","title":"Set the IP Address required to connect to an API server. — set_ip_addr","text":"set_ip_addr sets IP Address related API server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_ip_addr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the IP Address required to connect to an API server. — set_ip_addr","text":"","code":"set_ip_addr(workflow_obj, ip_addr)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_ip_addr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the IP Address required to connect to an API server. — set_ip_addr","text":"workflow_obj ai_workflow object created ai_workflow() first place. ip_addr IP address server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_ip_addr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the IP Address required to connect to an API server. — set_ip_addr","text":"Set IP Address related API server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_ip_addr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the IP Address required to connect to an API server. — set_ip_addr","text":"","code":"wflow <- ai_workflow() |> set_connector(\"ollama\") |>  set_ip_addr(ip_addr=\"127.0.0.1\") #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. #> → IP address has been changed to 127.0.0.1. wflow <- ai_workflow() |> set_connector(\"ollama\") |>  set_ip_addr(ip_addr=\"192.168.1.56\") #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. #> → IP address has been changed to 192.168.1.56."},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_mode.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the mode of the model used by the workflow. — set_mode","title":"Set the mode of the model used by the workflow. — set_mode","text":"set_mode sets mode model workflow. usually means either 'chat', 'completion' 'embeddings'.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_mode.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the mode of the model used by the workflow. — set_mode","text":"","code":"set_mode(workflow_obj, mode)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_mode.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the mode of the model used by the workflow. — set_mode","text":"workflow_obj ai_workflow object created ai_workflow() first place. mode mode used model workflow - needs either 'chat', 'completion' 'embeddings'.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_mode.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the mode of the model used by the workflow. — set_mode","text":"workflow object mode used applied new parameter.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_mode.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the mode of the model used by the workflow. — set_mode","text":"sets mode model workflow. usually means either 'chat','completion' 'embeddings'.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_mode.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the mode of the model used by the workflow. — set_mode","text":"","code":"my_workflow <- ai_workflow() |>  set_model(model_name=\"llama3:8b-instruct-q5_0\") |>  set_mode(\"chat\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the LLM model to be used by the workflow — set_model","title":"Set the LLM model to be used by the workflow — set_model","text":"set_model sets model used workflow object.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the LLM model to be used by the workflow — set_model","text":"","code":"set_model(workflow_obj, model_name)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the LLM model to be used by the workflow — set_model","text":"workflow_obj ai_workflow object created ai_workflow() first place. model_name name model use workflow","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the LLM model to be used by the workflow — set_model","text":"simple function set model used. Note model needs available instance connect .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the LLM model to be used by the workflow — set_model","text":"","code":"my_workflow <- ai_workflow() |>  set_model(model_name=\"llama3:8b-instruct-q5_0\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_n_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the number of tokens to be predicted (maximum) by the flow. — set_n_predict","title":"Set the number of tokens to be predicted (maximum) by the flow. — set_n_predict","text":"set_n_predict sets maximum number tokens predicted flow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_n_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the number of tokens to be predicted (maximum) by the flow. — set_n_predict","text":"","code":"set_n_predict(workflow_obj, n_predict)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_n_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the number of tokens to be predicted (maximum) by the flow. — set_n_predict","text":"workflow_obj ai_workflow object created ai_workflow() first place. n_predict number tokens predicted (maximum) flow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_n_predict.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the number of tokens to be predicted (maximum) by the flow. — set_n_predict","text":"sets maximum number tokens predicted flow. Note mean LLM constrain answer number, planned answer exceeds n_predict value, answer simply stop point.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_n_predict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the number of tokens to be predicted (maximum) by the flow. — set_n_predict","text":"","code":"wflow <- ai_workflow() |> set_connector(\"ollama\") |>  set_n_predict(n_predict=500) #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434."},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_num_ctx.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the length of the context to be handled by the model — set_num_ctx","title":"Set the length of the context to be handled by the model — set_num_ctx","text":"set_num_ctx lets define length context supported model","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_num_ctx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the length of the context to be handled by the model — set_num_ctx","text":"","code":"set_num_ctx(workflow_obj, num_ctx)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_num_ctx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the length of the context to be handled by the model — set_num_ctx","text":"workflow_obj workflow object containing parameters describing workflow required num_ctx numerical value defining length context used, tokens.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_num_ctx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the length of the context to be handled by the model — set_num_ctx","text":"workflow object new added num_ctx parameter","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_num_ctx.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the length of the context to be handled by the model — set_num_ctx","text":"Depending server settings, length context handled model may shorter expect. example, Ollama seems default context size 1024 tokens even model actually supports . order able fully use capabilities model, can specify length expect support num_ctx.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_num_ctx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the length of the context to be handled by the model — set_num_ctx","text":"","code":"my_workflow <- ai_workflow() |>  set_model(model_name=\"llama3:8b-instruct-q5_0\") |>  set_num_ctx(num_ctx=2048)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_overall_background.html","id":null,"dir":"Reference","previous_headings":"","what":"Set overall background info for your model before an answer is formulated — set_overall_background","title":"Set overall background info for your model before an answer is formulated — set_overall_background","text":"set_overall_background lets give additional background info supposed used model every answer.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_overall_background.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set overall background info for your model before an answer is formulated — set_overall_background","text":"","code":"set_overall_background(workflow_obj, overall_background)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_overall_background.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set overall background info for your model before an answer is formulated — set_overall_background","text":"workflow_obj workflow object containing parameters describing workflow required overall_background single-element text vector contains background information want system prompt ","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_overall_background.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set overall background info for your model before an answer is formulated — set_overall_background","text":"workflow object new added overall_background parameter","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_overall_background.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set overall background info for your model before an answer is formulated — set_overall_background","text":"Setting background info can help general knowledge reference expect LLM . Say, specific personal information want enter relevant answering several questions, want put . different RAG, RAG system basically pull relevant information every specific question. , akin letting LLM fundamental, general knowledge.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_overall_background.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set overall background info for your model before an answer is formulated — set_overall_background","text":"","code":"my_workflow <- ai_workflow() |>  set_system_prompt(system_prompt=\"You are a helpful AI assistant.  Answer to the best of your knowledge\") |> set_audience(\"Marketing Professionals\") |> set_overall_background(\"Our company, YOMAN & Co, has been struggling with our  recent products because of lack of market understanding.\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_port.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the port required to connect to the API server. — set_port","title":"Set the port required to connect to the API server. — set_port","text":"set_port sets port related API server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_port.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the port required to connect to the API server. — set_port","text":"","code":"set_port(workflow_obj, port)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_port.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the port required to connect to the API server. — set_port","text":"workflow_obj ai_workflow object created ai_workflow() first place. port port used access API server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_port.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the port required to connect to the API server. — set_port","text":"Set IP Address related API server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_port.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the port required to connect to the API server. — set_port","text":"","code":"wflow <- ai_workflow() |> set_connector(\"ollama\") |>  set_ip_addr(ip_addr=\"127.0.0.1\") |> set_port(port=\"11434\") #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. #> → IP address has been changed to 127.0.0.1. #> → Port has been changed to 11434."},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_presence_penalty.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the presence penalty of the model used by the flow. — set_presence_penalty","title":"Set the presence penalty of the model used by the flow. — set_presence_penalty","text":"set_presence_penalty sets presence penalty related model workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_presence_penalty.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the presence penalty of the model used by the flow. — set_presence_penalty","text":"","code":"set_presence_penalty(workflow_obj, presence_penalty)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_presence_penalty.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the presence penalty of the model used by the flow. — set_presence_penalty","text":"workflow_obj ai_workflow object created ai_workflow() first place. presence_penalty presence penalty value used model. value closer 1 encourages model generate novel diverse text. lower value, closer 0, encourages cliche phrases.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_presence_penalty.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the presence penalty of the model used by the flow. — set_presence_penalty","text":"Set presence_penalty used model workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_presence_penalty.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the presence penalty of the model used by the flow. — set_presence_penalty","text":"","code":"my_workflow <- ai_workflow() |>  set_model(model_name=\"llama3:8b-instruct-q5_0\") |>  set_presence_penalty(0.9)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_processing_skill.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the processing skill that you want to give the workflow. — set_processing_skill","title":"Set the processing skill that you want to give the workflow. — set_processing_skill","text":"set_processing_skill sets processing skill give workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_processing_skill.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the processing skill that you want to give the workflow. — set_processing_skill","text":"","code":"set_processing_skill(workflow_obj, processing_skill, ...)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_processing_skill.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the processing skill that you want to give the workflow. — set_processing_skill","text":"workflow_obj ai_workflow object created ai_workflow() first place. processing_skill string - processing skill want give workflow. ... additional parameters treated supported processing skill","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_processing_skill.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the processing skill that you want to give the workflow. — set_processing_skill","text":"workflow object appropriate processing skill applied prompt vector(s)","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_processing_skill.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the processing skill that you want to give the workflow. — set_processing_skill","text":"sets processing skill want give workflow. can list processing skills available default list_processing_skills().","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_processing_skill.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the processing skill that you want to give the workflow. — set_processing_skill","text":"","code":"my_workflow <- ai_workflow() |> set_model(model_name=\"llama3:8b-instruct-q5_0\") |>  set_processing_skill(\"fix_copy\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_repeat_penalty.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the repeat penalty of the model used by the flow. — set_repeat_penalty","title":"Set the repeat penalty of the model used by the flow. — set_repeat_penalty","text":"set_repeat_penalty sets repeat penalty related model workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_repeat_penalty.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the repeat penalty of the model used by the flow. — set_repeat_penalty","text":"","code":"set_repeat_penalty(workflow_obj, repeat_penalty)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_repeat_penalty.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the repeat penalty of the model used by the flow. — set_repeat_penalty","text":"workflow_obj ai_workflow object created ai_workflow() first place. repeat_penalty repeat penalty value used model. value 1 means penalty. value higher 1 means increased penalty repetition tokens.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_repeat_penalty.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the repeat penalty of the model used by the flow. — set_repeat_penalty","text":"Set repeat_penalty used model workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_repeat_penalty.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the repeat penalty of the model used by the flow. — set_repeat_penalty","text":"","code":"my_workflow <- ai_workflow() |>  set_model(model_name=\"llama3:8b-instruct-q5_0\") |>  set_repeat_penalty(1.3)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_seed.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the seed of the model used by the workflow. — set_seed","title":"Set the seed of the model used by the workflow. — set_seed","text":"set_seed sets seed model.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_seed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the seed of the model used by the workflow. — set_seed","text":"","code":"set_seed(workflow_obj, seed)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_seed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the seed of the model used by the workflow. — set_seed","text":"workflow_obj ai_workflow object created ai_workflow() first place. seed seed used model workflow, intend fix . seed give answer. interesting fix want compare effect parameters.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_seed.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the seed of the model used by the workflow. — set_seed","text":"workflow object seed applied.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_seed.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the seed of the model used by the workflow. — set_seed","text":"Set seed used model workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_seed.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the seed of the model used by the workflow. — set_seed","text":"","code":"my_workflow <- ai_workflow() |>  set_model(model_name=\"llama3:8b-instruct-q5_0\") |>  set_seed(12312312312)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_style_of_voice.html","id":null,"dir":"Reference","previous_headings":"","what":"Define a specific style of voice that you want the LLM to use when answering — set_style_of_voice","title":"Define a specific style of voice that you want the LLM to use when answering — set_style_of_voice","text":"set_style_of_voice lets define specific style voice want LLM use answering","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_style_of_voice.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define a specific style of voice that you want the LLM to use when answering — set_style_of_voice","text":"","code":"set_style_of_voice(workflow_obj, style_of_voice)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_style_of_voice.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define a specific style of voice that you want the LLM to use when answering — set_style_of_voice","text":"workflow_obj ai_workflow object created ai_workflow() first place. style_of_voice text description person style person want LLM imitate.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_style_of_voice.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define a specific style of voice that you want the LLM to use when answering — set_style_of_voice","text":"workflow object new added style_of_voice parameter","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_style_of_voice.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Define a specific style of voice that you want the LLM to use when answering — set_style_of_voice","text":"lets define specific style voice want LLM use answering","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_style_of_voice.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define a specific style of voice that you want the LLM to use when answering — set_style_of_voice","text":"","code":"my_workflow <- ai_workflow() |>  set_system_prompt(system_prompt=\"You are a helpful AI assistant.  Answer to the best of your knowledge\") |> set_audience(\"Marketing Professionals\") |> set_style_of_voice(\"Snoop Dog\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_system_prompt.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the system prompt to be used by the model. — set_system_prompt","title":"Set the system prompt to be used by the model. — set_system_prompt","text":"set_system_prompt sets system prompt used model.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_system_prompt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the system prompt to be used by the model. — set_system_prompt","text":"","code":"set_system_prompt(workflow_obj, system_prompt)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_system_prompt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the system prompt to be used by the model. — set_system_prompt","text":"workflow_obj ai_workflow object created ai_workflow() first place. system_prompt mode used model workflow - needs either 'chat' 'completion'.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_system_prompt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the system prompt to be used by the model. — set_system_prompt","text":"workflow object system prompt specified new parameter.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_system_prompt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the system prompt to be used by the model. — set_system_prompt","text":"sets system prompt model workflow. can give additional guidance personality change way model answer prompts.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_system_prompt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the system prompt to be used by the model. — set_system_prompt","text":"","code":"my_workflow <- ai_workflow() |>  set_system_prompt(system_prompt=\"You are a helpful AI assistant.  Answer to the best of your knowledge\") my_workflow <- ai_workflow() |>  set_system_prompt(system_prompt=\"You are a helpful AI assistant  and an expert in financial analysis.\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_temperature.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the temperature of the model used by the workflow. — set_temperature","title":"Set the temperature of the model used by the workflow. — set_temperature","text":"set_temperature sets temperature model used workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_temperature.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the temperature of the model used by the workflow. — set_temperature","text":"","code":"set_temperature(workflow_obj, temperature)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_temperature.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the temperature of the model used by the workflow. — set_temperature","text":"workflow_obj ai_workflow object created ai_workflow() first place. temperature temperature value used model. 0 1 (boundaries included).","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_temperature.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the temperature of the model used by the workflow. — set_temperature","text":"Set temperature used model workflow. temperature zero always give answer. temperature 1 give random answers may make full sense. Ideally want level randomness still remaining sensible answers, target value around 0.6 0.7. trying validate AI workflow specific answers specific prompts, highly recommended work temperature zero.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_temperature.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the temperature of the model used by the workflow. — set_temperature","text":"","code":"my_workflow <- ai_workflow() |>  set_model(model_name=\"llama3:8b-instruct-q5_0\") |>  set_temperature(0.8)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_paragraphs.html","id":null,"dir":"Reference","previous_headings":"","what":"Split text into paragraphs — split_text_as_paragraphs","title":"Split text into paragraphs — split_text_as_paragraphs","text":"split_text_as_paragraphs Splits text sentences-based chunks.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_paragraphs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split text into paragraphs — split_text_as_paragraphs","text":"","code":"split_text_as_paragraphs(text)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_paragraphs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split text into paragraphs — split_text_as_paragraphs","text":"text single piece text break paragraphs.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_paragraphs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Split text into paragraphs — split_text_as_paragraphs","text":"Splits text paragraphs-based chunks, leveraging tokenizers library.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_paragraphs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split text into paragraphs — split_text_as_paragraphs","text":"","code":"# c(\"Hi! How are you?\\n\\n Do you want to go for a walk?\") |> split_text_as_paragraphs()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_sentences.html","id":null,"dir":"Reference","previous_headings":"","what":"Split text into sentences — split_text_as_sentences","title":"Split text into sentences — split_text_as_sentences","text":"split_text_as_sentences Splits text sentences-based chunks.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_sentences.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split text into sentences — split_text_as_sentences","text":"","code":"split_text_as_sentences(text)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_sentences.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split text into sentences — split_text_as_sentences","text":"text single piece text break .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_sentences.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Split text into sentences — split_text_as_sentences","text":"Splits text sentences-based chunks, leveraging tokenizers library.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_sentences.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split text into sentences — split_text_as_sentences","text":"","code":"# c(\"Hi! How are you? Do you want to go for a walk?\") |> split_text_as_sentences()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/switch_to_workflow.html","id":null,"dir":"Reference","previous_headings":"","what":"Switch to workflow — switch_to_workflow","title":"Switch to workflow — switch_to_workflow","text":"switch_to_workflow function makes possible chain several workflows using pipes","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/switch_to_workflow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Switch to workflow — switch_to_workflow","text":"","code":"switch_to_workflow(workflow, new_workflow)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/switch_to_workflow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Switch to workflow — switch_to_workflow","text":"workflow workflow object containing parameters describing flow required new_workflow workflow object execute last one","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/switch_to_workflow.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Switch to workflow — switch_to_workflow","text":"function send output previous workflow next one.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_llamacpp_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"Confirm connection to a Llama.cpp server is working — test_llamacpp_connection","title":"Confirm connection to a Llama.cpp server is working — test_llamacpp_connection","text":"test_llamacpp_connection tests connection llama.cpp server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_llamacpp_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Confirm connection to a Llama.cpp server is working — test_llamacpp_connection","text":"","code":"test_llamacpp_connection(ip_ad = \"127.0.0.1\", port = \"8080\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_llamacpp_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Confirm connection to a Llama.cpp server is working — test_llamacpp_connection","text":"ip_ad IP address server running Llama.cpp. Default localhost 127.0.0.1. port port used run Llama.cpp service. Default 8080.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_llamacpp_connection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Confirm connection to a Llama.cpp server is working — test_llamacpp_connection","text":"simple function test connection llama.cpp server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_ollama_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"Confirm connection to ollama is working — test_ollama_connection","title":"Confirm connection to ollama is working — test_ollama_connection","text":"test_ollama_connection says hello uses name person(s) argument. y","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_ollama_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Confirm connection to ollama is working — test_ollama_connection","text":"","code":"test_ollama_connection(ip_ad = \"127.0.0.1\", port = \"11434\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_ollama_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Confirm connection to ollama is working — test_ollama_connection","text":"ip_ad IP address server running ollama. Default localhost 127.0.0.1. port port used run ollama service. Default 11374.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_ollama_connection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Confirm connection to ollama is working — test_ollama_connection","text":"Returns TRUE connection works, error object .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_ollama_connection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Confirm connection to ollama is working — test_ollama_connection","text":"simple function test connect ollama","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_ollama_connection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Confirm connection to ollama is working — test_ollama_connection","text":"","code":"#test_ollama_connection(ip_ad=\"127.0.0.1\", port=\"11434\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/write_vectors_to_feather_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Write Vectors to Feather File — write_vectors_to_feather_file","title":"Write Vectors to Feather File — write_vectors_to_feather_file","text":"write_vectors_to_feather_file lets write vector data feather file","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/write_vectors_to_feather_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write Vectors to Feather File — write_vectors_to_feather_file","text":"","code":"write_vectors_to_feather_file(vector_data, file_name)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/write_vectors_to_feather_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write Vectors to Feather File — write_vectors_to_feather_file","text":"vector_data vector data (embeddings) already generated another function. file_name file name save embeddings feather file","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/write_vectors_to_feather_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write Vectors to Feather File — write_vectors_to_feather_file","text":"nothing.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/write_vectors_to_feather_file.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Write Vectors to Feather File — write_vectors_to_feather_file","text":"function provides simple way save vector data (embeddings) binary feather file, instead using regular databases.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/write_vectors_to_feather_file.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write Vectors to Feather File — write_vectors_to_feather_file","text":"","code":"conn <- get_ollama_connection()  document <- \"Standing proudly on the Île de la Cité in the heart of Paris,  France's capital city, lies one of the world's most beloved and historic  landmarks: the magnificent Notre Dame Cathedral. This Gothic masterpiece  has been welcoming pilgrims and tourists alike for over 850 years, since its  construction began in 1163 under King Louis VII. With its towering spires,  stunning stained glass windows, and intricate stone carvings, this beautiful  church is a testament to medieval architecture and engineering skill.  Unfortunately, a devastating fire ravaged the cathedral on April 15, 2019,  but thanks to swift action from firefighters and restoration efforts  underway, Notre Dame continues to inspire awe in those who visit her.\"  writeLines(document, con = \"doc1.txt\")  write_vectors_to_feather_file(file_name = \"doc1.feather\", vector_data = generate_document_embeddings(conn,  document_path = \"doc1.txt\", splitter = \"paragraph\"))"}]
