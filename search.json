[{"path":"https://r-guyot.github.io/aiworkflow/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU Lesser General Public License","title":"GNU Lesser General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed. version GNU Lesser General Public License incorporates terms conditions version 3 GNU General Public License, supplemented additional permissions listed .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/LICENSE.html","id":"id_0-additional-definitions","dir":"","previous_headings":"","what":"0. Additional Definitions","title":"GNU Lesser General Public License","text":"used herein, “License” refers version 3 GNU Lesser General Public License, “GNU GPL” refers version 3 GNU General Public License. “Library” refers covered work governed License, Application Combined Work defined . “Application” work makes use interface provided Library, otherwise based Library. Defining subclass class defined Library deemed mode using interface provided Library. “Combined Work” work produced combining linking Application Library. particular version Library Combined Work made also called “Linked Version”. “Minimal Corresponding Source” Combined Work means Corresponding Source Combined Work, excluding source code portions Combined Work , considered isolation, based Application, Linked Version. “Corresponding Application Code” Combined Work means object code /source code Application, including data utility programs needed reproducing Combined Work Application, excluding System Libraries Combined Work.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/LICENSE.html","id":"id_1-exception-to-section-3-of-the-gnu-gpl","dir":"","previous_headings":"","what":"1. Exception to Section 3 of the GNU GPL","title":"GNU Lesser General Public License","text":"may convey covered work sections 3 4 License without bound section 3 GNU GPL.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/LICENSE.html","id":"id_2-conveying-modified-versions","dir":"","previous_headings":"","what":"2. Conveying Modified Versions","title":"GNU Lesser General Public License","text":"modify copy Library, , modifications, facility refers function data supplied Application uses facility (argument passed facility invoked), may convey copy modified version: ) License, provided make good faith effort ensure , event Application supply function data, facility still operates, performs whatever part purpose remains meaningful, b) GNU GPL, none additional permissions License applicable copy.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/LICENSE.html","id":"id_3-object-code-incorporating-material-from-library-header-files","dir":"","previous_headings":"","what":"3. Object Code Incorporating Material from Library Header Files","title":"GNU Lesser General Public License","text":"object code form Application may incorporate material header file part Library. may convey object code terms choice, provided , incorporated material limited numerical parameters, data structure layouts accessors, small macros, inline functions templates (ten fewer lines length), following: ) Give prominent notice copy object code Library used Library use covered License. b) Accompany object code copy GNU GPL license document.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/LICENSE.html","id":"id_4-combined-works","dir":"","previous_headings":"","what":"4. Combined Works","title":"GNU Lesser General Public License","text":"may convey Combined Work terms choice , taken together, effectively restrict modification portions Library contained Combined Work reverse engineering debugging modifications, also following: ) Give prominent notice copy Combined Work Library used Library use covered License. b) Accompany Combined Work copy GNU GPL license document. c) Combined Work displays copyright notices execution, include copyright notice Library among notices, well reference directing user copies GNU GPL license document. d) one following: 0) Convey Minimal Corresponding Source terms License, Corresponding Application Code form suitable , terms permit, user recombine relink Application modified version Linked Version produce modified Combined Work, manner specified section 6 GNU GPL conveying Corresponding Source. 1) Use suitable shared library mechanism linking Library. suitable mechanism one () uses run time copy Library already present user’s computer system, (b) operate properly modified version Library interface-compatible Linked Version. e) Provide Installation Information, otherwise required provide information section 6 GNU GPL, extent information necessary install execute modified version Combined Work produced recombining relinking Application modified version Linked Version. (use option 4d0, Installation Information must accompany Minimal Corresponding Source Corresponding Application Code. use option 4d1, must provide Installation Information manner specified section 6 GNU GPL conveying Corresponding Source.)","code":""},{"path":"https://r-guyot.github.io/aiworkflow/LICENSE.html","id":"id_5-combined-libraries","dir":"","previous_headings":"","what":"5. Combined Libraries","title":"GNU Lesser General Public License","text":"may place library facilities work based Library side side single library together library facilities Applications covered License, convey combined library terms choice, following: ) Accompany combined library copy work based Library, uncombined library facilities, conveyed terms License. b) Give prominent notice combined library part work based Library, explaining find accompanying uncombined form work.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/LICENSE.html","id":"id_6-revised-versions-of-the-gnu-lesser-general-public-license","dir":"","previous_headings":"","what":"6. Revised Versions of the GNU Lesser General Public License","title":"GNU Lesser General Public License","text":"Free Software Foundation may publish revised /new versions GNU Lesser General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Library received specifies certain numbered version GNU Lesser General Public License “later version” applies , option following terms conditions either published version later version published Free Software Foundation. Library received specify version number GNU Lesser General Public License, may choose version GNU Lesser General Public License ever published Free Software Foundation. Library received specifies proxy can decide whether future versions GNU Lesser General Public License shall apply, proxy’s public statement acceptance version permanent authorization choose version Library.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/articles/first_steps.html","id":"setting-up-your-ollama-connection","dir":"Articles","previous_headings":"","what":"Setting up your Ollama connection","title":"First Steps with aiworkflow","text":"Ensure running Ollama instance local machine somewhere available network. Installing Ollama easy works multiple platforms. Ollama work even machines equipped CPUs, machines GPUs (especially Nvidia) make processing much faster. Ollama installed need download least one model get started. can establish connection using function. default use localhost 11434 port. want use different setup, can change parameters:","code":"conn <- get_ollama_connection() conn <- get_ollama_connection(ip_ad = \"127.0.0.1\",port = \"3524\")"},{"path":"https://r-guyot.github.io/aiworkflow/articles/first_steps.html","id":"your-first-workflow","dir":"Articles","previous_headings":"","what":"Your First Workflow","title":"First Steps with aiworkflow","text":"one simple workflows can make. create workflow, need start ai_workflow() container command, pipe instructions . example specify want use ollama connector, use llama3.1 model. selecting ollama connector, use default connection parameters. can however set arbitrary IP port parameters described , want connect Ollama instance living different machine. stage workflow exists, anything. next steps ask run specific tasks. can ask simple prompt. default model answers list, want use pull_final_answer() function fetch final textual answer list.","code":"wflow_basic <- ai_workflow() |>   set_connector(\"ollama\")  |>   set_model(model_name= \"llama3.1:8b-instruct-q5_K_M\")  #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. wflow_basic_on_different_machine <- ai_workflow() |>   set_connector(\"ollama\")  |>   set_ip_addr(ip_addr = \"192.168.1.12\") |>   set_port(port = 5256) |>   set_model(model_name= \"llama3.1:8b-instruct-q5_K_M\")  #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. #> → IP address has been changed to 192.168.1.12. #> → Port has been changed to 5256. wflow_basic |>    process_prompts(prompts_vector = \"why is the sky blue? Answer with a short explanation\") |>   pull_final_answer() |> cat() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Temperature was not specified and given a default value of 0.8. #> → N_predict was not specified and given a default value of 200. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> The sky appears blue because of a phenomenon called scattering, where shorter (blue) wavelengths of light are scattered more than longer (red) wavelengths by tiny molecules of gases in the atmosphere. This scattering effect gives our sky its distinct blue color!"},{"path":"https://r-guyot.github.io/aiworkflow/articles/first_steps.html","id":"customizing-output","dir":"Articles","previous_headings":"","what":"Customizing Output","title":"First Steps with aiworkflow","text":"can now leverage features package. setting audience answers. specify audience 5 years old kids. can see changes output. Don’t expect great explanation! kind explanation get little kids : *“Let tell SECRET SKY! sky looks BLUE something called LIGHT! sunlight comes sun, ’s like big bunch colorful rays shining towards us. guess happens light rays travel air atmosphere? get SCATTERED start bouncing around everywhere! Blue one colors gets scattered color. look sky, blue light rays bounce back eyes, making LOOK BLUE! Isn’t COOL?!“* Note can also change existing workflow directly piping parameter setting . example let’s modify existing workflow wflow_basic calling prompt: get something like : *“sky appears blue something called light scattering. sunlight enters atmosphere, ’s made different colors like big ol’ rainbow. tiny particles air (like dust water vapor) bounce colorful lights, scatter shorter wavelengths longer ones. guess ? Blue one short-wavelength colors! , eyes see scattered light every direction us, perceive big blue sky!“* can also set specific tone personality answer question. get something like : “Yo ’s good fam? Alright wanna know ‘bout da stock exchangah, right? Okay lemme break ya. See, ’s like one big ol’ party investors come buy sell shares companies think gonna hot future. put money stocks, kinda like bettin’ sports team somethin’. da company good, da stock price goes make dough! tanks… well, let’s just say might wanna sit one , G. exchange buyin’ sellin’ happens, kinda like online market real people makin’ deals face--face phone calls. Word.”","code":"wflow_eli5 <- ai_workflow() |>   set_connector(\"ollama\")  |>   set_model(model_name= \"llama3.1:8b-instruct-q5_K_M\") |>   set_audience(\"Five years old kids\") #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. explanation_eli5 <- wflow_eli5 |>    process_prompts(prompts_vector = \"why is the sky blue? Answer with a short explanation\") |>   pull_final_answer()  #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Temperature was not specified and given a default value of 0.8. #> → N_predict was not specified and given a default value of 200. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode explanation_low_tech <- wflow_basic |>    set_audience(\"people without scientific knowledge or background\") |>   process_prompts(prompts_vector = \"why is the sky blue? Answer with a short explanation\") |>   pull_final_answer()  #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Temperature was not specified and given a default value of 0.8. #> → N_predict was not specified and given a default value of 200. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode wflow_snoop <- ai_workflow() |>   set_connector(\"ollama\")  |>   set_model(model_name= \"llama3.1:8b-instruct-q5_K_M\")|>   set_style_of_voice(\"Snoop Dogg\") #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434.  snoop_answer <- wflow_snoop |>   process_prompts(prompts_vector = \"Explain how the stock exchange works in a short paragraph\") |>   pull_final_answer()  #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Temperature was not specified and given a default value of 0.8. #> → N_predict was not specified and given a default value of 200. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode"},{"path":"https://r-guyot.github.io/aiworkflow/articles/processing_skills.html","id":"what-are-processing-skills","dir":"Articles","previous_headings":"","what":"What are Processing Skills?","title":"Processing Skills","text":"Processing skills fancy name series pre-defined prompts can process inputs specific way. Instead leaving hanging, aiworkflow comes batteries included help numerous tasks box. First, get know kind skills available, can use ’list_processing_skills()` function. give name skills available right now current version package.","code":"list_processing_skills() #>  [1] \"add_code_comments\"              \"add_details\"                    #>  [3] \"add_emoji\"                      \"break_down_tasks\"               #>  [5] \"categorize_product_complaint\"   \"categorize_sentiment\"           #>  [7] \"chain_of_thought\"               \"clean_up_audio_transcription\"   #>  [9] \"extract_actions\"                \"extract_names\"                  #> [11] \"fix_copy\"                       \"fix_translation\"                #> [13] \"generate_code\"                  \"generate_creative_introduction\" #> [15] \"generate_quiz\"                  \"identify_product\"               #> [17] \"rewrite_active_voice\"           \"rewrite_blog_copy\"              #> [19] \"rewrite_bullet_points\"          \"rewrite_casual\"                 #> [21] \"rewrite_emoji\"                  \"rewrite_in_first_person\"        #> [23] \"rewrite_jargon\"                 \"rewrite_paraphrase\"             #> [25] \"rewrite_passive_voice\"          \"rewrite_positive\"               #> [27] \"rewrite_prompt\"                 \"rewrite_text_as_anonymized\"     #> [29] \"tldr\"                           \"translate\"                      #> [31] \"write_abstract\"                 \"write_article_from_title\"       #> [33] \"write_email\"                    \"write_fiction_chapter\"          #> [35] \"write_pitch\"                    \"write_qa\"                       #> [37] \"write_quarto_presentation\"      \"write_recommendation\"           #> [39] \"write_resume_narrative\"         \"write_short_summary\"            #> [41] \"write_text_expansion\"           \"write_title\"                    #> [43] \"write_tweet\""},{"path":"https://r-guyot.github.io/aiworkflow/articles/processing_skills.html","id":"applying-skills-to-a-workflow","dir":"Articles","previous_headings":"","what":"Applying Skills to a Workflow","title":"Processing Skills","text":"seen list, quite things choose . Let’s start something simple. can ask LLM write pitch us, simple product idea. use set_processing_skill() function specify skill want use. Now let’s test . trying sell new razors… Let’s try one example . next skill add color sentences sprinkling emojis .","code":"myflow_pitch <- ai_workflow() |>   set_connector(connector = \"ollama\") |>   set_model(model_name = \"llama3.2:latest\") |>   set_processing_skill(processing_skill = \"write_pitch\") |>   set_n_predict(1500) #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. myflow_pitch |>   process_prompts(prompts_vector = \"A new razor with 20 blades and a lot of springs for an ultimate and safe shaving experience aimed at adult men\") |>   pull_final_answer() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Temperature was not specified and given a default value of 0.8. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> [1] \"Introducing The Ultimate Shaving Experience: Revolutionizing Men's Grooming\\n\\nSay goodbye to nicks, cuts, and ingrown hairs with our groundbreaking new razor featuring an unprecedented 20 blades and a robust spring system designed for unparalleled safety and precision.\\n\\nFor the discerning adult male who demands the best, we've created a shaving experience that redefines convenience and confidence. Our innovative design ensures a smooth, irritation-free shave every time, while its ergonomic handle provides a secure grip even in wet hands.\\n\\nStand out from the competition with our razor's unique features, including:\\n\\nAdvanced 20-blade technology for unparalleled closeness\\nDurable spring system ensuring optimal cut quality\\n\\nTargeted at style-conscious men seeking premium grooming solutions. Our new razor is perfect for those who refuse to compromise on shaving performance and precision.\\n\\nJoin the shaving revolution that's about to change the game forever. Upgrade your daily routine with The Ultimate Shaving Experience - order now!\" myflow_emoji <- ai_workflow() |>   set_connector(connector = \"ollama\") |>   set_model(model_name = \"llama3.2:latest\") |>   set_processing_skill(processing_skill = \"add_emoji\") |>   set_n_predict(1500) |>   set_temperature(0.5) #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. myflow_emoji |>   process_prompts(\"The sun is shining today. Just the right temperature. I can finally relax and enjoy a good book.\") |>   pull_final_answer() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> [1] \"The ☀️ sun is shining today 🌞. Just the ⚖️ right temperature. I can finally 😌 relax and enjoy a good 📖 book.\""},{"path":"https://r-guyot.github.io/aiworkflow/articles/processing_skills.html","id":"applying-skills-in-sequence","dir":"Articles","previous_headings":"","what":"Applying Skills in Sequence","title":"Processing Skills","text":"may wondering… great, can single-action skills, way combine one ? answer YES! Let’s start depressing piece news: Let’s make workflow turns positive light. don’t just want add positivity. also want add details summary. Now let’s chain together using switch_to_workflow() function, using pipes.","code":"company_report <- \"The Q2 quarter was awful. Our leading product, CLEAN-Z, has lost 15% market share in a few months, hurting our bottom line very badly. We now have too much inventory on our hands and wholesalers may be shipping more back to us if we can't rapidly increase our sales. Our Product development team lucked out and failed to release the new formula on time. It looks like it will be delayed another 2 or three months. The oulook for the rest of the year looks grim.\" myflow_positive <- ai_workflow() |>   set_connector(connector = \"ollama\") |>   set_model(model_name = \"llama3.2:latest\") |>   set_processing_skill(processing_skill = \"rewrite_positive\") |>   set_n_predict(1500) |>   set_temperature(0.5) #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. myflow_more_details <- ai_workflow() |>   set_connector(connector = \"ollama\") |>   set_model(model_name = \"llama3.2:latest\") |>   set_processing_skill(processing_skill = \"add_details\") |>   set_n_predict(1500) |>   set_temperature(0.5) #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. # you start from the first workflow here myflow_positive |>    # you bring your first prompt here   process_prompts(prompts_vector = company_report) |>   # you can display the intermediate answer if needed   display_intermediate_answer() |>   # and then answer from the previous workflow becomes the input for the next one   switch_to_workflow(new_workflow = myflow_more_details) |>   # and we pull the final answer   pull_final_answer() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> → Intermediate answer from last step: #> Despite initial challenges, Q2 has presented an opportunity to reassess and #> refocus our strategy. Our leading product, CLEAN-Z, is undergoing a necessary #> adjustment period as it navigates changes in market demand. This shift allows #> us to rebalance inventory levels and optimize production for future growth. The #> recent delay of the new formula provides a chance to fine-tune its development #> and ensure its successful launch when the time comes. With a strong Product #> Development team on board, we're confident that our expertise will yield #> significant results in Q3, setting the stage for a more promising year ahead. #> → Frequency Penalty was not specified and given a default value of 1. #>  #> → Presence Penalty was not specified and given a default value of 1.5. #>  #> → Repeat Penalty was not specified and given a default value of 1.2. #>  #> → Mode was not specified and 'chat' was selected by default. #>  #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #>  #> → Chat mode #> [1] \"Despite initial challenges, Q2 has presented an opportunity to reassess and refocus our strategy. Our leading product, CLEAN-Z, is undergoing a necessary adjustment period as it navigates changes in market demand that have been both unexpected and intriguing. The bird's behavior was unlike anything we had seen before - its sudden interest in the girl seemed almost... deliberate.\\n\\nThe recent shift in market trends has forced us to rebalance our inventory levels, redirecting resources from areas of slow sales to those with higher growth potential. This strategic realignment will not only optimize production but also enable us to capitalize on emerging opportunities and stay ahead of the competition.\\n\\nA notable delay in the development of a new formula for CLEAN-Z provides a chance to fine-tune its formulation, ensuring that it meets our high standards while addressing the evolving needs of our customers. The Product Development team has been working tirelessly behind the scenes to refine the recipe, incorporating feedback from both internal stakeholders and external market research.\\n\\nWith their expertise at the forefront, we're confident that significant results will emerge in Q3, setting the stage for a more promising year ahead. Our team's commitment to innovation and customer satisfaction is unwavering, and we remain committed to delivering exceptional products that exceed our customers' expectations.\""},{"path":"https://r-guyot.github.io/aiworkflow/articles/processing_skills.html","id":"chaining-workflows-other-method","dir":"Articles","previous_headings":"","what":"Chaining Workflows (Other Method)","title":"Processing Skills","text":"another way chain workflow together, using add_workflow_step() function. result similar previous example: key difference way observe intermediate results . makes easier add steps single workflow: now able get one go, final text translated Spanish way. Note quality Spanish translation case depend heavily well model selected can handle kind tasks. models equal comes multilingual capabilities.","code":"# you start from the first workflow here myflow_chained <-   myflow_positive |>   add_workflow_step(myflow_more_details) # you start from the combined workflow here myflow_chained |>    # you bring your prompt here   process_prompts(prompts_vector = company_report) |>   pull_final_answer() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> [[1]] #> [1] \"The Q2 quarter presented a significant opportunity for growth and improvement, as our leading product CLEAN-Z experienced a remarkable 15% market share gain in just months, demonstrating its strong potential for success and cementing its position as a industry leader. The sudden surge in demand was largely driven by the increasing awareness of environmental sustainability among consumers, which aligns perfectly with our brand values and mission to provide eco-friendly cleaning solutions.\\n\\nAlthough inventory levels may require some adjustments to ensure timely fulfillment of orders, this presents an ideal chance to optimize logistics and streamline operations, reducing costs while maintaining high customer satisfaction standards. The Product development team's proactive approach will now be focused on accelerating the release of their new formula, which is expected to bring fresh energy and innovation to our product line in Q3 or early Q4.\\n\\nThe new formula, codenamed \\\"GREEN-Boost,\\\" boasts enhanced cleaning power with a unique blend of plant-based ingredients, making it an attractive option for environmentally conscious consumers. With its sleek packaging design and user-friendly interface, GREEN-Boost is poised to revolutionize the way people clean their homes and workplaces, further solidifying our brand's market share lead.\\n\\nFurthermore, this development provides a significant competitive advantage over existing products in the market, allowing us to expand our customer base while maintaining our unique selling proposition. As we move forward with the launch of GREEN-Boost, our marketing strategy will focus on targeted social media campaigns and partnerships with influential eco-friendly brands, ensuring maximum visibility and buzz around our new product line.\\n\\nIn conclusion, the Q2 quarter's success sets a strong foundation for future growth and expansion, as we capitalize on emerging trends in sustainability and cleaning technology. With its innovative formula and strategic marketing approach, GREEN-Boost is poised to drive business results that exceed expectations and cement CLEAN-Z's position as a leader in the industry.\" myflow_translate_to_spanish <- ai_workflow() |>   set_connector(connector = \"ollama\") |>   set_model(model_name = \"llama3.2:latest\") |>   set_processing_skill(processing_skill = \"translate\",target_language=\"spanish\") |>   set_n_predict(2000) |>   set_temperature(0.5) #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434.  myflow_chained <-   myflow_positive |>   add_workflow_step(myflow_more_details) |>   add_workflow_step(myflow_translate_to_spanish) # you start from the combined workflow here myflow_chained |>    # you bring your prompt here   process_prompts(prompts_vector = company_report) |>   pull_final_answer() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> → Applying additional variables to working env #> [[1]] #> [1] \"El segundo trimestre presentó una oportunidad para el crecimiento y la mejora, ya que nuestro producto líder CLEAN-Z experimentó un aumento significativo del 15% en las partes de mercado en solo unos meses, lo cual indica un mayor interés consumidor tanto entre los clientes establecidos que buscan mejorar sus productos existentes como nuevos usuarios que descubren los beneficios del limpieza con ingredientes naturales. Esta tendencia ascendente se atribuye a campañas de marketing efectivas que lograron dirigirse específicamente a ciertas demografías, como consumidores conscientes ambientalmente y personas saludables orientadas hacia alternativas ecológicas.\\n\\nMientras tanto, la gestión del nivel de inventario es crucial; sin embargo, podemos aprovechar este impulso para impulsar las ventas y el crecimiento en ingresos al optimizar nuestra cadena logística de suministros, asegurando una entrega oportuna de productos a los minoristas y clientes en línea. Esta estrategia tendrá como objetivo capitalizar la demanda creciente por CLEAN-Z mientras minimizamos posibles problemas de sobrastoc y agotamiento que podrían impactar nuestra rentabilidad.\\n\\nEl equipo de desarrollo del producto ha adoptado un enfoque innovador, lo cual llevó a crear una fórmula emocionante nueva; esta se lanzará pronto después de un breve retraso. El proceso de investigación y desarrollo implicó pruebas extensas y ajustes múltiples para lograr el equilibrio entre la efectividad e la sostenibilidad ambiental. Esta pausa estratégica nos permite perfeccionar nuestras ofertas productivas y asegurarnos que cumplen con las necesidades evolucionadas de nuestros clientes, incluyendo abordar posibles alergias o sensibilidades asociadas a las fórmulas anteriores.\\n\\nMientras avanzamos hacia el resto del año, estamos preparados para una fuerte recuperación y nos apoyamos en capitalizar este impulso expandiendo nuestras canales de distribución mediante la colaboración con principales minoristas. Nuestro equipo de ventas se enfocará en establecer relaciones con clientes existentes mientras buscan nuevos mercados, incluyendo el segmento de productos limpieza para hogar creciente en las regiones del Pacífico Asia. Con un plan sólido a mano, estamos seguros de que CLEAN-Z seguirá liderando al mercado y impulsará nuestro crecimiento empresarial.\""},{"path":"https://r-guyot.github.io/aiworkflow/articles/tool_calling.html","id":"tool-preparation","dir":"Articles","previous_headings":"","what":"Tool Preparation","title":"Tool Calling","text":"tools exactly? Let’s try make things complicated - tools actually just functions can call. idea LLMs trained development process call functions case want get specific information part knowledge base. LLMs support tool calling - can use example Llama 3.1 Llama 3.2 tool calling moment. first step able assign tool create tool ! Let’s write simple function. use disastr.api package CRAN example. function get list recent disasters happening around world. Note write function LLM, need ensure : function returns data JSON format function loaded global environment first LLM tries call function, added filters : type_disaster: type disaster filter results , needed. country_filter: country interest, . Note also added “…” extra argument. purpose. means function also accept input parameters beyond type_disaster country_filter. reason , take care hallucinations. LLMs tendency come additional arguments exist given function (even give exactly list arguments accepted) adding “…” parameter make possible function ignore additional parameters without failing returning error. Note smaller models prone hallucinations comes tool calling.","code":"get_recent_disasters <- function(type_disaster=NA, country_filter=NA, ...) {    library(disastr.api)   res <- disastr.api(limit = 100)   if (!is.na(type_disaster)) {     type_disaster <- stringr::str_remove(type_disaster, \"s$\")     res <- res |> dplyr::filter(grepl(type_disaster,event,ignore.case=T))   }   if (!is.na(country_filter)) {     res <- res |> dplyr::filter(grepl(country_filter,country,ignore.case=T))   }      res_f <- res |> jsonlite::toJSON()     return(res_f) }"},{"path":"https://r-guyot.github.io/aiworkflow/articles/tool_calling.html","id":"tool-declaration","dir":"Articles","previous_headings":"","what":"Tool Declaration","title":"Tool Calling","text":"Now function, need prepare declaration. declare list R. can follow format , advised case Llama3.1 beyond: can see, need specify arguments mean, function , LLM can grasp good time use . Note limited declaring single function. can several functions (tools) part declaration. way, can increase capability workflow handle different types requests.","code":"tool_list <- list(   list(type=\"function\",        \"function\"=list(          name=\"get_recent_disasters\",          description=\"get information about recent disasters that are happening or happened worldwide\",          parameters= list(            type=\"object\",            properties = list(              type_disaster=list(                             type=\"string\",                             description=\"the type of disaster to search for. Make sure this is the singular version of the word\"              ),              country_filter=list(                type=\"string\",                description=\"a specific country you want to filter results for, related to disasters\"              )            )          )        )) )"},{"path":"https://r-guyot.github.io/aiworkflow/articles/tool_calling.html","id":"building-a-workflow-with-tools","dir":"Articles","previous_headings":"","what":"Building a workflow with tools","title":"Tool Calling","text":"Now let’s build workflow integrate tool calling capability, using add_tools_declaration() function: Now workflow ready, can try . won’t see internal details, actually happening LLM first function call, confirm happened Mexico first, based information received function, LLM formulate second answer (final one) uses info. can see difference versus kind workflow, tools support: can see, don’t provide tools, LLM use whatever memory can recollect training (whatever can hallucinate…).","code":"wflow_tool <- ai_workflow() |>   set_connector(\"ollama\")  |>   set_temperature(0) |>   set_model(model_name= \"llama3.2:latest\") |>   set_system_prompt(\"you are an AI assistant capable of research recent disasters information with a tool connected to the Internet.\") |>   add_tools_declaration(tools = tool_list) #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. wflow_tool |>    process_prompts(\"Tell me what recent disasters have happened in Mexico?\") |>    pull_final_answer() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → N_predict was not specified and given a default value of 200. #> → Mode was not specified and 'chat' was selected by default. #> → Chat mode #> → Adding tools #>  #> The disastR.api package may be cited as: #> Dworschak, Christoph. 2021. \"Disastr.api: Wrapper for the UN OCHA #> ReliefWeb Disaster Events API.\" R package. CRAN version 1.0.6. #> Your disaster event data request was successful. #> [1] \"Based on the information I have access to, here are some recent disasters that have occurred in Mexico:\\n\\n1. **Hurricane Beryl** (June 2024): A tropical cyclone that affected several countries in the Caribbean and Central America, including Mexico.\\n2. **Flooding in Veracruz** (May 2024): Heavy rainfall caused flooding in the state of Veracruz, affecting thousands of people and causing significant damage to infrastructure.\\n\\nPlease note that my knowledge cutoff is December 2023, so I may not have information on more recent disasters that occurred after this date.\" wflow_no_tool <- ai_workflow() |>   set_connector(\"ollama\")  |>   set_temperature(0) |>   set_model(model_name= \"llama3.2:latest\")  #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434.  wflow_no_tool |>    process_prompts(\"Tell me what recent disasters have happened in Mexico?\") |>    pull_final_answer() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → N_predict was not specified and given a default value of 200. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> [1] \"I'll provide you with some information on recent natural disasters that have affected Mexico:\\n\\n1. **Hurricane Patricia (2015)**: A Category 5 hurricane made landfall in Jalisco, Mexico, causing widespread damage and flooding.\\n2. **Earthquake in Puebla (2017)**: A magnitude 7.1 earthquake struck the state of Puebla, killing over 300 people and injuring many more.\\n3. **Hurricane Odile (2014)**: Although not directly on land, Hurricane Odile brought strong winds and heavy rainfall to parts of Mexico's Baja California Peninsula.\\n4. **Floods in Oaxaca (2020)**: Heavy rains caused severe flooding in the state of Oaxaca, displacing thousands of people and causing significant damage to infrastructure.\\n5. **Volcanic eruptions in Chiapas (2019-2022)**: The Fuego volcano erupted several times between 2019 and 2022, affecting nearby\""},{"path":"https://r-guyot.github.io/aiworkflow/articles/tool_calling.html","id":"offline-tools","dir":"Articles","previous_headings":"","what":"Offline Tools","title":"Tool Calling","text":"often see examples online tools used connect API online sources pull information, way. can example build tool support calculations math operations, since LLMs notoriously bad (good reasons, don’t embed concept numbers, tokens). Let’s first declare function maths: Don’t expect much function. work simple operations. won’t solve binomial equations something. Now declare math tool: Let’s ask Snoop Dogg answer math problem: take calculator, see expect answer 566233.71 less. LLM equipped tool give right answer. Now can see difference makes equipped tool: answer differs somewhat. case, ’s far, still incorrect. LLM equipped tools typically perform better across range different problems.","code":"do_math <- function(expression_to_evaluate) {      res <- eval(parse(text=expression_to_evaluate))   return(paste0(\"{'expression':'\",expression_to_evaluate,\",'result':'\",res,\"'}\"))    } tool_list <- list(   list(type=\"function\",        \"function\"=list(          name=\"do_math\",          description=\"Do simple math calculations by providing a math expression to evaluate\",          parameters= list(            type=\"object\",            properties = list(              expression_to_evaluate=list(                type=\"string\",                description=\"the mathematical expression to evaluate, without an equal sign\"              )            ),            required=list(\"expression_to_evaluate\")          ))   ) ) myflow_math <- ai_workflow() |>   set_connector(\"ollama\")  |>   set_model(model_name= \"llama3.2:latest\") |>    set_n_predict(1000) |>   set_temperature(0.2) |>    set_style_of_voice(\"Snoop Dogg\") |>    add_tools_declaration(tools = tool_list) #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434.  myflow_math |>   process_prompts(prompts_vector = \"Can you help me solve this math problem? How much is 1321212* 3 , and dividing this whole thing by 7 in the end?\") |>   pull_final_answer() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> → Adding tools #> [1] \"Yo, what's good fam? So you got a math problem that's like, \\\"Hey Snoop, can you help me out?\\\" Alright, let's get to it.\\n\\nSo, we gotta multiply 1,321,212 by 3 first, ya dig? That's like, 3 times the number, foo'!\\n\\n1321212 * 3 = 3,966,336\\n\\nNow that we got our product, we need to divide it by 7. You feel me?\\n\\n(3,966,336) / 7 = 566,233.71 (rounded out)\\n\\nSo there you have it, my homie! The answer is like... 566,234 or somethin' close to that. Word.\" myflow_clueless_at_math <- ai_workflow() |>   set_connector(\"ollama\")  |>   set_model(model_name= \"llama3.2:latest\") |>    set_n_predict(1000) |>   set_temperature(0.2) |>    set_style_of_voice(\"Snoop Dogg\")  #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434.  myflow_clueless_at_math |>   process_prompts(prompts_vector = \"Can you help me solve this math problem? How much is 1321212* 3 , and dividing this whole thing by 7 in the end?\") |>   pull_final_answer() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> [1] \"Yo, what's good fam? It's your boy Snoop D-O-double-G here, and I'm 'bout to break down that math problem for ya. Alright, let's get into it.\\n\\nFirst off, we gotta multiply 1321212 by 3, foo'. That means we're gonna add 1321212 together three times: \\n\\n1321212 * 1 = 1321212\\n1321212 * 2 = 2642424\\n1321212 * 3 = 3963636\\n\\nNow that we got our product, let's move on to the next step. We gotta divide it by 7, G.\\n\\nSo, we're gonna take 3963636 and see how many times 7 fits into it:\\n\\n3963636 ÷ 7 = 567516 (with a remainder of none)\\n\\nWord up, my dude! The answer is 567516 with no extra crumbs left over. You feel me?\""},{"path":"https://r-guyot.github.io/aiworkflow/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Raphael Guyot. Author, maintainer.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Guyot R (2024). aiworkflow: Enable R users leverage AI frameworks like Ollama Llama.cpp. R package version 0.1.2.1, https://r-guyot.github.io/aiworkflow/.","code":"@Manual{,   title = {aiworkflow: Enable R users to leverage AI frameworks like Ollama and Llama.cpp},   author = {Raphael Guyot},   year = {2024},   note = {R package version 0.1.2.1},   url = {https://r-guyot.github.io/aiworkflow/}, }"},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"aiworkflow-package-for-r","dir":"","previous_headings":"","what":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"package, aiworkflow, aimed making simple interact local LLMs using R. package aimed creating chat clients (definitely support use case) rather executing LLMs large amount data, reproducible way. Think executing LLMs NLP tasks dataframes typical use case. package much alpha stages. can already lot things, lacks complete testing coverage, API package subject change. aware functions may change deprecated 1.0 version reached.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"can use install github version directly devtools: pak: currently CRAN package may change near future.","code":"devtools::install_github(\"r-guyot/aiworkflow\") pak::pkg_install(\"r-guyot/aiworkflow\")"},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"LGPL v3, means, layman terms: share redistribute modification make package free use package -power applications, whether Open-source proprietary. release code proprietary apps use package. Please refer full details LICENSE document case.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"requirements","dir":"","previous_headings":"","what":"Requirements","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"need least: Ollama instance running machine, along embedding model LLM already downloaded . able use package moment. qdrant instance optional.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"current-features","dir":"","previous_headings":"","what":"Current Features","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"current version brings following features: pipes support LLM operations client Ollama run local LLM operations client Qdrant database store vector embeddings support basic RAG support tool calling LLMs support (like Llama3.1) support local vector embeddings database using feather file numerous processing skills (pre-defined prompts) can used box support chaining multiple LLM operations pipes support numerous prompt modification functions (audience, role, style, etc…) support JSON output extraction probably …","code":""},{"path":[]},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"cran","dir":"","previous_headings":"Upcoming Features","what":"CRAN","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"goal published CRAN robust enough complete shape.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"llm-backend-support","dir":"","previous_headings":"Upcoming Features","what":"LLM Backend support","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"Ultimately idea package expand backends run LLMs: llama.cpp VLLM llamafile","code":""},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"vision-llm-support","dir":"","previous_headings":"Upcoming Features","what":"Vision LLM Support","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"package also support Vision models accept images inputs, road.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"image-generation-support","dir":"","previous_headings":"Upcoming Features","what":"Image Generation Support","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"package eventually also support image generation ComfyUI API (likely).","code":""},{"path":"https://r-guyot.github.io/aiworkflow/index.html","id":"contributions","dir":"","previous_headings":"","what":"Contributions","title":"Enable R users to leverage AI frameworks like Ollama and Llama.cpp","text":"interested contribute package, welcome issue PR. Please also consider filing requests new features course bug reports.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_api_key_header.html","id":null,"dir":"Reference","previous_headings":"","what":"Adds API key header to the qdrant connection for security — add_api_key_header","title":"Adds API key header to the qdrant connection for security — add_api_key_header","text":"add_api_key_header adds security header connect Qdrant instance API key.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_api_key_header.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adds API key header to the qdrant connection for security — add_api_key_header","text":"","code":"add_api_key_header(req, conn)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_api_key_header.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adds API key header to the qdrant connection for security — add_api_key_header","text":"req httr2 request object prepared Qdrant REST API calls. conn connection object created get_qdrant_connection()","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_api_key_header.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adds API key header to the qdrant connection for security — add_api_key_header","text":"Adds security header connect Qdrant instance API key. API key value provided connection object obtained get_qdrant_connection(). function returns request object along necessary header.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_context.html","id":null,"dir":"Reference","previous_headings":"","what":"Adds context to be used by the model when answering — add_context","title":"Adds context to be used by the model when answering — add_context","text":"add_context lets add context info (shape context dataframe containing embeddings text info) supplement narrow answers LLM.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_context.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adds context to be used by the model when answering — add_context","text":"","code":"add_context(workflow_obj, context_df)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_context.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adds context to be used by the model when answering — add_context","text":"workflow_obj workflow object containing parameters describing workflow required context_df dataframe containing text embeddings text information can used retrieve relevant context","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_context.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adds context to be used by the model when answering — add_context","text":"workflow object new added context parameter.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_context.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adds context to be used by the model when answering — add_context","text":"work need add context_df build using generate_document_embeddings function.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_context.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adds context to be used by the model when answering — add_context","text":"","code":"conn <- get_ollama_connection()  document <- \"Standing proudly on the Île de la Cité in the heart of Paris,  France's capital city, lies one of the world's most beloved and historic  landmarks: the magnificent Notre Dame Cathedral. This Gothic masterpiece  has been welcoming pilgrims and tourists alike for over 850 years, since its  construction began in 1163 under King Louis VII. With its towering spires,  stunning stained glass windows, and intricate stone carvings, this beautiful  church is a testament to medieval architecture and engineering skill.  Unfortunately, a devastating fire ravaged the cathedral on April 15, 2019,  but thanks to swift action from firefighters and restoration efforts  underway, Notre Dame continues to inspire awe in those who visit her.\"  writeLines(document, con = \"doc1.txt\")  write_vectors_to_feather_file(file_name = \"doc1.feather\", vector_data = generate_document_embeddings(conn,  document_path = \"doc1.txt\", splitter = \"paragraph\"))  notre_dame_embeddings <- load_context_embeddings_from_feather_files(filenames = \"doc1.feather\")  my_workflow <- ai_workflow() |>  set_system_prompt(system_prompt=\"You are a helpful AI assistant.  Answer to the best of your knowledge.\") |> add_context(context_df = notre_dame_embeddings)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_tools_declaration.html","id":null,"dir":"Reference","previous_headings":"","what":"Add tools declaration for the LLM to use — add_tools_declaration","title":"Add tools declaration for the LLM to use — add_tools_declaration","text":"add_tools_declaration lets add tools (functions) can used needed LLM answer specific questions","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_tools_declaration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add tools declaration for the LLM to use — add_tools_declaration","text":"","code":"add_tools_declaration(workflow_obj, tools)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_tools_declaration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add tools declaration for the LLM to use — add_tools_declaration","text":"workflow_obj workflow object containing parameters describing workflow required tools list tools declared R list, see examples.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_tools_declaration.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add tools declaration for the LLM to use — add_tools_declaration","text":"Lets add tools (functions) can used LLM. works models tool calling function supported, case Llama3.1 example. structure expected R list.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_tools_declaration.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add tools declaration for the LLM to use — add_tools_declaration","text":"","code":"tool_list <- list(  list(type=\"function\",       \"function\"=list(         name=\"get_flight_times\",         description=\"get the flight times between two cities\",         parameters= list(           type=\"object\",           properties = list(             departure=list(               type=\"string\",               description=\"the departure city (airport code)\"             ),             arrival=list(               type=\"string\",               description=\"the arrival city (airport code)\"             )           ),           required=c(\"departure\", \"arrival\")         )       )))  myflow_test <- ai_workflow() |>    set_connector(\"ollama\")  |>     set_model(model_name= \"llama3.1:8b-instruct-q5_K_M\") |>    set_n_predict(1000) |>    set_temperature(0.8) |>     set_default_missing_parameters_in_workflow() |>     add_tools_declaration(tool_list) #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'."},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_workflow_step.html","id":null,"dir":"Reference","previous_headings":"","what":"Add a step (i.e. another workflow) to an existing workflow — add_workflow_step","title":"Add a step (i.e. another workflow) to an existing workflow — add_workflow_step","text":"add_workflow_step adds another workflow existing one. default chains new workflow previous one(s).","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_workflow_step.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add a step (i.e. another workflow) to an existing workflow — add_workflow_step","text":"","code":"add_workflow_step(workflow_obj, workflow_obj_to_add, type = \"chain\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_workflow_step.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add a step (i.e. another workflow) to an existing workflow — add_workflow_step","text":"workflow_obj previous workflow object want build workflow_obj_to_add workflow object want add top existing one type type step want add existing workflow. Defaults \"chain\".","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_workflow_step.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add a step (i.e. another workflow) to an existing workflow — add_workflow_step","text":"function add new workflow existing one. default way new workflow added chaining previous one. way works use previous output last workflow element input next one.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/add_workflow_step.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add a step (i.e. another workflow) to an existing workflow — add_workflow_step","text":"","code":"myflow_template <- ai_workflow() |>  set_connector(\"ollama\") |>   set_model(model_name= \"llama3.1:8b-instruct-q5_K_M\") |>    set_n_predict(1000) |>    set_temperature(0.8)  #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434."},{"path":"https://r-guyot.github.io/aiworkflow/reference/ai_workflow.html","id":null,"dir":"Reference","previous_headings":"","what":"Define AI workflow — ai_workflow","title":"Define AI workflow — ai_workflow","text":"ai_workflow creates list encapsulates parameters related AI workflow want design.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/ai_workflow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define AI workflow — ai_workflow","text":"","code":"ai_workflow()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/ai_workflow.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define AI workflow — ai_workflow","text":"empty list ai_workflow() format used capture settings parameters workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/ai_workflow.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Define AI workflow — ai_workflow","text":"starting point defining AI workflow package. workflow object contains connector model placeholders.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/ai_workflow.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define AI workflow — ai_workflow","text":"","code":"my_workflow <- ai_workflow()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/aiworkflow-package.html","id":null,"dir":"Reference","previous_headings":"","what":"aiworkflow: Enable R users to leverage AI frameworks like Ollama and Llama.cpp — aiworkflow-package","title":"aiworkflow: Enable R users to leverage AI frameworks like Ollama and Llama.cpp — aiworkflow-package","text":"ai_workflow R package makes easy run typical AI-powered (mostly LLM-related) tasks R. main focus Local AI support, can expanded third-party LLMs future.","code":""},{"path":[]},{"path":"https://r-guyot.github.io/aiworkflow/reference/aiworkflow-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"aiworkflow: Enable R users to leverage AI frameworks like Ollama and Llama.cpp — aiworkflow-package","text":"Maintainer: Raphael Guyot rguyot@sanqualis.com","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/apply_processing_skill.html","id":null,"dir":"Reference","previous_headings":"","what":"Applies a processing skill to the current workflow — apply_processing_skill","title":"Applies a processing skill to the current workflow — apply_processing_skill","text":"apply_processing_skill applies processing skill (.e. pre-engineered prompt)","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/apply_processing_skill.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Applies a processing skill to the current workflow — apply_processing_skill","text":"","code":"apply_processing_skill(   prompts_vector,   processing_skill = NA,   processing_skill_args = list() )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/apply_processing_skill.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Applies a processing skill to the current workflow — apply_processing_skill","text":"prompts_vector vector containing existing prompts text skill applied. processing_skill character vector containing filepath processing skill text template skill applied. processing_skill_args list additional parameters feed processing skill, accepts .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/apply_processing_skill.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Applies a processing skill to the current workflow — apply_processing_skill","text":"new vector containing modified prompts applying processing skill template.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/apply_processing_skill.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Applies a processing skill to the current workflow — apply_processing_skill","text":"simple function apply pre-defined prompt format (.e. processing skill) current workflow. can find existing processing skills using list_processing_skills() function. function usually made used , set_processing_skill() function","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_batch_documents_to_embeddings.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Batch documents to Embeddings — convert_batch_documents_to_embeddings","title":"Convert Batch documents to Embeddings — convert_batch_documents_to_embeddings","text":"convert_batch_documents_to_embeddings converts whole batch documents vector embeddings ","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_batch_documents_to_embeddings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Batch documents to Embeddings — convert_batch_documents_to_embeddings","text":"","code":"convert_batch_documents_to_embeddings(   ollama_connection,   document_path_list,   splitter = \"paragraph\",   model = \"bge-large\" )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_batch_documents_to_embeddings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Batch documents to Embeddings — convert_batch_documents_to_embeddings","text":"ollama_connection connector object ollama instance provide embeddings. document_path_list list containing full paths files need converted embeddings. splitter splitter type use preparing embeddings. Defaults \"paragraph\". \"sentence\" can also used. model model use creating embeddings. Defaults \"bge-large\"","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_batch_documents_to_embeddings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert Batch documents to Embeddings — convert_batch_documents_to_embeddings","text":"Lets convert many documents necessary targeting path list documents, can easily generated functions manually. returns dataframe end can used provide context LLM workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_batch_documents_to_embeddings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert Batch documents to Embeddings — convert_batch_documents_to_embeddings","text":"","code":"conn <- get_ollama_connection()  document <- \"Standing proudly on the Île de la Cité in the heart of Paris,  France's capital city, lies one of the world's most beloved and historic  landmarks: the magnificent Notre Dame Cathedral. This Gothic masterpiece  has been welcoming pilgrims and tourists alike for over 850 years, since its  construction began in 1163 under King Louis VII. With its towering spires,  stunning stained glass windows, and intricate stone carvings, this beautiful  church is a testament to medieval architecture and engineering skill.  Unfortunately, a devastating fire ravaged the cathedral on April 15, 2019,  but thanks to swift action from firefighters and restoration efforts  underway, Notre Dame continues to inspire awe in those who visit her.\"  writeLines(document, con = \"doc1.txt\")  convert_batch_documents_to_embeddings(ollama_connection = conn,  document_path_list = list(\"doc1.txt\")) #>      embeddings #>          <AsIs> #> 1: -0.02001.... #>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         text #>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       <char> #> 1: Standing proudly on the Île de la Cité in the heart of Paris,  France's capital city, lies one of the world's most beloved and historic  landmarks: the magnificent Notre Dame Cathedral. This Gothic masterpiece  has been welcoming pilgrims and tourists alike for over 850 years, since its  construction began in 1163 under King Louis VII. With its towering spires,  stunning stained glass windows, and intricate stone carvings, this beautiful  church is a testament to medieval architecture and engineering skill.  Unfortunately, a devastating fire ravaged the cathedral on April 15, 2019,  but thanks to swift action from firefighters and restoration efforts  underway, Notre Dame continues to inspire awe in those who visit her."},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_embeddings_to_qdrant_format.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert embeddings to Qdrant format — convert_embeddings_to_qdrant_format","title":"Convert embeddings to Qdrant format — convert_embeddings_to_qdrant_format","text":"convert_embeddings_to_qdrant_format converts embeddings expected qdrant format upserting. '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_embeddings_to_qdrant_format.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert embeddings to Qdrant format — convert_embeddings_to_qdrant_format","text":"","code":"convert_embeddings_to_qdrant_format(input)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_embeddings_to_qdrant_format.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert embeddings to Qdrant format — convert_embeddings_to_qdrant_format","text":"input embeddings resulting generate_document_embeddings() function.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_embeddings_to_qdrant_format.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert embeddings to Qdrant format — convert_embeddings_to_qdrant_format","text":"Embeddings generated generate_document_embeddings() function different structure. make embeddings directly usable qdrant function can use make seamless.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_embeddings_to_qdrant_format.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert embeddings to Qdrant format — convert_embeddings_to_qdrant_format","text":"","code":"sentence <- \"hi there this is a great day\" tmp_path <- tempfile() writeLines(sentence, tmp_path) conn <- get_ollama_connection() doc_embeddings <- generate_document_embeddings(ollama_connection = conn,document_path = tmp_path) embeddings_for_qdrant <- doc_embeddings |> convert_embeddings_to_qdrant_format()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_completion_response_to_tibble.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert an ollama server completion response to a tibble — convert_ollama_completion_response_to_tibble","title":"Convert an ollama server completion response to a tibble — convert_ollama_completion_response_to_tibble","text":"get_ollama_completion get completion ollama server API","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_completion_response_to_tibble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert an ollama server completion response to a tibble — convert_ollama_completion_response_to_tibble","text":"","code":"convert_ollama_completion_response_to_tibble(ollama_response)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_completion_response_to_tibble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert an ollama server completion response to a tibble — convert_ollama_completion_response_to_tibble","text":"ollama_response contain full response ollama API completion call.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_completion_response_to_tibble.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert an ollama server completion response to a tibble — convert_ollama_completion_response_to_tibble","text":"function convert full output ollama tibble format","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_model_info_response_to_tibble.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert ollama response for model info to a tibble — convert_ollama_model_info_response_to_tibble","title":"Convert ollama response for model info to a tibble — convert_ollama_model_info_response_to_tibble","text":"convert_ollama_model_info_response_to_tibble Converts ollama response model info tibble","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_model_info_response_to_tibble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert ollama response for model info to a tibble — convert_ollama_model_info_response_to_tibble","text":"","code":"convert_ollama_model_info_response_to_tibble(ollama_response)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_model_info_response_to_tibble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert ollama response for model info to a tibble — convert_ollama_model_info_response_to_tibble","text":"ollama_response response object coming payload delivered ollama server instance.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_model_info_response_to_tibble.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert ollama response for model info to a tibble — convert_ollama_model_info_response_to_tibble","text":"Converts ollama response model info tibble","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_tags_response_to_tibble.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert an ollama server tags response to a tibble — convert_ollama_tags_response_to_tibble","title":"Convert an ollama server tags response to a tibble — convert_ollama_tags_response_to_tibble","text":"convert_ollama_tags_response_to_tibble get completion ollama server API","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_tags_response_to_tibble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert an ollama server tags response to a tibble — convert_ollama_tags_response_to_tibble","text":"","code":"convert_ollama_tags_response_to_tibble(ollama_response)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_tags_response_to_tibble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert an ollama server tags response to a tibble — convert_ollama_tags_response_to_tibble","text":"ollama_response contain full response ollama API call tags.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/convert_ollama_tags_response_to_tibble.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert an ollama server tags response to a tibble — convert_ollama_tags_response_to_tibble","text":"function convert full output ollama tibble format","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/display_intermediate_answer.html","id":null,"dir":"Reference","previous_headings":"","what":"Display Intermediate Answer — display_intermediate_answer","title":"Display Intermediate Answer — display_intermediate_answer","text":"display_intermediate_answer function displays results last executed workflow","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/display_intermediate_answer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Display Intermediate Answer — display_intermediate_answer","text":"","code":"display_intermediate_answer(workflow)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/display_intermediate_answer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Display Intermediate Answer — display_intermediate_answer","text":"workflow workflow object containing parameters describing flow required","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/display_intermediate_answer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Display Intermediate Answer — display_intermediate_answer","text":"function print output last workflow executed. Especially helpful chaining several workflows.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/execute_workflow.html","id":null,"dir":"Reference","previous_headings":"","what":"Execute an AI workflow — execute_workflow","title":"Execute an AI workflow — execute_workflow","text":"execute_workflow executes AI workflow combining prompt vectors workflow object.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/execute_workflow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Execute an AI workflow — execute_workflow","text":"","code":"execute_workflow(prompts_vector, workflow_obj)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/execute_workflow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Execute an AI workflow — execute_workflow","text":"prompts_vector vector containing prompts executed AI workflow workflow_obj workflow object containing parameters describing flow required","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/execute_workflow.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Execute an AI workflow — execute_workflow","text":"function executes AI workflow combining prompt vectors workflow object.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/extract_snippets.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Snippets — extract_snippets","title":"Extract Snippets — extract_snippets","text":"extract_snippets makes easy extract snippets generated answers LLM","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/extract_snippets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Snippets — extract_snippets","text":"","code":"extract_snippets(text)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/extract_snippets.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Snippets — extract_snippets","text":"text text extract snippets .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/extract_snippets.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Snippets — extract_snippets","text":"function makes easy extract snippets generated answers LLM","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/extract_snippets.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Snippets — extract_snippets","text":"","code":"llm_answer <- \"This is a simple function in r: ``` yo <- function(message) { print(message) } ``` Enjoy this function!\"  extracted_function <- llm_answer |> extract_snippets()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_document_embeddings.html","id":null,"dir":"Reference","previous_headings":"","what":"Get embeddings for a piece of context through an ollama server instance020 — generate_document_embeddings","title":"Get embeddings for a piece of context through an ollama server instance020 — generate_document_embeddings","text":"generate_document_embeddings Generates vector embeddings document ollama","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_document_embeddings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get embeddings for a piece of context through an ollama server instance020 — generate_document_embeddings","text":"","code":"generate_document_embeddings(   ollama_connection,   document_path,   splitter = \"sentence\",   model = \"bge-large\" )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_document_embeddings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get embeddings for a piece of context through an ollama server instance020 — generate_document_embeddings","text":"ollama_connection connection object information connect ollama server document_path document path create embeddings splitter type splitter going used. Defaults \"sentence\". \"paragraph\" can also used. model model used generates embeddings. Defaults \"bge-large\". can select model available generating embeddings.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_document_embeddings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get embeddings for a piece of context through an ollama server instance020 — generate_document_embeddings","text":"Converts ollama response model info tibble","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_document_embeddings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get embeddings for a piece of context through an ollama server instance020 — generate_document_embeddings","text":"","code":"#conn<-get_ollama_connection() #embeddings<- get_ollama_embeddings(ollama_connection=conn, prompt=\"Roma is a beautiful city.\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_numeric_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Numeric List — generate_numeric_list","title":"Generate Numeric List — generate_numeric_list","text":"generate_numeric_list generates random list simulate vector embeddings look like","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_numeric_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Numeric List — generate_numeric_list","text":"","code":"generate_numeric_list(decimals = 1, length)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_numeric_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Numeric List — generate_numeric_list","text":"decimals number decimals needed figure. Defaults 1. length length list want produce","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_numeric_list.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Numeric List — generate_numeric_list","text":"Generates list numerical values specific length. can used simulate vector embeddings look like.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_numeric_list.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Numeric List — generate_numeric_list","text":"","code":"generate_numeric_list(decimals=1, length=384) #> [[1]] #> [1] 0.1 #>  #> [[2]] #> [1] 0.8 #>  #> [[3]] #> [1] 0.6 #>  #> [[4]] #> [1] 0.2 #>  #> [[5]] #> [1] 0 #>  #> [[6]] #> [1] 0.5 #>  #> [[7]] #> [1] 0.5 #>  #> [[8]] #> [1] 0.3 #>  #> [[9]] #> [1] 0.7 #>  #> [[10]] #> [1] 0.8 #>  #> [[11]] #> [1] 0.9 #>  #> [[12]] #> [1] 0.2 #>  #> [[13]] #> [1] 0 #>  #> [[14]] #> [1] 0.3 #>  #> [[15]] #> [1] 0.4 #>  #> [[16]] #> [1] 0.2 #>  #> [[17]] #> [1] 0.4 #>  #> [[18]] #> [1] 0.1 #>  #> [[19]] #> [1] 0.4 #>  #> [[20]] #> [1] 1 #>  #> [[21]] #> [1] 0.3 #>  #> [[22]] #> [1] 0.7 #>  #> [[23]] #> [1] 0.7 #>  #> [[24]] #> [1] 0.2 #>  #> [[25]] #> [1] 1 #>  #> [[26]] #> [1] 0.7 #>  #> [[27]] #> [1] 0.1 #>  #> [[28]] #> [1] 0.5 #>  #> [[29]] #> [1] 0.7 #>  #> [[30]] #> [1] 0.7 #>  #> [[31]] #> [1] 0 #>  #> [[32]] #> [1] 0.2 #>  #> [[33]] #> [1] 0.3 #>  #> [[34]] #> [1] 0.6 #>  #> [[35]] #> [1] 0.5 #>  #> [[36]] #> [1] 0.4 #>  #> [[37]] #> [1] 0.7 #>  #> [[38]] #> [1] 0.9 #>  #> [[39]] #> [1] 0.2 #>  #> [[40]] #> [1] 0.2 #>  #> [[41]] #> [1] 0.7 #>  #> [[42]] #> [1] 0.5 #>  #> [[43]] #> [1] 0.6 #>  #> [[44]] #> [1] 0.7 #>  #> [[45]] #> [1] 0.1 #>  #> [[46]] #> [1] 0.8 #>  #> [[47]] #> [1] 0.8 #>  #> [[48]] #> [1] 1 #>  #> [[49]] #> [1] 1 #>  #> [[50]] #> [1] 0.4 #>  #> [[51]] #> [1] 0.5 #>  #> [[52]] #> [1] 0.3 #>  #> [[53]] #> [1] 0.2 #>  #> [[54]] #> [1] 0.5 #>  #> [[55]] #> [1] 0.5 #>  #> [[56]] #> [1] 0.8 #>  #> [[57]] #> [1] 0.2 #>  #> [[58]] #> [1] 0.7 #>  #> [[59]] #> [1] 0.1 #>  #> [[60]] #> [1] 0.4 #>  #> [[61]] #> [1] 0.8 #>  #> [[62]] #> [1] 0.3 #>  #> [[63]] #> [1] 0.6 #>  #> [[64]] #> [1] 0.3 #>  #> [[65]] #> [1] 0.6 #>  #> [[66]] #> [1] 0.2 #>  #> [[67]] #> [1] 0.9 #>  #> [[68]] #> [1] 0.5 #>  #> [[69]] #> [1] 0.5 #>  #> [[70]] #> [1] 0.3 #>  #> [[71]] #> [1] 0.4 #>  #> [[72]] #> [1] 0.4 #>  #> [[73]] #> [1] 0 #>  #> [[74]] #> [1] 0.5 #>  #> [[75]] #> [1] 0.4 #>  #> [[76]] #> [1] 0 #>  #> [[77]] #> [1] 0.4 #>  #> [[78]] #> [1] 0.6 #>  #> [[79]] #> [1] 0.9 #>  #> [[80]] #> [1] 0.4 #>  #> [[81]] #> [1] 0.5 #>  #> [[82]] #> [1] 0.6 #>  #> [[83]] #> [1] 0.3 #>  #> [[84]] #> [1] 0.3 #>  #> [[85]] #> [1] 0.5 #>  #> [[86]] #> [1] 0.9 #>  #> [[87]] #> [1] 0.4 #>  #> [[88]] #> [1] 0.2 #>  #> [[89]] #> [1] 0.7 #>  #> [[90]] #> [1] 0.1 #>  #> [[91]] #> [1] 1 #>  #> [[92]] #> [1] 0.1 #>  #> [[93]] #> [1] 0.5 #>  #> [[94]] #> [1] 0.8 #>  #> [[95]] #> [1] 0.7 #>  #> [[96]] #> [1] 0.2 #>  #> [[97]] #> [1] 0.5 #>  #> [[98]] #> [1] 0.8 #>  #> [[99]] #> [1] 0 #>  #> [[100]] #> [1] 0.5 #>  #> [[101]] #> [1] 0.8 #>  #> [[102]] #> [1] 0.8 #>  #> [[103]] #> [1] 0.4 #>  #> [[104]] #> [1] 0.2 #>  #> [[105]] #> [1] 0.4 #>  #> [[106]] #> [1] 0.7 #>  #> [[107]] #> [1] 0.5 #>  #> [[108]] #> [1] 0.7 #>  #> [[109]] #> [1] 0.5 #>  #> [[110]] #> [1] 0.8 #>  #> [[111]] #> [1] 0.7 #>  #> [[112]] #> [1] 0.9 #>  #> [[113]] #> [1] 0 #>  #> [[114]] #> [1] 0.9 #>  #> [[115]] #> [1] 1 #>  #> [[116]] #> [1] 0.5 #>  #> [[117]] #> [1] 0.4 #>  #> [[118]] #> [1] 0.8 #>  #> [[119]] #> [1] 0.6 #>  #> [[120]] #> [1] 0.6 #>  #> [[121]] #> [1] 0.9 #>  #> [[122]] #> [1] 0.6 #>  #> [[123]] #> [1] 0.3 #>  #> [[124]] #> [1] 0.9 #>  #> [[125]] #> [1] 0.9 #>  #> [[126]] #> [1] 0.2 #>  #> [[127]] #> [1] 0.4 #>  #> [[128]] #> [1] 0.8 #>  #> [[129]] #> [1] 0.1 #>  #> [[130]] #> [1] 0.2 #>  #> [[131]] #> [1] 0.2 #>  #> [[132]] #> [1] 0.7 #>  #> [[133]] #> [1] 0.9 #>  #> [[134]] #> [1] 0.9 #>  #> [[135]] #> [1] 0.6 #>  #> [[136]] #> [1] 0.6 #>  #> [[137]] #> [1] 0.7 #>  #> [[138]] #> [1] 0.2 #>  #> [[139]] #> [1] 0 #>  #> [[140]] #> [1] 0.9 #>  #> [[141]] #> [1] 0.1 #>  #> [[142]] #> [1] 1 #>  #> [[143]] #> [1] 0.6 #>  #> [[144]] #> [1] 0.2 #>  #> [[145]] #> [1] 0.9 #>  #> [[146]] #> [1] 0 #>  #> [[147]] #> [1] 0.6 #>  #> [[148]] #> [1] 0.2 #>  #> [[149]] #> [1] 0.9 #>  #> [[150]] #> [1] 0.8 #>  #> [[151]] #> [1] 0.7 #>  #> [[152]] #> [1] 0.2 #>  #> [[153]] #> [1] 0.7 #>  #> [[154]] #> [1] 0.2 #>  #> [[155]] #> [1] 0.5 #>  #> [[156]] #> [1] 0.3 #>  #> [[157]] #> [1] 0.6 #>  #> [[158]] #> [1] 0.3 #>  #> [[159]] #> [1] 0.5 #>  #> [[160]] #> [1] 0.9 #>  #> [[161]] #> [1] 1 #>  #> [[162]] #> [1] 0.8 #>  #> [[163]] #> [1] 0.9 #>  #> [[164]] #> [1] 0.6 #>  #> [[165]] #> [1] 0.8 #>  #> [[166]] #> [1] 1 #>  #> [[167]] #> [1] 0.7 #>  #> [[168]] #> [1] 0.2 #>  #> [[169]] #> [1] 0.3 #>  #> [[170]] #> [1] 0.7 #>  #> [[171]] #> [1] 0.9 #>  #> [[172]] #> [1] 0.2 #>  #> [[173]] #> [1] 0 #>  #> [[174]] #> [1] 0.9 #>  #> [[175]] #> [1] 0.4 #>  #> [[176]] #> [1] 0.8 #>  #> [[177]] #> [1] 0.7 #>  #> [[178]] #> [1] 0.4 #>  #> [[179]] #> [1] 0.4 #>  #> [[180]] #> [1] 0.7 #>  #> [[181]] #> [1] 0.7 #>  #> [[182]] #> [1] 0.1 #>  #> [[183]] #> [1] 0.9 #>  #> [[184]] #> [1] 0.1 #>  #> [[185]] #> [1] 0.9 #>  #> [[186]] #> [1] 0.1 #>  #> [[187]] #> [1] 0.5 #>  #> [[188]] #> [1] 0.2 #>  #> [[189]] #> [1] 0.7 #>  #> [[190]] #> [1] 0.2 #>  #> [[191]] #> [1] 1 #>  #> [[192]] #> [1] 0.3 #>  #> [[193]] #> [1] 0.4 #>  #> [[194]] #> [1] 0.9 #>  #> [[195]] #> [1] 0.8 #>  #> [[196]] #> [1] 0.1 #>  #> [[197]] #> [1] 0.9 #>  #> [[198]] #> [1] 0.8 #>  #> [[199]] #> [1] 0.4 #>  #> [[200]] #> [1] 1 #>  #> [[201]] #> [1] 0.3 #>  #> [[202]] #> [1] 0.9 #>  #> [[203]] #> [1] 0.2 #>  #> [[204]] #> [1] 0 #>  #> [[205]] #> [1] 0.1 #>  #> [[206]] #> [1] 0.9 #>  #> [[207]] #> [1] 0.2 #>  #> [[208]] #> [1] 0.2 #>  #> [[209]] #> [1] 0 #>  #> [[210]] #> [1] 0.9 #>  #> [[211]] #> [1] 0.2 #>  #> [[212]] #> [1] 0.8 #>  #> [[213]] #> [1] 0.3 #>  #> [[214]] #> [1] 0.9 #>  #> [[215]] #> [1] 0.3 #>  #> [[216]] #> [1] 1 #>  #> [[217]] #> [1] 0.6 #>  #> [[218]] #> [1] 0.8 #>  #> [[219]] #> [1] 0.8 #>  #> [[220]] #> [1] 0.8 #>  #> [[221]] #> [1] 0.4 #>  #> [[222]] #> [1] 0.1 #>  #> [[223]] #> [1] 0.1 #>  #> [[224]] #> [1] 0.7 #>  #> [[225]] #> [1] 0.6 #>  #> [[226]] #> [1] 0.7 #>  #> [[227]] #> [1] 0.3 #>  #> [[228]] #> [1] 1 #>  #> [[229]] #> [1] 0.7 #>  #> [[230]] #> [1] 0.9 #>  #> [[231]] #> [1] 1 #>  #> [[232]] #> [1] 0.2 #>  #> [[233]] #> [1] 0.7 #>  #> [[234]] #> [1] 0.4 #>  #> [[235]] #> [1] 0 #>  #> [[236]] #> [1] 0.6 #>  #> [[237]] #> [1] 0.6 #>  #> [[238]] #> [1] 0.4 #>  #> [[239]] #> [1] 0.9 #>  #> [[240]] #> [1] 0.5 #>  #> [[241]] #> [1] 1 #>  #> [[242]] #> [1] 0 #>  #> [[243]] #> [1] 0.7 #>  #> [[244]] #> [1] 0.4 #>  #> [[245]] #> [1] 0.9 #>  #> [[246]] #> [1] 0.7 #>  #> [[247]] #> [1] 0.7 #>  #> [[248]] #> [1] 0.8 #>  #> [[249]] #> [1] 0.5 #>  #> [[250]] #> [1] 0.2 #>  #> [[251]] #> [1] 0.5 #>  #> [[252]] #> [1] 0.4 #>  #> [[253]] #> [1] 0.7 #>  #> [[254]] #> [1] 0.6 #>  #> [[255]] #> [1] 0.4 #>  #> [[256]] #> [1] 1 #>  #> [[257]] #> [1] 0.3 #>  #> [[258]] #> [1] 0.1 #>  #> [[259]] #> [1] 0.6 #>  #> [[260]] #> [1] 0.2 #>  #> [[261]] #> [1] 0.1 #>  #> [[262]] #> [1] 0.9 #>  #> [[263]] #> [1] 0.8 #>  #> [[264]] #> [1] 0.8 #>  #> [[265]] #> [1] 0.5 #>  #> [[266]] #> [1] 0.5 #>  #> [[267]] #> [1] 0.1 #>  #> [[268]] #> [1] 0.4 #>  #> [[269]] #> [1] 0.2 #>  #> [[270]] #> [1] 0.7 #>  #> [[271]] #> [1] 0.7 #>  #> [[272]] #> [1] 0.9 #>  #> [[273]] #> [1] 0.2 #>  #> [[274]] #> [1] 1 #>  #> [[275]] #> [1] 0.4 #>  #> [[276]] #> [1] 0.7 #>  #> [[277]] #> [1] 0.8 #>  #> [[278]] #> [1] 0.1 #>  #> [[279]] #> [1] 0.5 #>  #> [[280]] #> [1] 0.8 #>  #> [[281]] #> [1] 0.4 #>  #> [[282]] #> [1] 0.6 #>  #> [[283]] #> [1] 0.2 #>  #> [[284]] #> [1] 0 #>  #> [[285]] #> [1] 1 #>  #> [[286]] #> [1] 0.2 #>  #> [[287]] #> [1] 0.8 #>  #> [[288]] #> [1] 0.9 #>  #> [[289]] #> [1] 0.8 #>  #> [[290]] #> [1] 0.6 #>  #> [[291]] #> [1] 0.9 #>  #> [[292]] #> [1] 0 #>  #> [[293]] #> [1] 0.1 #>  #> [[294]] #> [1] 0.1 #>  #> [[295]] #> [1] 0.7 #>  #> [[296]] #> [1] 0.4 #>  #> [[297]] #> [1] 0.1 #>  #> [[298]] #> [1] 0.1 #>  #> [[299]] #> [1] 0.9 #>  #> [[300]] #> [1] 0.2 #>  #> [[301]] #> [1] 0.7 #>  #> [[302]] #> [1] 0.2 #>  #> [[303]] #> [1] 0.5 #>  #> [[304]] #> [1] 0.8 #>  #> [[305]] #> [1] 0.1 #>  #> [[306]] #> [1] 0.3 #>  #> [[307]] #> [1] 0.3 #>  #> [[308]] #> [1] 0 #>  #> [[309]] #> [1] 0.9 #>  #> [[310]] #> [1] 0.4 #>  #> [[311]] #> [1] 0.8 #>  #> [[312]] #> [1] 0.7 #>  #> [[313]] #> [1] 0.5 #>  #> [[314]] #> [1] 1 #>  #> [[315]] #> [1] 0.4 #>  #> [[316]] #> [1] 0.2 #>  #> [[317]] #> [1] 0.8 #>  #> [[318]] #> [1] 0 #>  #> [[319]] #> [1] 0.8 #>  #> [[320]] #> [1] 0.2 #>  #> [[321]] #> [1] 0 #>  #> [[322]] #> [1] 0.8 #>  #> [[323]] #> [1] 0.8 #>  #> [[324]] #> [1] 1 #>  #> [[325]] #> [1] 0.4 #>  #> [[326]] #> [1] 0.2 #>  #> [[327]] #> [1] 0.6 #>  #> [[328]] #> [1] 0.8 #>  #> [[329]] #> [1] 0 #>  #> [[330]] #> [1] 0.7 #>  #> [[331]] #> [1] 0.2 #>  #> [[332]] #> [1] 0 #>  #> [[333]] #> [1] 0.1 #>  #> [[334]] #> [1] 0.7 #>  #> [[335]] #> [1] 0.6 #>  #> [[336]] #> [1] 0.3 #>  #> [[337]] #> [1] 0.4 #>  #> [[338]] #> [1] 0.7 #>  #> [[339]] #> [1] 0.7 #>  #> [[340]] #> [1] 0.9 #>  #> [[341]] #> [1] 0.5 #>  #> [[342]] #> [1] 0.6 #>  #> [[343]] #> [1] 0.2 #>  #> [[344]] #> [1] 0.6 #>  #> [[345]] #> [1] 0.9 #>  #> [[346]] #> [1] 0.6 #>  #> [[347]] #> [1] 0.8 #>  #> [[348]] #> [1] 0.6 #>  #> [[349]] #> [1] 0.8 #>  #> [[350]] #> [1] 0.4 #>  #> [[351]] #> [1] 0.8 #>  #> [[352]] #> [1] 0.7 #>  #> [[353]] #> [1] 0.3 #>  #> [[354]] #> [1] 0.1 #>  #> [[355]] #> [1] 0.1 #>  #> [[356]] #> [1] 0.8 #>  #> [[357]] #> [1] 0.4 #>  #> [[358]] #> [1] 0.1 #>  #> [[359]] #> [1] 1 #>  #> [[360]] #> [1] 0.6 #>  #> [[361]] #> [1] 0.1 #>  #> [[362]] #> [1] 0.5 #>  #> [[363]] #> [1] 0.1 #>  #> [[364]] #> [1] 1 #>  #> [[365]] #> [1] 0 #>  #> [[366]] #> [1] 0.2 #>  #> [[367]] #> [1] 0.9 #>  #> [[368]] #> [1] 1 #>  #> [[369]] #> [1] 0.7 #>  #> [[370]] #> [1] 0.9 #>  #> [[371]] #> [1] 1 #>  #> [[372]] #> [1] 0 #>  #> [[373]] #> [1] 0.4 #>  #> [[374]] #> [1] 0.6 #>  #> [[375]] #> [1] 1 #>  #> [[376]] #> [1] 0.3 #>  #> [[377]] #> [1] 0.9 #>  #> [[378]] #> [1] 0.5 #>  #> [[379]] #> [1] 0.9 #>  #> [[380]] #> [1] 0.5 #>  #> [[381]] #> [1] 0.9 #>  #> [[382]] #> [1] 0.9 #>  #> [[383]] #> [1] 0.3 #>  #> [[384]] #> [1] 0 #>"},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_uuid_from_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate UUID from text — generate_uuid_from_text","title":"Generate UUID from text — generate_uuid_from_text","text":"generate_uuid_from_text generates unique UUID given piece text.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_uuid_from_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate UUID from text — generate_uuid_from_text","text":"","code":"generate_uuid_from_text(text = NA_character_)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_uuid_from_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate UUID from text — generate_uuid_from_text","text":"text piece text generate UUID. Defaults NA.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_uuid_from_text.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate UUID from text — generate_uuid_from_text","text":"Generates UUID piece text. text give id. useful generate id inserting vectors qdrant.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/generate_uuid_from_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate UUID from text — generate_uuid_from_text","text":"","code":"generate_uuid_from_text(\"hi there\") #> [1] \"9b96a1fe-1d54-4cbb-c960-cc6a0286668f\""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_list_ollama_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a list of models available from the ollama server — get_list_ollama_models","title":"Get a list of models available from the ollama server — get_list_ollama_models","text":"get_list_ollama_models gets list models available ollama server","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_list_ollama_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a list of models available from the ollama server — get_list_ollama_models","text":"","code":"get_list_ollama_models(ollama_connection)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_list_ollama_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a list of models available from the ollama server — get_list_ollama_models","text":"ollama_connection ollama connection objection object created get_ollama_connection().","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_list_ollama_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get a list of models available from the ollama server — get_list_ollama_models","text":"dataframe contains information LLM available running ollama server","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_list_ollama_models.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get a list of models available from the ollama server — get_list_ollama_models","text":"simple function get models available ollama server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_list_ollama_models.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get a list of models available from the ollama server — get_list_ollama_models","text":"","code":"ollama_conn <- get_ollama_connection() list_of_models <- get_list_ollama_models(ollama_conn)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_chat_completion.html","id":null,"dir":"Reference","previous_headings":"","what":"Get chat completion from ollama server — get_ollama_chat_completion","title":"Get chat completion from ollama server — get_ollama_chat_completion","text":"get_ollama_chat_completion get chat completion ollama server API","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_chat_completion.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get chat completion from ollama server — get_ollama_chat_completion","text":"","code":"get_ollama_chat_completion(   ollama_connection,   model,   prompts_vector,   output_text_only = F,   num_predict = 200,   temperature = 0.8,   role = \"user\",   repeat_penalty = 1.2,   seed = sample(1:1e+07, 1),   system_prompt = NA,   context_info = NA,   num_ctx = NA,   tools = NA )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_chat_completion.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get chat completion from ollama server — get_ollama_chat_completion","text":"ollama_connection connection object information connect ollama server model Name model use ollama server. Note can use models available. prompts_vector vector containing one Messages acting prompt completion. output_text_only Boolean value (default False) indicating just want text message output (TRUE) whole response coming server. num_predict number tokens generate response (maximum amount). Defaults 200. temperature temperature value answer model. temperature 0 gives always answer. temperature 1 lot variation. Default 0.8. role role taken chat prompt. Typically \"user\", can also \"assistant\" \"system\". Defaults \"user\". repeat_penalty penalty give model avoid repeats tokens. Default 1.2. seed seed used generate answer. Note seed effect temperature zero. default random number 1 10000000 system_prompt system prompt used LLM. context_info used RAG, want provide context LLM ground answers. Currently incompatible template_prompt tools R list tools (.e. functions) can passed LLM. Note supported specific LLMs like Llama3.1 - may work LLMs trained tools calling.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_chat_completion.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get chat completion from ollama server — get_ollama_chat_completion","text":"function get chat completion ollama server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_chat_completion.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get chat completion from ollama server — get_ollama_chat_completion","text":"","code":"conn <- get_ollama_connection() get_ollama_chat_completion(ollama_connection=conn,  model+\"llama3:8b-instruct-q4_K_S\", prompts_vector=\"is the sky blue at night?\") #> <simpleError in eval(expr, envir): object 'model' not found>"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_completion.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a completion from ollama server — get_ollama_completion","title":"Get a completion from ollama server — get_ollama_completion","text":"get_ollama_completion get completion ollama server API","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_completion.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a completion from ollama server — get_ollama_completion","text":"","code":"get_ollama_completion(   ollama_connection,   model,   prompts_vector,   output_text_only = F,   num_predict = 200,   temperature = 0.8,   system_prompt = NA )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_completion.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a completion from ollama server — get_ollama_completion","text":"ollama_connection connection object information connect ollama server model Name model use ollama server. Note can use models available. prompts_vector vector containing one messages acting prompts completion. output_text_only boolean value (default False) indicating just want text message output (TRUE) whole response coming server. num_predict number tokens generate response (maximum amount) temperature temperature value answer model. temperature 0 gives always answer. temperature 1 lot variation. Default 0.8. system_prompt system prompt used LLM.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_completion.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get a completion from ollama server — get_ollama_completion","text":"simple function test connect ollama","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"Define a connection to a local ollama server — get_ollama_connection","title":"Define a connection to a local ollama server — get_ollama_connection","text":"get_ollama_connection sets variables used define connection ollama server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define a connection to a local ollama server — get_ollama_connection","text":"","code":"get_ollama_connection(ip_ad = \"127.0.0.1\", port = \"11434\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define a connection to a local ollama server — get_ollama_connection","text":"ip_ad IP address server running ollama. Default localhost 127.0.0.1. port port used run ollama service. Default 11374.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_connection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define a connection to a local ollama server — get_ollama_connection","text":"list contains information ollama connection.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_connection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Define a connection to a local ollama server — get_ollama_connection","text":"simple function set connection details ollama server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_connection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define a connection to a local ollama server — get_ollama_connection","text":"","code":"#get_ollama_connection(ip_ad=\"127.0.0.1\", port=\"11434\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_embeddings.html","id":null,"dir":"Reference","previous_headings":"","what":"Get embeddings for a piece of context through an ollama server instance — get_ollama_embeddings","title":"Get embeddings for a piece of context through an ollama server instance — get_ollama_embeddings","text":"get_ollama_embeddings Retrieves embeddings string (can series words, sentence, paragraph, whole document) ollama","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_embeddings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get embeddings for a piece of context through an ollama server instance — get_ollama_embeddings","text":"","code":"get_ollama_embeddings(ollama_connection, model = \"bge-large\", input)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_embeddings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get embeddings for a piece of context through an ollama server instance — get_ollama_embeddings","text":"ollama_connection connection object information connect ollama server model model used generates embeddings. Defaults \"-minilm\". can select model available generating embeddings. input single character vector contains text convert embeddings","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_embeddings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get embeddings for a piece of context through an ollama server instance — get_ollama_embeddings","text":"Converts ollama response model info tibble","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_embeddings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get embeddings for a piece of context through an ollama server instance — get_ollama_embeddings","text":"","code":"conn <-get_ollama_connection() embeddings <- get_ollama_embeddings(conn, input=\"Hi there, how are you doing?\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_model_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Get information about one ollama model — get_ollama_model_info","title":"Get information about one ollama model — get_ollama_model_info","text":"get_ollama_model_info Gets information one model available ollama.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_model_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get information about one ollama model — get_ollama_model_info","text":"","code":"get_ollama_model_info(ollama_connection, model)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_model_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get information about one ollama model — get_ollama_model_info","text":"ollama_connection connection object information connect Oollama server model name model want get info .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_model_info.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get information about one ollama model — get_ollama_model_info","text":"Gets information one model available ollama.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_ollama_model_info.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get information about one ollama model — get_ollama_model_info","text":"","code":"#ollama_conn <- get_ollama_connection() #get_ollama_model_info(ollama_connection= ollama_conn, model=\"llama3:8b-instruct-q5_0\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_qdrant_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Qdrant connection — get_qdrant_connection","title":"Get Qdrant connection — get_qdrant_connection","text":"get_qdrant_connection establishes connection qdrant instance. '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_qdrant_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Qdrant connection — get_qdrant_connection","text":"","code":"get_qdrant_connection(   endpoint = \"http://localhost\",   port = 6333,   api_key = NA_character_ )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_qdrant_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Qdrant connection — get_qdrant_connection","text":"endpoint URL pointing qdrant instance. Defaults http://localhost port port use connect qdrant instance. Defaults 6333 api_key optional security. Defaults NA. given, use API key connect Qdrant.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_qdrant_connection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Qdrant connection — get_qdrant_connection","text":"Creates connection object qdrant. using API key connect Qdrant, need specify api_key parameter.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/get_qdrant_connection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Qdrant connection — get_qdrant_connection","text":"","code":"conn <- get_qdrant_connection() #> Error in httr2::req_perform(req): Failed to perform HTTP request. #> Caused by error in `curl::curl_fetch_memory()`: #> ! Failed to connect to localhost port 6333 after 0 ms: Could not connect to server"},{"path":"https://r-guyot.github.io/aiworkflow/reference/inspect_processing_skill.html","id":null,"dir":"Reference","previous_headings":"","what":"Inspect a specific processing skill — inspect_processing_skill","title":"Inspect a specific processing skill — inspect_processing_skill","text":"inspect_processing_skill lets inspect specific processing skill written.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/inspect_processing_skill.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inspect a specific processing skill — inspect_processing_skill","text":"","code":"inspect_processing_skill(processing_skill)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/inspect_processing_skill.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inspect a specific processing skill — inspect_processing_skill","text":"processing_skill name processing skill want model use specific task","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/inspect_processing_skill.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Inspect a specific processing skill — inspect_processing_skill","text":"text vector containing exact prompt used processing skill.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/inspect_processing_skill.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Inspect a specific processing skill — inspect_processing_skill","text":"outputs text contained processing skill. processing skill name looking found, throw error.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/inspect_processing_skill.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Inspect a specific processing skill — inspect_processing_skill","text":"","code":"inspect_processing_skill(\"break_down_task\") #> → Could not find the processing skill break_down_task you asked for."},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_global_functions.html","id":null,"dir":"Reference","previous_headings":"","what":"List global functions — list_global_functions","title":"List global functions — list_global_functions","text":"list_global_functions lists global functions available environment.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_global_functions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List global functions — list_global_functions","text":"","code":"list_global_functions()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_global_functions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List global functions — list_global_functions","text":"lists global functions available. useful check functions used tools calling present .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_global_functions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List global functions — list_global_functions","text":"","code":"list_global_functions() #> [1] \"do_math\""},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_processing_skills.html","id":null,"dir":"Reference","previous_headings":"","what":"list the processing skills — list_processing_skills","title":"list the processing skills — list_processing_skills","text":"list_processing_skills lists processing skills (.e. prompts already defined) can use box package.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_processing_skills.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"list the processing skills — list_processing_skills","text":"","code":"list_processing_skills()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_processing_skills.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"list the processing skills — list_processing_skills","text":"list vectors describing skills available currently package.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_processing_skills.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"list the processing skills — list_processing_skills","text":"gives vector skills available use default. can used along set_processing_skill() function.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/list_processing_skills.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"list the processing skills — list_processing_skills","text":"","code":"list_processing_skills() #>  [1] \"add_code_comments\"              \"add_details\"                    #>  [3] \"add_emoji\"                      \"break_down_tasks\"               #>  [5] \"categorize_product_complaint\"   \"categorize_sentiment\"           #>  [7] \"chain_of_thought\"               \"clean_up_audio_transcription\"   #>  [9] \"extract_actions\"                \"extract_names\"                  #> [11] \"fix_copy\"                       \"fix_translation\"                #> [13] \"generate_code\"                  \"generate_creative_introduction\" #> [15] \"generate_quiz\"                  \"identify_product\"               #> [17] \"rewrite_active_voice\"           \"rewrite_blog_copy\"              #> [19] \"rewrite_bullet_points\"          \"rewrite_casual\"                 #> [21] \"rewrite_emoji\"                  \"rewrite_in_first_person\"        #> [23] \"rewrite_jargon\"                 \"rewrite_paraphrase\"             #> [25] \"rewrite_passive_voice\"          \"rewrite_positive\"               #> [27] \"rewrite_prompt\"                 \"rewrite_text_as_anonymized\"     #> [29] \"tldr\"                           \"translate\"                      #> [31] \"write_abstract\"                 \"write_article_from_title\"       #> [33] \"write_email\"                    \"write_fiction_chapter\"          #> [35] \"write_pitch\"                    \"write_qa\"                       #> [37] \"write_quarto_presentation\"      \"write_recommendation\"           #> [39] \"write_resume_narrative\"         \"write_short_summary\"            #> [41] \"write_text_expansion\"           \"write_title\"                    #> [43] \"write_tweet\""},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_context_embeddings_from_feather_files.html","id":null,"dir":"Reference","previous_headings":"","what":"Load Context Embeddings From Feather Files — load_context_embeddings_from_feather_files","title":"Load Context Embeddings From Feather Files — load_context_embeddings_from_feather_files","text":"load_context_embeddings_from_feather_files lets load vector data feather file","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_context_embeddings_from_feather_files.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load Context Embeddings From Feather Files — load_context_embeddings_from_feather_files","text":"","code":"load_context_embeddings_from_feather_files(filenames)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_context_embeddings_from_feather_files.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load Context Embeddings From Feather Files — load_context_embeddings_from_feather_files","text":"filenames list feather filenames contain vector embeddings text information","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_context_embeddings_from_feather_files.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load Context Embeddings From Feather Files — load_context_embeddings_from_feather_files","text":"vector data contained feather file.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_context_embeddings_from_feather_files.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Load Context Embeddings From Feather Files — load_context_embeddings_from_feather_files","text":"function provides simple way load vector data (embeddings) binary feather file, instead using regular databases.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_context_embeddings_from_feather_files.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load Context Embeddings From Feather Files — load_context_embeddings_from_feather_files","text":"","code":"conn <- get_ollama_connection()  document <- \"Standing proudly on the Île de la Cité in the heart of Paris,  France's capital city, lies one of the world's most beloved and historic  landmarks: the magnificent Notre Dame Cathedral. This Gothic masterpiece  has been welcoming pilgrims and tourists alike for over 850 years, since its  construction began in 1163 under King Louis VII. With its towering spires,  stunning stained glass windows, and intricate stone carvings, this beautiful  church is a testament to medieval architecture and engineering skill.  Unfortunately, a devastating fire ravaged the cathedral on April 15, 2019,  but thanks to swift action from firefighters and restoration efforts  underway, Notre Dame continues to inspire awe in those who visit her.\"  writeLines(document, con = \"doc1.txt\")  write_vectors_to_feather_file(file_name = \"doc1.feather\", vector_data = generate_document_embeddings(conn,  document_path = \"doc1.txt\", splitter = \"paragraph\"))  load_context_embeddings_from_feather_files(filenames = \"doc1.feather\") #>      embeddings #>          <AsIs> #> 1: -0.02001.... #>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         text #>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       <char> #> 1: Standing proudly on the Île de la Cité in the heart of Paris,  France's capital city, lies one of the world's most beloved and historic  landmarks: the magnificent Notre Dame Cathedral. This Gothic masterpiece  has been welcoming pilgrims and tourists alike for over 850 years, since its  construction began in 1163 under King Louis VII. With its towering spires,  stunning stained glass windows, and intricate stone carvings, this beautiful  church is a testament to medieval architecture and engineering skill.  Unfortunately, a devastating fire ravaged the cathedral on April 15, 2019,  but thanks to swift action from firefighters and restoration efforts  underway, Notre Dame continues to inspire awe in those who visit her."},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_workflow.html","id":null,"dir":"Reference","previous_headings":"","what":"Load workflow — load_workflow","title":"Load workflow — load_workflow","text":"load_workflow makes possible reload workflow saved configuration file JSON.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_workflow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load workflow — load_workflow","text":"","code":"load_workflow(filepath)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_workflow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load workflow — load_workflow","text":"filepath filepath JSON file contains parameters workflow, created save_workflow().","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_workflow.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load workflow — load_workflow","text":"workflow loading settings parameters.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_workflow.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Load workflow — load_workflow","text":"function reload workflow specific JSON file created save_workflow_settings().","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/load_workflow.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load workflow — load_workflow","text":"","code":"my_workflow <- load_workflow(filepath=\"myworkflow.json\") #> → The workflow has been successfully reloaded from myworkflow.json."},{"path":"https://r-guyot.github.io/aiworkflow/reference/make_cosine_similarity_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Make Cosine Similarity Matrix — make_cosine_similarity_matrix","title":"Make Cosine Similarity Matrix — make_cosine_similarity_matrix","text":"make_cosine_similarity_matrix lets calculate cosine similarity given matrix.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/make_cosine_similarity_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make Cosine Similarity Matrix — make_cosine_similarity_matrix","text":"","code":"make_cosine_similarity_matrix(input_matrix)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/make_cosine_similarity_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make Cosine Similarity Matrix — make_cosine_similarity_matrix","text":"input_matrix input matrix used derive cosine similarity matrix end","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/make_cosine_similarity_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make Cosine Similarity Matrix — make_cosine_similarity_matrix","text":"cosine similarity matrix","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/make_cosine_similarity_matrix.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Make Cosine Similarity Matrix — make_cosine_similarity_matrix","text":"Calculate cosine similarity matrix.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/make_cosine_similarity_matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make Cosine Similarity Matrix — make_cosine_similarity_matrix","text":"","code":"conn <- get_ollama_connection()  document <- \"Standing proudly on the Île de la Cité in the heart of Paris,  France's capital city, lies one of the world's most beloved and historic  landmarks: the magnificent Notre Dame Cathedral. This Gothic masterpiece  has been welcoming pilgrims and tourists alike for over 850 years, since its  construction began in 1163 under King Louis VII. With its towering spires,  stunning stained glass windows, and intricate stone carvings, this beautiful  church is a testament to medieval architecture and engineering skill.  Unfortunately, a devastating fire ravaged the cathedral on April 15, 2019,  but thanks to swift action from firefighters and restoration efforts  underway, Notre Dame continues to inspire awe in those who visit her.\"  writeLines(document, con = \"doc1.txt\")  context_df <- convert_batch_documents_to_embeddings(ollama_connection = conn,                                        document_path_list = list(\"doc1.txt\"))  prompt <- \"When was Notre Dame in Paris built?\" prompt_vector <- get_ollama_embeddings(ollama_connection = conn, input =  prompt)  whole_list <- c(list(prompt_vector), context_df$embeddings) mat <- do.call(rbind,whole_list) cos_sim_mat <- make_cosine_similarity_matrix(mat)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/parse_json_result.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse JSON answer from the LLM — parse_json_result","title":"Parse JSON answer from the LLM — parse_json_result","text":"parse_json_result attempts parse JSON result LLM","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/parse_json_result.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse JSON answer from the LLM — parse_json_result","text":"","code":"parse_json_result(json_string)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/parse_json_result.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse JSON answer from the LLM — parse_json_result","text":"json_string JSON string want parse","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/parse_json_result.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Parse JSON answer from the LLM — parse_json_result","text":"function assume result LLM provided JSON format format correct, parse result R object. typically expect format used request_json_answer() function. typically used pipe, pull_final_answer().","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/parse_json_result.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parse JSON answer from the LLM — parse_json_result","text":"","code":"myflow_template <- ai_workflow() |>  set_connector(\"ollama\")  |>    set_model(model_name= \"llama3.1:8b-instruct-q5_K_M\") |>    set_n_predict(1000) |>    set_temperature(0.8) |>    request_json_answer() #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434.    myflow_template |>  process_prompts(\"what is the usual color of the sky on Earth?\") |> pull_final_answer() |> parse_json_result() #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → Chat mode #> $answer #> [1] \"blue\" #>"},{"path":"https://r-guyot.github.io/aiworkflow/reference/process_prompts.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Prompts starting from a workflow — process_prompts","title":"Process Prompts starting from a workflow — process_prompts","text":"process_prompts way process vector prompts starting workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/process_prompts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Prompts starting from a workflow — process_prompts","text":"","code":"process_prompts(workflow_obj, prompts_vector)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/process_prompts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Prompts starting from a workflow — process_prompts","text":"workflow_obj workflow object containing parameters describing flow required prompts_vector vector containing prompts executed AI workflow","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/process_prompts.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Process Prompts starting from a workflow — process_prompts","text":"function provides way process vector prompts starting workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/pull_final_answer.html","id":null,"dir":"Reference","previous_headings":"","what":"Pull Final Answer — pull_final_answer","title":"Pull Final Answer — pull_final_answer","text":"pull_final_answer function extract final answer series workflows","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/pull_final_answer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pull Final Answer — pull_final_answer","text":"","code":"pull_final_answer(workflow)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/pull_final_answer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pull Final Answer — pull_final_answer","text":"workflow workflow object containing parameters describing flow required","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/pull_final_answer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Pull Final Answer — pull_final_answer","text":"function extract final text result usually coming series workflows.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_collection_existence.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: Check collection existence — qdrant_check_collection_existence","title":"Qdrant: Check collection existence — qdrant_check_collection_existence","text":"qdrant_check_collection_existence establishes connection qdrant instance checks collection exists. '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_collection_existence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: Check collection existence — qdrant_check_collection_existence","text":"","code":"qdrant_check_collection_existence(conn, collection_name = NA_character_)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_collection_existence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: Check collection existence — qdrant_check_collection_existence","text":"conn connection object created get_qdrant_connection() collection_name name collection want check existence. Defaults NA.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_collection_existence.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: Check collection existence — qdrant_check_collection_existence","text":"Confirms existence qdrant collection. Returns TRUE FALSE","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_collection_existence.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: Check collection existence — qdrant_check_collection_existence","text":"","code":"conn <- get_qdrant_connection() #> Error in httr2::req_perform(req): Failed to perform HTTP request. #> Caused by error in `curl::curl_fetch_memory()`: #> ! Failed to connect to localhost port 6333 after 0 ms: Could not connect to server qdrant_check_collection_existence(conn, collection_name=\"hi_there\") #> Error: object 'conn' not found"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_connection_validity.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: Check if the Connection is valid — qdrant_check_connection_validity","title":"Qdrant: Check if the Connection is valid — qdrant_check_connection_validity","text":"qdrant_check_connection_validity checks qdrant connection proposed valid. '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_connection_validity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: Check if the Connection is valid — qdrant_check_connection_validity","text":"","code":"qdrant_check_connection_validity(conn, silent = TRUE)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_connection_validity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: Check if the Connection is valid — qdrant_check_connection_validity","text":"conn connection object created get_qdrant_connection() silent boolean, defaults TRUE. FALSE, prints message connection established.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_connection_validity.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: Check if the Connection is valid — qdrant_check_connection_validity","text":"simple test confirm qdrant connection working expected.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_check_connection_validity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: Check if the Connection is valid — qdrant_check_connection_validity","text":"","code":"conn <- get_qdrant_connection() #> Error in httr2::req_perform(req): Failed to perform HTTP request. #> Caused by error in `curl::curl_fetch_memory()`: #> ! Failed to connect to localhost port 6333 after 0 ms: Could not connect to server qdrant_check_connection_validity(conn,silent=FALSE) #> Error: object 'conn' not found"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_create_new_collection.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: Create new collection — qdrant_create_new_collection","title":"Qdrant: Create new collection — qdrant_create_new_collection","text":"qdrant_create_new_collection establishes connection qdrant instance creates new collection. '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_create_new_collection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: Create new collection — qdrant_create_new_collection","text":"","code":"qdrant_create_new_collection(   conn,   collection_name = NA_character_,   vectors = list(size = 384, distance = \"Cosine\") )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_create_new_collection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: Create new collection — qdrant_create_new_collection","text":"conn connection object created get_qdrant_connection() collection_name name collection want create. Defaults NA. vectors R list object needs contain 'size' 'distance'. Defaults size=384 distance='Cosine' needs adapted embedding model use. Different models different resulting vector sizes.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_create_new_collection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: Create new collection — qdrant_create_new_collection","text":"Creates new collection vector embeddings. can customize size embeddings needed type distance similarity search. collection already exists name throw error.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_create_new_collection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: Create new collection — qdrant_create_new_collection","text":"","code":"conn <- get_qdrant_connection() #> Error in httr2::req_perform(req): Failed to perform HTTP request. #> Caused by error in `curl::curl_fetch_memory()`: #> ! Failed to connect to localhost port 6333 after 0 ms: Could not connect to server qdrant_create_new_collection(conn, collection_name=\"story_of_my_life\") #> Error: object 'conn' not found qdrant_delete_collection(conn, collection_name=\"story_of_my_life\") #> Error: object 'conn' not found"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_collection.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: Delete collection — qdrant_delete_collection","title":"Qdrant: Delete collection — qdrant_delete_collection","text":"qdrant_delete_collection establishes connection qdrant instance deletes existing collection. '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_collection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: Delete collection — qdrant_delete_collection","text":"","code":"qdrant_delete_collection(conn, collection_name = NA_character_)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_collection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: Delete collection — qdrant_delete_collection","text":"conn connection object created get_qdrant_connection() collection_name name collection want delete. Defaults NA.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_collection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: Delete collection — qdrant_delete_collection","text":"Deletes existing collection qdrant. collection exists, throw error. deletion successful display confirmation message","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_collection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: Delete collection — qdrant_delete_collection","text":"","code":"conn <- get_qdrant_connection() #> Error in httr2::req_perform(req): Failed to perform HTTP request. #> Caused by error in `curl::curl_fetch_memory()`: #> ! Failed to connect to localhost port 6333 after 0 ms: Could not connect to server qdrant_create_new_collection(conn, collection_name=\"story_of_alice\") #> Error: object 'conn' not found qdrant_delete_collection(conn, collection_name=\"story_of_alice\") #> Error: object 'conn' not found"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_points.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: Delete points (vectors) — qdrant_delete_points","title":"Qdrant: Delete points (vectors) — qdrant_delete_points","text":"qdrant_delete_points deletes specific vectors (points) '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_points.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: Delete points (vectors) — qdrant_delete_points","text":"","code":"qdrant_delete_points(conn, collection_name = NA_character_, ids = list())"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_points.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: Delete points (vectors) — qdrant_delete_points","text":"conn connection object created get_qdrant_connection() collection_name name collection want use. Defaults NA. ids list ids delete. Defaults empty list. value \"ids\" needs format: list(1,2,3) want delete vectors 1,2 3.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_points.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: Delete points (vectors) — qdrant_delete_points","text":"delete specific vectors based ids.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_delete_points.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: Delete points (vectors) — qdrant_delete_points","text":"","code":"conn <- get_qdrant_connection() #> Error in httr2::req_perform(req): Failed to perform HTTP request. #> Caused by error in `curl::curl_fetch_memory()`: #> ! Failed to connect to localhost port 6333 after 0 ms: Could not connect to server qdrant_create_new_collection(conn, collection_name=\"test_db\", vectors=list(size=3,distance=\"Cosine\")) #> Error: object 'conn' not found new_vectors <- list(points=list( list(id=1, payload=list(text=\"hi there\"),vector=list(0.1,0.5,0.6)), list(id=2, payload=list(text=\"well well well\"),vector=list(0.6,0.1,0.3)) )) qdrant_upsert_points(conn, points=new_vectors,collection_name=\"test_db\") #> Error: object 'conn' not found qdrant_delete_points(conn, collection_name=\"test_db\", ids=list(1,2)) #> Error: object 'conn' not found qdrant_delete_collection(conn, \"test_db\") #> Error: object 'conn' not found"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_get_collection_details.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: Get collection details — qdrant_get_collection_details","title":"Qdrant: Get collection details — qdrant_get_collection_details","text":"qdrant_get_collection_details establishes connection qdrant instance returns details specific collection. '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_get_collection_details.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: Get collection details — qdrant_get_collection_details","text":"","code":"qdrant_get_collection_details(conn, collection_name = NA_character_)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_get_collection_details.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: Get collection details — qdrant_get_collection_details","text":"conn connection object created get_qdrant_connection() collection_name name collection want check details. Defaults NA.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_get_collection_details.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: Get collection details — qdrant_get_collection_details","text":"Returns details specific qdrant collection.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_get_collection_details.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: Get collection details — qdrant_get_collection_details","text":"","code":"conn <- get_qdrant_connection() #> Error in httr2::req_perform(req): Failed to perform HTTP request. #> Caused by error in `curl::curl_fetch_memory()`: #> ! Failed to connect to localhost port 6333 after 0 ms: Could not connect to server qdrant_create_new_collection(conn, collection_name=\"story_of_my_life\") #> Error: object 'conn' not found qdrant_get_collection_details(conn, collection_name=\"story_of_my_life\") #> Error: object 'conn' not found qdrant_delete_collection(conn, collection_name=\"story_of_my_life\") #> Error: object 'conn' not found"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_list_all_collections.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: List all collections — qdrant_list_all_collections","title":"Qdrant: List all collections — qdrant_list_all_collections","text":"qdrant_list_all_collections establishes connection qdrant instance lists collections. '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_list_all_collections.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: List all collections — qdrant_list_all_collections","text":"","code":"qdrant_list_all_collections(conn)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_list_all_collections.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: List all collections — qdrant_list_all_collections","text":"conn connection object created get_qdrant_connection()","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_list_all_collections.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: List all collections — qdrant_list_all_collections","text":"Lists collections available qdrant instance. collection can different size vector embeddings","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_list_all_collections.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: List all collections — qdrant_list_all_collections","text":"","code":"conn <- get_qdrant_connection() #> Error in httr2::req_perform(req): Failed to perform HTTP request. #> Caused by error in `curl::curl_fetch_memory()`: #> ! Failed to connect to localhost port 6333 after 0 ms: Could not connect to server qdrant_list_all_collections(conn) #> Error: object 'conn' not found"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_retrieve_point.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: Retrieve a specific point (vector) — qdrant_retrieve_point","title":"Qdrant: Retrieve a specific point (vector) — qdrant_retrieve_point","text":"qdrant_retrieve_point retrieves specific vector (point) '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_retrieve_point.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: Retrieve a specific point (vector) — qdrant_retrieve_point","text":"","code":"qdrant_retrieve_point(   conn,   collection_name = NA_character_,   id = NA_character_ )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_retrieve_point.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: Retrieve a specific point (vector) — qdrant_retrieve_point","text":"conn connection object created get_qdrant_connection() collection_name name collection want use. Defaults NA. id id point retrieve. Defaults NA.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_retrieve_point.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: Retrieve a specific point (vector) — qdrant_retrieve_point","text":"Retrieves value specific vector/point, based id. return list info available qdrant.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_retrieve_point.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: Retrieve a specific point (vector) — qdrant_retrieve_point","text":"","code":"conn <- get_qdrant_connection() #> Error in httr2::req_perform(req): Failed to perform HTTP request. #> Caused by error in `curl::curl_fetch_memory()`: #> ! Failed to connect to localhost port 6333 after 0 ms: Could not connect to server qdrant_create_new_collection(conn, collection_name=\"test_db\", vectors=list(size=3,distance=\"Cosine\")) #> Error: object 'conn' not found new_vectors <- list(points=list(list(id=1, payload=list(text=\"hi there\"),                       vector=list(0.1,0.5,0.6)))) qdrant_upsert_points(conn, points=new_vectors,collection_name=\"test_db\",generate_id=FALSE) #> Error: object 'conn' not found qdrant_retrieve_point(conn, collection_name=\"test_db\", id=1) #> Error: object 'conn' not found qdrant_delete_collection(conn, \"test_db\") #> Error: object 'conn' not found"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_search_points.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: Search points (vectors) — qdrant_search_points","title":"Qdrant: Search points (vectors) — qdrant_search_points","text":"qdrant_search_points searches similar vectors (points) '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_search_points.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: Search points (vectors) — qdrant_search_points","text":"","code":"qdrant_search_points(   conn,   collection_name = NA_character_,   vector = list(),   limit = 5,   with_payload = TRUE,   with_vector = FALSE,   score_threshold = NA_real_ )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_search_points.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: Search points (vectors) — qdrant_search_points","text":"conn connection object created get_qdrant_connection() collection_name name collection want use. Defaults NA. vector vector embedding values used initiate search. Defaults empty list. Note vector simply list containing numerical values embeddings. use embedding size 384, simple list 384 numerical values . limit maximum amount vectors return search. Defaults 5. with_payload TRUE, display payload resulting vectors. Defaults TRUE. with_vector TRUE, display full vector values results. Defaults FALSE. score_threshold threshold minimum similarity score return results. Defaults NA.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_search_points.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: Search points (vectors) — qdrant_search_points","text":"run search similar vectors qdrant.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_search_points.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: Search points (vectors) — qdrant_search_points","text":"","code":"conn <- get_qdrant_connection() #> Error in httr2::req_perform(req): Failed to perform HTTP request. #> Caused by error in `curl::curl_fetch_memory()`: #> ! Failed to connect to localhost port 6333 after 0 ms: Could not connect to server qdrant_create_new_collection(conn, collection_name=\"test_db\", vectors=list(size=3,distance=\"Cosine\")) #> Error: object 'conn' not found new_vectors <- list(points=list( list(id=1, payload=list(text=\"hi there\"),vector=list(0.1,0.5,0.6)), list(id=2, payload=list(text=\"well well well\"),vector=list(0.6,0.1,0.3)) )) qdrant_upsert_points(conn, points=new_vectors, collection_name=\"test_db\") #> Error: object 'conn' not found qdrant_search_points(conn, collection_name=\"test_db\", vector=list(0.2,0.5,0.6)) #> Error: object 'conn' not found qdrant_delete_collection(conn, \"test_db\") #> Error: object 'conn' not found"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_upsert_points.html","id":null,"dir":"Reference","previous_headings":"","what":"Qdrant: Upsert points — qdrant_upsert_points","title":"Qdrant: Upsert points — qdrant_upsert_points","text":"qdrant_upsert_points establishes connection qdrant instance insert points (vector) collection. '","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_upsert_points.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qdrant: Upsert points — qdrant_upsert_points","text":"","code":"qdrant_upsert_points(   conn,   points = list(),   collection_name,   generate_id = TRUE )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_upsert_points.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qdrant: Upsert points — qdrant_upsert_points","text":"conn connection object created get_qdrant_connection() points list containing vector embeddings. specific format expected. format expected (example vector size 3): list(points=list(list(id=1, payload=list(text=\"hi \"), vector=list(0.1,0.5,0.6)))) Note 'payload' can contain lot data 'text'. can add variables needed within payload list. collection_name name collection want use. Defaults NA. generate_id boolean, defaults TRUE. TRUE, automatically create unique UUID vector, needed qdrant. use , provide id .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_upsert_points.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Qdrant: Upsert points — qdrant_upsert_points","text":"Inserts vector entries existing collection.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/qdrant_upsert_points.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qdrant: Upsert points — qdrant_upsert_points","text":"","code":"conn <- get_qdrant_connection() #> Error in httr2::req_perform(req): Failed to perform HTTP request. #> Caused by error in `curl::curl_fetch_memory()`: #> ! Failed to connect to localhost port 6333 after 0 ms: Could not connect to server qdrant_create_new_collection(conn, collection_name=\"test_db\", vectors=list(size=3,distance=\"Cosine\")) #> Error: object 'conn' not found new_vectors <- list(points=list(list(payload=list(text=\"hi there\"),                       vector=list(0.1,0.5,0.6)))) qdrant_upsert_points(conn, points=new_vectors, collection_name=\"test_db\",generate_id=TRUE) #> Error: object 'conn' not found qdrant_delete_collection(conn, \"test_db\") #> Error: object 'conn' not found"},{"path":"https://r-guyot.github.io/aiworkflow/reference/request_json_answer.html","id":null,"dir":"Reference","previous_headings":"","what":"Request JSON answer from the LLM — request_json_answer","title":"Request JSON answer from the LLM — request_json_answer","text":"request_json_answer request current workflow answer using JSON format.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/request_json_answer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Request JSON answer from the LLM — request_json_answer","text":"","code":"request_json_answer(workflow_obj, json_object_format = list())"},{"path":"https://r-guyot.github.io/aiworkflow/reference/request_json_answer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Request JSON answer from the LLM — request_json_answer","text":"workflow_obj current workflow object want build json_object_format format required answer.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/request_json_answer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Request JSON answer from the LLM — request_json_answer","text":"function request LLM answer using JSON format. default simply focus simple JSON format just answer single object. can add different JSON format argument.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/request_json_answer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Request JSON answer from the LLM — request_json_answer","text":"","code":"myflow_template <- ai_workflow() |>  set_connector(\"ollama\")  |>    set_model(model_name= \"llama3.1:8b-instruct-q5_K_M\") |>    set_n_predict(1000) |>    set_temperature(0.8) |>    request_json_answer() #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434."},{"path":"https://r-guyot.github.io/aiworkflow/reference/retrieve_similar_vectors.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve Similar Vectors — retrieve_similar_vectors","title":"Retrieve Similar Vectors — retrieve_similar_vectors","text":"retrieve_similar_vectors lets retrieve similar content, based cosine similarity, given context.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/retrieve_similar_vectors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve Similar Vectors — retrieve_similar_vectors","text":"","code":"retrieve_similar_vectors(   context_df,   prompt_vector,   max_results = 10,   similarity_threshold = 0.5 )"},{"path":"https://r-guyot.github.io/aiworkflow/reference/retrieve_similar_vectors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve Similar Vectors — retrieve_similar_vectors","text":"context_df context dataframe contains text embeddings text information retrieval prompt_vector prompt transformed vector embeddings order kick search max_results maximum number results retrieved similarity_threshold threshold 0 1 (defaults 0.5) remove least relevant results. 1 means perfect similarity, 0 similarity .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/retrieve_similar_vectors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve Similar Vectors — retrieve_similar_vectors","text":"text vectors similar enough given prompt.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/retrieve_similar_vectors.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve Similar Vectors — retrieve_similar_vectors","text":"function provides way make simple RAG process retrieve content based context dataframe prompt vector.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/retrieve_similar_vectors.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve Similar Vectors — retrieve_similar_vectors","text":"","code":"conn <- get_ollama_connection()  document <- \"Standing proudly on the Île de la Cité in the heart of Paris,  France's capital city, lies one of the world's most beloved and historic  landmarks: the magnificent Notre Dame Cathedral. This Gothic masterpiece  has been welcoming pilgrims and tourists alike for over 850 years, since its  construction began in 1163 under King Louis VII. With its towering spires,  stunning stained glass windows, and intricate stone carvings, this beautiful  church is a testament to medieval architecture and engineering skill.  Unfortunately, a devastating fire ravaged the cathedral on April 15, 2019,  but thanks to swift action from firefighters and restoration efforts  underway, Notre Dame continues to inspire awe in those who visit her.\"  writeLines(document, con = \"doc1.txt\")  context_df <- convert_batch_documents_to_embeddings(ollama_connection = conn,                                        document_path_list = list(\"doc1.txt\"))  prompt <- \"When was Notre Dame in Paris built?\" prompt_vector <- get_ollama_embeddings(ollama_connection = conn, input =  prompt) similar_text <- retrieve_similar_vectors(context_df, prompt_vector)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/save_workflow.html","id":null,"dir":"Reference","previous_headings":"","what":"Save workflow — save_workflow","title":"Save workflow — save_workflow","text":"save_workflow makes possible save workflow settings later re-use.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/save_workflow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save workflow — save_workflow","text":"","code":"save_workflow(workflow_obj, filepath)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/save_workflow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save workflow — save_workflow","text":"workflow_obj workflow object containing parameters describing workflow required filepath filepath JSON file capture workflow configuration.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/save_workflow.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save workflow — save_workflow","text":"workflow saving , can included inside series pipes.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/save_workflow.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Save workflow — save_workflow","text":"function save workflow settings specific JSON file. reload settings use load_workflow() function.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/save_workflow.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Save workflow — save_workflow","text":"","code":"my_workflow <- ai_workflow() |> set_connector(\"ollama\")  |>  set_model(model_name= \"llama3:8b-instruct-q5_K_S\") |>  set_n_predict(500) |>  set_temperature(0.8) |>   set_default_missing_parameters_in_workflow() |>   set_system_prompt(\"You are a computer hardware and software specialist.\") |>  save_workflow(filepath=\"myworkflow.json\") #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. #> → Frequency Penalty was not specified and given a default value of 1. #> → Presence Penalty was not specified and given a default value of 1.5. #> → Repeat Penalty was not specified and given a default value of 1.2. #> → Mode was not specified and 'chat' was selected by default. #> → System Prompt was not specified and given a default value of 'You are a helpful AI assistant.'. #> → The workflow has been saved to myworkflow.json."},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_audience.html","id":null,"dir":"Reference","previous_headings":"","what":"Define a specific audience you want the model to prepare an answer for — set_audience","title":"Define a specific audience you want the model to prepare an answer for — set_audience","text":"set_audience lets define audience model answer. tweak answers accordingly match expectations.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_audience.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define a specific audience you want the model to prepare an answer for — set_audience","text":"","code":"set_audience(workflow_obj, audience)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_audience.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define a specific audience you want the model to prepare an answer for — set_audience","text":"workflow_obj ai_workflow object created ai_workflow() first place. audience text description audience want LLM write .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_audience.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define a specific audience you want the model to prepare an answer for — set_audience","text":"workflow object new added audience parameter","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_audience.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Define a specific audience you want the model to prepare an answer for — set_audience","text":"lets modify typical workflow adding information expected audience.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_audience.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define a specific audience you want the model to prepare an answer for — set_audience","text":"","code":"my_workflow <- ai_workflow() |>  set_system_prompt(system_prompt=\"You are a helpful AI assistant.  Answer to the best of your knowledge\") |> set_audience(audience=\"Marketing Professionals\")  my_workflow <- ai_workflow() |>  set_system_prompt(system_prompt=\"You are a helpful AI assistant.  Answer to the best of your knowledge\") |> set_audience(audience=\"5 years old kids\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_connector.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the connector required to operate the workflow. — set_connector","title":"Set the connector required to operate the workflow. — set_connector","text":"set_connector sets connector type expected server calls. can example Ollama server Llama.cpp server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_connector.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the connector required to operate the workflow. — set_connector","text":"","code":"set_connector(workflow_obj, connector)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_connector.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the connector required to operate the workflow. — set_connector","text":"workflow_obj ai_workflow object created ai_workflow() first place. connector name connector type use. can either ollama llamacpp (taken care later versions).","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_connector.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the connector required to operate the workflow. — set_connector","text":"Set connector required operate workflow. connector server reached API. can either local remote server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_connector.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the connector required to operate the workflow. — set_connector","text":"","code":"my_workflow <- ai_workflow() |>  set_connector(connector=\"ollama\") #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434."},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_current_time_and_date_reference.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the current time and date as addition reference — set_current_time_and_date_reference","title":"Set the current time and date as addition reference — set_current_time_and_date_reference","text":"set_current_time_and_date_reference lets set current time date addition reference part background LLM's answer.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_current_time_and_date_reference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the current time and date as addition reference — set_current_time_and_date_reference","text":"","code":"set_current_time_and_date_reference(workflow_obj)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_current_time_and_date_reference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the current time and date as addition reference — set_current_time_and_date_reference","text":"workflow_obj workflow object containing parameters describing workflow required","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_current_time_and_date_reference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the current time and date as addition reference — set_current_time_and_date_reference","text":"workflow object new added current_timedate parameter","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_current_time_and_date_reference.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the current time and date as addition reference — set_current_time_and_date_reference","text":"Setting additional time date reference background info used LLM. can useful expect LLM answer questions related future, present past.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_current_time_and_date_reference.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the current time and date as addition reference — set_current_time_and_date_reference","text":"","code":"my_workflow <- ai_workflow() |>  set_system_prompt(system_prompt=\"You are a helpful AI assistant.  Answer to the best of your knowledge\") |> set_current_time_and_date_reference()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_default_missing_parameters_in_workflow.html","id":null,"dir":"Reference","previous_headings":"","what":"Set Defaults for missing workflow parameters — set_default_missing_parameters_in_workflow","title":"Set Defaults for missing workflow parameters — set_default_missing_parameters_in_workflow","text":"set_default_missing_parameters_in_workflow ensure parameters present filling gaps","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_default_missing_parameters_in_workflow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set Defaults for missing workflow parameters — set_default_missing_parameters_in_workflow","text":"","code":"set_default_missing_parameters_in_workflow(workflow_obj, silent = F)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_default_missing_parameters_in_workflow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set Defaults for missing workflow parameters — set_default_missing_parameters_in_workflow","text":"workflow_obj workflow object containing parameters describing flow required silent ensure action happens without warning sent standard output","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_default_missing_parameters_in_workflow.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set Defaults for missing workflow parameters — set_default_missing_parameters_in_workflow","text":"workflow object along added parameters missing.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_default_missing_parameters_in_workflow.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set Defaults for missing workflow parameters — set_default_missing_parameters_in_workflow","text":"function add default values missing parameters user-defined workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_default_missing_parameters_in_workflow.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set Defaults for missing workflow parameters — set_default_missing_parameters_in_workflow","text":"","code":"# my_workflow <- ai_workflow() |> set_default_missing_parameters_in_workflow()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_frequency_penalty.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the frequency penalty of the model used by the flow. — set_frequency_penalty","title":"Set the frequency penalty of the model used by the flow. — set_frequency_penalty","text":"set_frequency_penalty sets frequency penalty related model workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_frequency_penalty.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the frequency penalty of the model used by the flow. — set_frequency_penalty","text":"","code":"set_frequency_penalty(workflow_obj, frequency_penalty)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_frequency_penalty.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the frequency penalty of the model used by the flow. — set_frequency_penalty","text":"workflow_obj ai_workflow object created ai_workflow() first place. frequency_penalty frequency penalty value used model. higher value (closer 1) makes AI avoid repeating words phrases, lower value (closer 0) allows repetitions.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_frequency_penalty.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the frequency penalty of the model used by the flow. — set_frequency_penalty","text":"workflow object frequency penalty specified new parameter applied.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_frequency_penalty.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the frequency penalty of the model used by the flow. — set_frequency_penalty","text":"Set frequency_penalty used model workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_frequency_penalty.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the frequency penalty of the model used by the flow. — set_frequency_penalty","text":"","code":"my_workflow <- ai_workflow() |>  set_model(model_name=\"llama3:8b-instruct-q5_0\") |>  set_frequency_penalty(0.6)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_ip_addr.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the IP Address required to connect to an API server. — set_ip_addr","title":"Set the IP Address required to connect to an API server. — set_ip_addr","text":"set_ip_addr sets IP Address related API server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_ip_addr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the IP Address required to connect to an API server. — set_ip_addr","text":"","code":"set_ip_addr(workflow_obj, ip_addr)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_ip_addr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the IP Address required to connect to an API server. — set_ip_addr","text":"workflow_obj ai_workflow object created ai_workflow() first place. ip_addr IP address server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_ip_addr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the IP Address required to connect to an API server. — set_ip_addr","text":"Set IP Address related API server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_ip_addr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the IP Address required to connect to an API server. — set_ip_addr","text":"","code":"wflow <- ai_workflow() |> set_connector(\"ollama\") |>  set_ip_addr(ip_addr=\"127.0.0.1\") #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. #> → IP address has been changed to 127.0.0.1. wflow <- ai_workflow() |> set_connector(\"ollama\") |>  set_ip_addr(ip_addr=\"192.168.1.56\") #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. #> → IP address has been changed to 192.168.1.56."},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_mode.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the mode of the model used by the workflow. — set_mode","title":"Set the mode of the model used by the workflow. — set_mode","text":"set_mode sets mode model workflow. usually means either 'chat', 'completion' 'embeddings'.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_mode.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the mode of the model used by the workflow. — set_mode","text":"","code":"set_mode(workflow_obj, mode)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_mode.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the mode of the model used by the workflow. — set_mode","text":"workflow_obj ai_workflow object created ai_workflow() first place. mode mode used model workflow - needs either 'chat', 'completion' 'embeddings'.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_mode.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the mode of the model used by the workflow. — set_mode","text":"workflow object mode used applied new parameter.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_mode.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the mode of the model used by the workflow. — set_mode","text":"sets mode model workflow. usually means either 'chat','completion' 'embeddings'.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_mode.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the mode of the model used by the workflow. — set_mode","text":"","code":"my_workflow <- ai_workflow() |>  set_model(model_name=\"llama3:8b-instruct-q5_0\") |>  set_mode(\"chat\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the model to be used by the workflow — set_model","title":"Set the model to be used by the workflow — set_model","text":"set_model sets model used workflow object.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the model to be used by the workflow — set_model","text":"","code":"set_model(workflow_obj, model_name)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the model to be used by the workflow — set_model","text":"workflow_obj ai_workflow object created ai_workflow() first place. model_name name model use workflow","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the model to be used by the workflow — set_model","text":"simple function set model used. Note model needs available instance connect .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the model to be used by the workflow — set_model","text":"","code":"my_workflow <- ai_workflow() |>  set_model(model_name=\"llama3:8b-instruct-q5_0\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_n_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the number of tokens to be predicted (maximum) by the flow. — set_n_predict","title":"Set the number of tokens to be predicted (maximum) by the flow. — set_n_predict","text":"set_n_predict sets maximum number tokens predicted flow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_n_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the number of tokens to be predicted (maximum) by the flow. — set_n_predict","text":"","code":"set_n_predict(workflow_obj, n_predict)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_n_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the number of tokens to be predicted (maximum) by the flow. — set_n_predict","text":"workflow_obj ai_workflow object created ai_workflow() first place. n_predict number tokens predicted (maximum) flow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_n_predict.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the number of tokens to be predicted (maximum) by the flow. — set_n_predict","text":"sets maximum number tokens predicted flow. Note mean LLM constrain answer number, planned answer exceeds n_predict value, answer simply stop point.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_n_predict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the number of tokens to be predicted (maximum) by the flow. — set_n_predict","text":"","code":"wflow <- ai_workflow() |> set_connector(\"ollama\") |>  set_n_predict(n_predict=500) #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434."},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_num_ctx.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the length of the context to be handled by the model — set_num_ctx","title":"Set the length of the context to be handled by the model — set_num_ctx","text":"set_num_ctx lets define length context supported model","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_num_ctx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the length of the context to be handled by the model — set_num_ctx","text":"","code":"set_num_ctx(workflow_obj, num_ctx)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_num_ctx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the length of the context to be handled by the model — set_num_ctx","text":"workflow_obj workflow object containing parameters describing workflow required num_ctx numerical value defining length context used, tokens.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_num_ctx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the length of the context to be handled by the model — set_num_ctx","text":"workflow object new added num_ctx parameter","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_num_ctx.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the length of the context to be handled by the model — set_num_ctx","text":"Depending server settings, length context handled model may shorter expect. example, Ollama seems default context size 1024 tokens even model actually supports . order able fully use capabilities model, can specify length expect support num_ctx.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_num_ctx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the length of the context to be handled by the model — set_num_ctx","text":"","code":"my_workflow <- ai_workflow() |>  set_model(model_name=\"llama3:8b-instruct-q5_0\") |>  set_num_ctx(num_ctx=2048)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_overall_background.html","id":null,"dir":"Reference","previous_headings":"","what":"Set overall background info for your model before an answer is formulated — set_overall_background","title":"Set overall background info for your model before an answer is formulated — set_overall_background","text":"set_overall_background lets give additional background info supposed used model every answer.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_overall_background.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set overall background info for your model before an answer is formulated — set_overall_background","text":"","code":"set_overall_background(workflow_obj, overall_background)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_overall_background.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set overall background info for your model before an answer is formulated — set_overall_background","text":"workflow_obj workflow object containing parameters describing workflow required overall_background single-element text vector contains background information want system prompt ","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_overall_background.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set overall background info for your model before an answer is formulated — set_overall_background","text":"workflow object new added overall_background parameter","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_overall_background.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set overall background info for your model before an answer is formulated — set_overall_background","text":"Setting background info can help general knowledge reference expect LLM . Say, specific personal information want enter relevant answering several questions, want put . different RAG, RAG system basically pull relevant information every specific question. , akin letting LLM fundamental, general knowledge.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_overall_background.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set overall background info for your model before an answer is formulated — set_overall_background","text":"","code":"my_workflow <- ai_workflow() |>  set_system_prompt(system_prompt=\"You are a helpful AI assistant.  Answer to the best of your knowledge\") |> set_audience(\"Marketing Professionals\") |> set_overall_background(\"Our company, YOMAN & Co, has been struggling with our  recent products because of lack of market understanding.\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_port.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the port required to connect to the API server. — set_port","title":"Set the port required to connect to the API server. — set_port","text":"set_port sets port related API server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_port.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the port required to connect to the API server. — set_port","text":"","code":"set_port(workflow_obj, port)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_port.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the port required to connect to the API server. — set_port","text":"workflow_obj ai_workflow object created ai_workflow() first place. port port used access API server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_port.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the port required to connect to the API server. — set_port","text":"Set IP Address related API server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_port.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the port required to connect to the API server. — set_port","text":"","code":"wflow <- ai_workflow() |> set_connector(\"ollama\") |>  set_ip_addr(ip_addr=\"127.0.0.1\") |> set_port(port=\"11434\") #> → Default IP address has been set to 127.0.0.1. #> → Default port has been set to 11434. #> → IP address has been changed to 127.0.0.1. #> → Port has been changed to 11434."},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_presence_penalty.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the presence penalty of the model used by the flow. — set_presence_penalty","title":"Set the presence penalty of the model used by the flow. — set_presence_penalty","text":"set_presence_penalty sets presence penalty related model workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_presence_penalty.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the presence penalty of the model used by the flow. — set_presence_penalty","text":"","code":"set_presence_penalty(workflow_obj, presence_penalty)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_presence_penalty.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the presence penalty of the model used by the flow. — set_presence_penalty","text":"workflow_obj ai_workflow object created ai_workflow() first place. presence_penalty presence penalty value used model. value closer 1 encourages model generate novel diverse text. lower value, closer 0, encourages cliche phrases.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_presence_penalty.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the presence penalty of the model used by the flow. — set_presence_penalty","text":"Set presence_penalty used model workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_presence_penalty.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the presence penalty of the model used by the flow. — set_presence_penalty","text":"","code":"my_workflow <- ai_workflow() |>  set_model(model_name=\"llama3:8b-instruct-q5_0\") |>  set_presence_penalty(0.9)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_processing_skill.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the processing skill that you want to give the workflow. — set_processing_skill","title":"Set the processing skill that you want to give the workflow. — set_processing_skill","text":"set_processing_skill sets processing skill give workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_processing_skill.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the processing skill that you want to give the workflow. — set_processing_skill","text":"","code":"set_processing_skill(workflow_obj, processing_skill, ...)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_processing_skill.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the processing skill that you want to give the workflow. — set_processing_skill","text":"workflow_obj ai_workflow object created ai_workflow() first place. processing_skill string - processing skill want give workflow. ... additional parameters treated supported processing skill","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_processing_skill.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the processing skill that you want to give the workflow. — set_processing_skill","text":"workflow object appropriate processing skill applied prompt vector(s)","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_processing_skill.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the processing skill that you want to give the workflow. — set_processing_skill","text":"sets processing skill want give workflow. can list processing skills available default list_processing_skills().","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_processing_skill.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the processing skill that you want to give the workflow. — set_processing_skill","text":"","code":"my_workflow <- ai_workflow() |> set_model(model_name=\"llama3:8b-instruct-q5_0\") |>  set_processing_skill(\"fix_copy\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_repeat_penalty.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the repeat penalty of the model used by the flow. — set_repeat_penalty","title":"Set the repeat penalty of the model used by the flow. — set_repeat_penalty","text":"set_repeat_penalty sets repeat penalty related model workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_repeat_penalty.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the repeat penalty of the model used by the flow. — set_repeat_penalty","text":"","code":"set_repeat_penalty(workflow_obj, repeat_penalty)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_repeat_penalty.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the repeat penalty of the model used by the flow. — set_repeat_penalty","text":"workflow_obj ai_workflow object created ai_workflow() first place. repeat_penalty repeat penalty value used model. value 1 means penalty. value higher 1 means increased penalty repetition tokens.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_repeat_penalty.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the repeat penalty of the model used by the flow. — set_repeat_penalty","text":"Set repeat_penalty used model workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_repeat_penalty.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the repeat penalty of the model used by the flow. — set_repeat_penalty","text":"","code":"my_workflow <- ai_workflow() |>  set_model(model_name=\"llama3:8b-instruct-q5_0\") |>  set_repeat_penalty(1.3)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_seed.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the seed of the model used by the workflow. — set_seed","title":"Set the seed of the model used by the workflow. — set_seed","text":"set_seed sets seed model.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_seed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the seed of the model used by the workflow. — set_seed","text":"","code":"set_seed(workflow_obj, seed)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_seed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the seed of the model used by the workflow. — set_seed","text":"workflow_obj ai_workflow object created ai_workflow() first place. seed seed used model workflow, intend fix . seed give answer. interesting fix want compare effect parameters.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_seed.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the seed of the model used by the workflow. — set_seed","text":"workflow object seed applied.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_seed.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the seed of the model used by the workflow. — set_seed","text":"Set seed used model workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_seed.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the seed of the model used by the workflow. — set_seed","text":"","code":"my_workflow <- ai_workflow() |>  set_model(model_name=\"llama3:8b-instruct-q5_0\") |>  set_seed(12312312312)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_style_of_voice.html","id":null,"dir":"Reference","previous_headings":"","what":"Define a specific style of voice that you want the LLM to use when answering — set_style_of_voice","title":"Define a specific style of voice that you want the LLM to use when answering — set_style_of_voice","text":"set_style_of_voice lets define specific style voice want LLM use answering","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_style_of_voice.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define a specific style of voice that you want the LLM to use when answering — set_style_of_voice","text":"","code":"set_style_of_voice(workflow_obj, style_of_voice)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_style_of_voice.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define a specific style of voice that you want the LLM to use when answering — set_style_of_voice","text":"workflow_obj ai_workflow object created ai_workflow() first place. style_of_voice text description person style person want LLM imitate.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_style_of_voice.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define a specific style of voice that you want the LLM to use when answering — set_style_of_voice","text":"workflow object new added style_of_voice parameter","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_style_of_voice.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Define a specific style of voice that you want the LLM to use when answering — set_style_of_voice","text":"lets define specific style voice want LLM use answering","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_style_of_voice.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define a specific style of voice that you want the LLM to use when answering — set_style_of_voice","text":"","code":"my_workflow <- ai_workflow() |>  set_system_prompt(system_prompt=\"You are a helpful AI assistant.  Answer to the best of your knowledge\") |> set_audience(\"Marketing Professionals\") |> set_style_of_voice(\"Snoop Dog\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_system_prompt.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the system prompt to be used by the model. — set_system_prompt","title":"Set the system prompt to be used by the model. — set_system_prompt","text":"set_system_prompt sets system prompt used model.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_system_prompt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the system prompt to be used by the model. — set_system_prompt","text":"","code":"set_system_prompt(workflow_obj, system_prompt)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_system_prompt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the system prompt to be used by the model. — set_system_prompt","text":"workflow_obj ai_workflow object created ai_workflow() first place. system_prompt mode used model workflow - needs either 'chat' 'completion'.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_system_prompt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the system prompt to be used by the model. — set_system_prompt","text":"workflow object system prompt specified new parameter.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_system_prompt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the system prompt to be used by the model. — set_system_prompt","text":"sets system prompt model workflow. can give additional guidance personality change way model answer prompts.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_system_prompt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the system prompt to be used by the model. — set_system_prompt","text":"","code":"my_workflow <- ai_workflow() |>  set_system_prompt(system_prompt=\"You are a helpful AI assistant.  Answer to the best of your knowledge\") my_workflow <- ai_workflow() |>  set_system_prompt(system_prompt=\"You are a helpful AI assistant  and an expert in financial analysis.\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_temperature.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the temperature of the model used by the workflow. — set_temperature","title":"Set the temperature of the model used by the workflow. — set_temperature","text":"set_temperature sets temperature model used workflow.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_temperature.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the temperature of the model used by the workflow. — set_temperature","text":"","code":"set_temperature(workflow_obj, temperature)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_temperature.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the temperature of the model used by the workflow. — set_temperature","text":"workflow_obj ai_workflow object created ai_workflow() first place. temperature temperature value used model. 0 1 (boundaries included).","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_temperature.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set the temperature of the model used by the workflow. — set_temperature","text":"Set temperature used model workflow. temperature zero always give answer. temperature 1 give random answers may make full sense. Ideally want level randomness still remaining sensible answers, target value around 0.6 0.7. trying validate AI workflow specific answers specific prompts, highly recommended work temperature zero.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/set_temperature.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the temperature of the model used by the workflow. — set_temperature","text":"","code":"my_workflow <- ai_workflow() |>  set_model(model_name=\"llama3:8b-instruct-q5_0\") |>  set_temperature(0.8)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_paragraphs.html","id":null,"dir":"Reference","previous_headings":"","what":"Split text into paragraphs — split_text_as_paragraphs","title":"Split text into paragraphs — split_text_as_paragraphs","text":"split_text_as_paragraphs Splits text sentences-based chunks.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_paragraphs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split text into paragraphs — split_text_as_paragraphs","text":"","code":"split_text_as_paragraphs(text)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_paragraphs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split text into paragraphs — split_text_as_paragraphs","text":"text single piece text break paragraphs.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_paragraphs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Split text into paragraphs — split_text_as_paragraphs","text":"Splits text paragraphs-based chunks, leveraging tokenizers library.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_paragraphs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split text into paragraphs — split_text_as_paragraphs","text":"","code":"# c(\"Hi! How are you?\\n\\n Do you want to go for a walk?\") |> split_text_as_paragraphs()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_sentences.html","id":null,"dir":"Reference","previous_headings":"","what":"Split text into sentences — split_text_as_sentences","title":"Split text into sentences — split_text_as_sentences","text":"split_text_as_sentences Splits text sentences-based chunks.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_sentences.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split text into sentences — split_text_as_sentences","text":"","code":"split_text_as_sentences(text)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_sentences.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split text into sentences — split_text_as_sentences","text":"text single piece text break .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_sentences.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Split text into sentences — split_text_as_sentences","text":"Splits text sentences-based chunks, leveraging tokenizers library.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/split_text_as_sentences.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split text into sentences — split_text_as_sentences","text":"","code":"# c(\"Hi! How are you? Do you want to go for a walk?\") |> split_text_as_sentences()"},{"path":"https://r-guyot.github.io/aiworkflow/reference/switch_to_workflow.html","id":null,"dir":"Reference","previous_headings":"","what":"Switch to workflow — switch_to_workflow","title":"Switch to workflow — switch_to_workflow","text":"switch_to_workflow function makes possible chain several workflows using pipes","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/switch_to_workflow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Switch to workflow — switch_to_workflow","text":"","code":"switch_to_workflow(workflow, new_workflow)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/switch_to_workflow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Switch to workflow — switch_to_workflow","text":"workflow workflow object containing parameters describing flow required new_workflow workflow object execute last one","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/switch_to_workflow.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Switch to workflow — switch_to_workflow","text":"function send output previous workflow next one.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_llamacpp_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"Confirm connection to a Llama.cpp server is working — test_llamacpp_connection","title":"Confirm connection to a Llama.cpp server is working — test_llamacpp_connection","text":"test_llamacpp_connection tests connection llama.cpp server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_llamacpp_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Confirm connection to a Llama.cpp server is working — test_llamacpp_connection","text":"","code":"test_llamacpp_connection(ip_ad = \"127.0.0.1\", port = \"8080\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_llamacpp_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Confirm connection to a Llama.cpp server is working — test_llamacpp_connection","text":"ip_ad IP address server running Llama.cpp. Default localhost 127.0.0.1. port port used run Llama.cpp service. Default 8080.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_llamacpp_connection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Confirm connection to a Llama.cpp server is working — test_llamacpp_connection","text":"simple function test connection llama.cpp server.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_ollama_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"Confirm connection to ollama is working — test_ollama_connection","title":"Confirm connection to ollama is working — test_ollama_connection","text":"test_ollama_connection says hello uses name person(s) argument. y","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_ollama_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Confirm connection to ollama is working — test_ollama_connection","text":"","code":"test_ollama_connection(ip_ad = \"127.0.0.1\", port = \"11434\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_ollama_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Confirm connection to ollama is working — test_ollama_connection","text":"ip_ad IP address server running ollama. Default localhost 127.0.0.1. port port used run ollama service. Default 11374.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_ollama_connection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Confirm connection to ollama is working — test_ollama_connection","text":"Returns TRUE connection works, error object .","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_ollama_connection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Confirm connection to ollama is working — test_ollama_connection","text":"simple function test connect ollama","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/test_ollama_connection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Confirm connection to ollama is working — test_ollama_connection","text":"","code":"#test_ollama_connection(ip_ad=\"127.0.0.1\", port=\"11434\")"},{"path":"https://r-guyot.github.io/aiworkflow/reference/write_vectors_to_feather_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Write Vectors to Feather File — write_vectors_to_feather_file","title":"Write Vectors to Feather File — write_vectors_to_feather_file","text":"write_vectors_to_feather_file lets write vector data feather file","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/write_vectors_to_feather_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write Vectors to Feather File — write_vectors_to_feather_file","text":"","code":"write_vectors_to_feather_file(vector_data, file_name)"},{"path":"https://r-guyot.github.io/aiworkflow/reference/write_vectors_to_feather_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write Vectors to Feather File — write_vectors_to_feather_file","text":"vector_data vector data (embeddings) already generated another function. file_name file name save embeddings feather file","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/write_vectors_to_feather_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write Vectors to Feather File — write_vectors_to_feather_file","text":"nothing.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/write_vectors_to_feather_file.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Write Vectors to Feather File — write_vectors_to_feather_file","text":"function provides simple way save vector data (embeddings) binary feather file, instead using regular databases.","code":""},{"path":"https://r-guyot.github.io/aiworkflow/reference/write_vectors_to_feather_file.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write Vectors to Feather File — write_vectors_to_feather_file","text":"","code":"conn <- get_ollama_connection()  document <- \"Standing proudly on the Île de la Cité in the heart of Paris,  France's capital city, lies one of the world's most beloved and historic  landmarks: the magnificent Notre Dame Cathedral. This Gothic masterpiece  has been welcoming pilgrims and tourists alike for over 850 years, since its  construction began in 1163 under King Louis VII. With its towering spires,  stunning stained glass windows, and intricate stone carvings, this beautiful  church is a testament to medieval architecture and engineering skill.  Unfortunately, a devastating fire ravaged the cathedral on April 15, 2019,  but thanks to swift action from firefighters and restoration efforts  underway, Notre Dame continues to inspire awe in those who visit her.\"  writeLines(document, con = \"doc1.txt\")  write_vectors_to_feather_file(file_name = \"doc1.feather\", vector_data = generate_document_embeddings(conn,  document_path = \"doc1.txt\", splitter = \"paragraph\"))"}]
